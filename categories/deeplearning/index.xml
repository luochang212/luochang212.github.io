<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DeepLearning on Chang Luo</title>
    <link>https://luochang212.github.io/categories/deeplearning/</link>
    <description>Recent content in DeepLearning on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 09 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/categories/deeplearning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深度嵌入聚类算法 DEC</title>
      <link>https://luochang212.github.io/posts/dec_pytorch/</link>
      <pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/dec_pytorch/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;如何对图片做聚类，我的直觉是：先用预训练模型计算图片 Embedding，然后用 cosine 度量的 DBSCAN 无监督地计算图片 label，再用 MLP 有监督地学习上一步产生的 label。DBSCAN 的好处是可以把无监督转为有监督，且由于它基于密度的特性，还不需要指定聚类的类别数，这方便了生产环境使用，因为生产环境通常也是不知道类别数的。MLP 的好处是可以对输入泛化，即使没见过的输入，在不重训练的情况下，也可以有一个对应输出。图片特征提取器 + 传统聚类 + 神经网络，是简单且符合直觉的方法，但恐怕不是最好的方法。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;我们探索使用更端到端的方法实现图片聚类：DINOv2 特征提取器 + DEC 聚类器&lt;/p&gt;&#xA;&lt;p&gt;使用 DEC 的好处起码有两点。一是让训练过程更简单，端到端的架构肯定比两阶段模型的架构更简单。二是 DEC 用特征向量表示聚类中心，这和传统聚类用标签表示不同。特征向量表示的类心更便于微调和增量更新。K-Means 每次更新 label 都是乱的，需要用匈牙利算法，对前后两次结果进行桥接。神经网络在这一点上天生有优势，因为它是顺着梯度一点一点更新的，所以前后两次结果是天然有联系，并且可以限制更新的幅度。&lt;/p&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/dec-pytorch&#34; target=&#34;_blank&#34;&gt;dec-pytorch&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;本文的工作包括：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用 DINOv2 模型生成图片 Embeddings&lt;/li&gt;&#xA;&lt;li&gt;用 FastAPI 开发 DINOv2 批量推理服务，支持分 batch 和 模型结果归一化&lt;/li&gt;&#xA;&lt;li&gt;训练 DEC 模型的三阶段：训练降噪自编码器、初始化聚类中心、训练 DEC&lt;/li&gt;&#xA;&lt;li&gt;开发集成的 DEC 训练框架，支持训练、推理、保存，详见 &lt;a href=&#34;https://github.com/luochang212/dec-pytorch/blob/main/dec.py&#34; target=&#34;_blank&#34;&gt;dec.py&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;在我的数据集上，对比 DEC 与传统聚类算法的效果：与 K-Means 接近&lt;/li&gt;&#xA;&lt;li&gt;介绍 DEC 的创新点：软分配策略和目标分布优化&lt;/li&gt;&#xA;&lt;li&gt;在线学习探索：尝试两种思路，对 DEC 模型做小幅度的增量更新&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;✨ DEC 论文在这里 &lt;a href=&#34;https://arxiv.org/abs/1511.06335&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Unsupervised Deep Embedding for Clustering Analysis&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Triton 部署 CLIP 图文 Embedding 推理服务</title>
      <link>https://luochang212.github.io/posts/clip_triton_server/</link>
      <pubDate>Sat, 30 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/clip_triton_server/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文介绍如何用 Triton 在多 GPU 环境下部署高性能 CLIP 推理服务。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/clip-server&#34; target=&#34;_blank&#34;&gt;clip-server&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;CLIP 是一个多模态模型。它能将图像和文本映射到同一个向量空间中，由此可以产生诸多应用。比如，通过计算图片与文本的相似性，可以用近似最近邻 (ANN) 从相册中检索与给定 query 语义相近的图片。此外，CLIP 的 Vision Encoder 可以作为特征提取器使用，用于生成的图像 Embedding。如果在 Vision Encoder 后加一个 fc 层，并且冻住骨干网络仅对 fc 层做训练，通常可以得到一个效果不错的图像分类器。&lt;/p&gt;&#xA;&lt;p&gt;本文涉及的内容包括：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;用 &lt;code&gt;transformers&lt;/code&gt; 库运行 &lt;code&gt;openai/clip-vit-base-patch32&lt;/code&gt; 的简单示例&lt;/li&gt;&#xA;&lt;li&gt;在 &lt;code&gt;titanic&lt;/code&gt; 数据集上训练一个 MLP 并导出成 ONNX 格式&lt;/li&gt;&#xA;&lt;li&gt;介绍如何安装预装了 Triton 的 Nvidia 官方 Docker 镜像 &lt;code&gt;&amp;amp;&lt;/code&gt; 启动容器&lt;/li&gt;&#xA;&lt;li&gt;介绍如何将 MLP 的 ONNX 模型配置到 Triton 模型仓库中&lt;/li&gt;&#xA;&lt;li&gt;写了一个简单的 &lt;a href=&#34;https://github.com/luochang212/clip-server/blob/main/utils.py#L108&#34; target=&#34;_blank&#34;&gt;客户端&lt;/a&gt; 用于获取 Triton 的推理结果&lt;/li&gt;&#xA;&lt;li&gt;介绍 Triton 的 Python Backend，其通常用于模型预处理和后处理&lt;/li&gt;&#xA;&lt;li&gt;用 Model Ensemble 组装 Python Backend 和 ONNX 组成完整的推理服务&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;✨ 注意：运行以下代码依赖 &lt;a href=&#34;https://github.com/luochang212/clip-server/blob/main/utils.py&#34; target=&#34;_blank&#34;&gt;utils.py&lt;/a&gt; 文件和 &lt;a href=&#34;https://github.com/luochang212/clip-server/blob/main/mlp.py&#34; target=&#34;_blank&#34;&gt;mlp.py&lt;/a&gt; 文件。&lt;/p&gt;</description>
    </item>
    <item>
      <title>三种图神经网络算法：GraphSAGE, GCN 和 GAT</title>
      <link>https://luochang212.github.io/posts/graph_embedding/</link>
      <pubDate>Sat, 10 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/graph_embedding/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;GNN 处理图数据的方式还是很符合直觉的，基本沿袭了 CNN 的思路：每个神经元只看局部信息，通过层层汇聚掌握全貌。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/graph-embedding&#34; target=&#34;_blank&#34;&gt;graph-embedding&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;本文做了什么：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对 GraphSAGE 的简单实现做逐行注释&lt;/li&gt;&#xA;&lt;li&gt;在 Docker 环境运行 GraphSAGE 的原版示例&lt;/li&gt;&#xA;&lt;li&gt;用 &lt;a href=&#34;https://pytorch-geometric.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34;&gt;PyG&lt;/a&gt; 实现了 GCN 和 GAT&lt;/li&gt;&#xA;&lt;li&gt;为运行 PyG 写了一些 pipeline 代码&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;✨ 注意：运行以下代码依赖 &lt;a href=&#34;https://github.com/luochang212/graph-embedding/blob/main/util.py&#34; target=&#34;_blank&#34;&gt;util.py&lt;/a&gt; 文件。&lt;/p&gt;&#xA;&lt;h2 id=&#34;一graphsage-的简单实现&#34;&gt;一、GraphSAGE 的简单实现&lt;/h2&gt;&#xA;&lt;p&gt;主流图算法大致分两种：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;图嵌入算法 (GE): DeepWalk, Node2Vec 等&lt;/li&gt;&#xA;&lt;li&gt;图神经网络算法 (GNN): GraphSAGE, GCN, GAT 等&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;11-绪论图神经网络&#34;&gt;1.1 绪论：图神经网络&lt;/h3&gt;&#xA;&lt;!-- 图神经网络的输入是 **节点间关联关系** 和 **节点特征**，输出是图嵌入。 --&gt;&#xA;&lt;p&gt;图神经网络算法做的事，相当于把图这种复杂的数据结构，转换成低维向量，而低维向量往往是很好用的。&lt;/p&gt;&#xA;&lt;p&gt;拿到图嵌入可以做很多事情，比如：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;节点分类&lt;/li&gt;&#xA;&lt;li&gt;链接预测&lt;/li&gt;&#xA;&lt;li&gt;社区发现&lt;/li&gt;&#xA;&lt;li&gt;相似度量&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;总之，图嵌入是一种非常有用的特征。在实践中，甚至可以将图嵌入和其他特征 concat 起来，训练更复杂的模型。&lt;/p&gt;&#xA;&lt;h4 id=&#34;111-gnn-和-cnn&#34;&gt;1.1.1 GNN 和 CNN&lt;/h4&gt;&#xA;&lt;p&gt;GNN 和 CNN 的思路还挺像的，可以看作 CNN 在图数据上的推广。&lt;/p&gt;</description>
    </item>
    <item>
      <title>像搭积木一样搭神经网络</title>
      <link>https://luochang212.github.io/posts/dl_tricks/</link>
      <pubDate>Sun, 07 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/dl_tricks/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;搭积木的时候，需要将不同类型的积木搭在一起：门框、窗子、走廊、屋顶。对每一种类型的积木，又有多种变体可供选择。比如，屋顶可以用文艺复兴风格，也可以用中式庭园风格。神经网络也是，学神经网络，本质上就是认识各种各样“积木”的过程。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/dl-tricks/blob/main/note.ipynb&#34; target=&#34;_blank&#34;&gt;dl-tricks/note.ipynb&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;一必要组件&#34;&gt;一、必要组件&lt;/h3&gt;&#xA;&lt;h4 id=&#34;11-从-mlp-说起&#34;&gt;1.1 从 MLP 说起&lt;/h4&gt;&#xA;&lt;p&gt;我们从最简单的深度神经网络 &lt;strong&gt;多层感知机&lt;/strong&gt; (MLP) 开始说起。麻雀虽小，五脏俱全。了解数据如何在 MLP 中流动，就能大致勾勒一个神经网络的 &lt;strong&gt;必要组件&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;下图是一个 4 层感知机，左边是特征，右边是标签。训练开始时，样本数据先从左到右做 &lt;strong&gt;正向传播&lt;/strong&gt;。待数据流到右侧，用 &lt;strong&gt;损失函数&lt;/strong&gt; 计算损失。此时损失是一个标量，而最后一层的节点权重 &lt;code&gt;$W$&lt;/code&gt; 是一个矩阵，标量对矩阵的偏导是矩阵。&lt;strong&gt;优化器&lt;/strong&gt; 会用大小合适的梯度矩阵，沿负梯度方向逐层反向更新权重 &lt;code&gt;$W$&lt;/code&gt;。这样下一 &lt;strong&gt;批量&lt;/strong&gt; (batch) 数据进入网络时，正好能用上一轮更新后的参数做正向传播。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://luochang212.github.io/img/mlp_network.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h4 id=&#34;12-dataloader&#34;&gt;1.2 DataLoader&lt;/h4&gt;&#xA;&lt;p&gt;样本是有限的，为了让模型获得最强性能，必须榨干每个样本的价值。&lt;/p&gt;&#xA;&lt;p&gt;因此在训练中，一个样本往往要复用多次。&lt;code&gt;DataLoader&lt;/code&gt; 就在做这样一件事。它把数据编排成一个个批量，并构建一个迭代器。每次调用它，会返回一个从第一个批量开始遍历的迭代器。这个特性使得复用样本变得更加方便。&lt;/p&gt;&#xA;&lt;p&gt;原生的 PyTorch &lt;code&gt;DataLoader&lt;/code&gt; 很复杂，让我们来实现一个野生 &lt;code&gt;DataLoader&lt;/code&gt;：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import math&#xA;import torch&#xA;&#xA;class DataLoader:&#xA;    def __init__(self, data: list, batch_size: int):&#xA;        self.i = 0&#xA;        self.batch_size = batch_size&#xA;        self.batch_num = math.ceil(len(data) / batch_size)&#xA;        self._data = self.gen_batch(data)&#xA;&#xA;    def gen_batch(self, data):&#xA;        lst = []&#xA;        s = self.batch_size&#xA;        for i in range(self.batch_num):&#xA;            start, end = s * i, s * (i + 1)&#xA;            X = torch.tensor([e[0] for e in data[start:end]])&#xA;            y = torch.tensor([e[1] for e in data[start:end]])&#xA;            lst.append((X, y))&#xA;&#xA;        return lst&#xA;&#xA;    def __iter__(self):&#xA;        self.i = 0&#xA;        return self&#xA;&#xA;    def __next__(self):&#xA;        if self.i &amp;lt; len(self._data):&#xA;            self.i += 1&#xA;            return self._data[self.i - 1]&#xA;        else:&#xA;            raise StopIteration&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;假设有 2560 个样本。计划分成 10 个批量，则每批量有 256 个样本。我们可以用上面的野生 &lt;code&gt;DataLoader&lt;/code&gt; 加载这些样本。&lt;/p&gt;</description>
    </item>
    <item>
      <title>文本情感分析</title>
      <link>https://luochang212.github.io/posts/sentiment_analysis/</link>
      <pubDate>Sat, 06 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/sentiment_analysis/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;用 Bert + Transformer Encoder + MLP 做文本情感分析。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/sentiment-analysis&#34; target=&#34;_blank&#34;&gt;sentiment-analysis&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;IMDB 数据集：&lt;a href=&#34;https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/data&#34; target=&#34;_blank&#34;&gt;imdb-dataset&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;本文要点：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;建立 词向量 ⇋ CSV 文件 双向 Pipeline&lt;/li&gt;&#xA;&lt;li&gt;用两种方法对 IMDB 电影评论做情感分析：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Bert 预训练词向量 + MLP&lt;/li&gt;&#xA;&lt;li&gt;Bert + Transformer Encoder + 全连接层&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;✨ PS: 前两章是 Pipeline 代码，建议从第三章看起。&lt;/p&gt;&#xA;&lt;h3 id=&#34;一读写词向量&#34;&gt;一、读写词向量&lt;/h3&gt;&#xA;&lt;p&gt;本节的主要目标是完成 &lt;code&gt;词向量 -&amp;gt; CSV 文件&lt;/code&gt; 和 &lt;code&gt;CSV 文件 -&amp;gt; 词向量&lt;/code&gt; 的 Pipeline。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;对语料做预处理&lt;/li&gt;&#xA;&lt;li&gt;获取词向量和句子向量&lt;/li&gt;&#xA;&lt;li&gt;将词向量存入 csv&lt;/li&gt;&#xA;&lt;li&gt;从 csv 中读取词向量&lt;/li&gt;&#xA;&lt;li&gt;将读写词向量功能整合成函数&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/sentiment-analysis/blob/main/1.store_embedding.ipynb&#39;)&#34;&gt;查看示例&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;二获取-imdb-数据集的-embedding&#34;&gt;二、获取 IMDB 数据集的 Embedding&lt;/h3&gt;&#xA;&lt;p&gt;将 IMDB 数据集中的电影评论转换成句子向量，然后存在 CSV 文件中。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;文本预处理&lt;/li&gt;&#xA;&lt;li&gt;计算句子向量&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/sentiment-analysis/blob/main/2.imdb_embedding.ipynb&#39;)&#34;&gt;查看示例&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;三用-mlp-做文本情感分析&#34;&gt;三、用 MLP 做文本情感分析&lt;/h3&gt;&#xA;&lt;p&gt;用 Bert + MLP 做 IMDB 电影评论文本情感分析。&lt;/p&gt;</description>
    </item>
    <item>
      <title>中文词向量生成</title>
      <link>https://luochang212.github.io/posts/chinese_embedding/</link>
      <pubDate>Sat, 29 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/chinese_embedding/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文做了一件有意思的事，用 Bert 生成《红楼梦》人名的词嵌入，再用 t-SNE 将高维的词向量降为二维后做可视化。结果发现，主角团三人的名字在向量空间中是紧紧挨在一起的。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/chinese-embedding&#34; target=&#34;_blank&#34;&gt;luochang212/chinese-embedding&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;本文探索的内容包括：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如何生成词向量&lt;/li&gt;&#xA;&lt;li&gt;如何生成句子向量&lt;/li&gt;&#xA;&lt;li&gt;如何找到语义相近的词&lt;/li&gt;&#xA;&lt;li&gt;如何对 embedding 做降维及可视化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;一用-word2vec-生成词向量&#34;&gt;一、用 Word2Vec 生成词向量&lt;/h3&gt;&#xA;&lt;p&gt;用 Word2Vec 生成中英文词向量。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;英文词向量&lt;/li&gt;&#xA;&lt;li&gt;中文词向量&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/chinese-embedding/blob/main/1.word2vec.ipynb&#39;)&#34;&gt;查看笔记&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;二用-bert-生成句子向量&#34;&gt;二、用 Bert 生成句子向量&lt;/h3&gt;&#xA;&lt;p&gt;用 Bert 生成中英文句子向量。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;英文句子向量&lt;/li&gt;&#xA;&lt;li&gt;中文句子向量&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/chinese-embedding/blob/main/2.bert.ipynb&#39;)&#34;&gt;查看笔记&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;三寻找最近邻-embedding&#34;&gt;三、寻找最近邻 embedding&lt;/h3&gt;&#xA;&lt;p&gt;我的构想是：拿到红楼梦里所有词汇的 embedding，然后看我们感兴趣的词（比如林黛玉）离哪个词最近。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;分词&lt;/li&gt;&#xA;&lt;li&gt;批量计算 embedding&lt;/li&gt;&#xA;&lt;li&gt;计算每个词的 embedding&lt;/li&gt;&#xA;&lt;li&gt;计算我们关心词汇的近邻 embedding&lt;/li&gt;&#xA;&lt;li&gt;整合成一个类&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/chinese-embedding/blob/main/3.neighbour.ipynb&#39;)&#34;&gt;查看笔记&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;四embedding-可视化&#34;&gt;四、Embedding 可视化&lt;/h3&gt;&#xA;&lt;p&gt;用 t-SNE 和 PCA 对 embedding 降维，做 2D &amp;amp; 3D 可视化&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;红楼梦中的人物关系&lt;/li&gt;&#xA;&lt;li&gt;中英美城市群&#xA;&lt;ul&gt;&#xA;&lt;li&gt;t-SNE&lt;/li&gt;&#xA;&lt;li&gt;PCA&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/chinese-embedding/blob/main/4.visualization.ipynb&#39;)&#34;&gt;查看笔记&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;五头脑风暴&#34;&gt;五、头脑风暴&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Embedding 的稳定性&lt;/li&gt;&#xA;&lt;li&gt;Embedding 差值的意义&lt;/li&gt;&#xA;&lt;li&gt;输出 embedding 的数量&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/chinese-embedding/blob/main/5.brainstorming.ipynb&#39;)&#34;&gt;查看笔记&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;附录和-qwen-25-的聊天记录&#34;&gt;附录：和 Qwen-2.5 的聊天记录&lt;/h3&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; nn.Embedding 是一种预训练的embedding模型吗&lt;/p&gt;</description>
    </item>
    <item>
      <title>Attention Is All You Need 论文精读</title>
      <link>https://luochang212.github.io/posts/transformer_arxiv/</link>
      <pubDate>Sun, 16 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/transformer_arxiv/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;Transformer 的编码器变成了 BERT，解码器变成了 GPT。BERT 推动过去几年搜广推算法增长，而 GPT 促成了今天 GenAI 浪潮的爆发。这篇发表于 2017 年的论文，对今天产生了难以估量的影响。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;原文：&lt;a href=&#34;https://luochang212.github.io/gadget/attention-is-all-you-need/&#34; target=&#34;_blank&#34;&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;论文翻译&#34;&gt;论文翻译&lt;/h2&gt;&#xA;&lt;h3 id=&#34;摘要&#34;&gt;摘要&lt;/h3&gt;&#xA;&lt;p&gt;主流 seq2seq 模型是基于编解码器架构实现的复杂 RNN 或 CNN 网络，其中表现最好的模型还会使用注意力机制来连接编码器和解码器。我们提出一种全新的简单网络架构：Transformer。它完全基于注意力机制，不使用 RNN 和 CNN。在两个机器翻译任务上的实验表明，它拥有更好的并行度，并且训练时间大大减少。在 WMT 2014 英德翻译任务上，我们的模型取得了 28.4 的 BLEU 分数，比现有最好模型提升 2 BLEU。在 WMT 2014 英法翻译任务上，我们的模型在 8 台 GPU 上训练 3.5 天后，在单一模型评分指标下获得 41.8 的最高分。相比之前文献的最佳模型，Transformer 极大降低了训练成本。我们还通过英语成分句法分析任务展示了 Transformer 的泛化能力，无论数据集大小，Transformer 都能很好地泛化到其他任务上。&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-介绍&#34;&gt;1. 介绍&lt;/h3&gt;&#xA;&lt;p&gt;循环神经网络、长短记忆网络和门控循环网络被证明是序列模型和处理语言建模和机器翻译这类转换问题的最先进方法。在此之后，人们又花费大量努力挖掘循环神经网络语言模型和编解码器架构的潜力。&lt;/p&gt;&#xA;&lt;p&gt;循环神经网络对输入输出词元按位置进行计算，将词元的位置与时间步进行对齐，生成一系列隐状态 $h_t$。该隐状态是前一个隐状态 $h_{t-1}$ 和时间步 $t$ 时刻输入 $X_t$ 的函数。在训练样本时，这种内在的序列关系天然阻碍并行。对长序列文本，因为内存限制了批量样本的处理，导致这种阻碍更加明显。最近的研究利用因子分解和条件计算两种方法显著提升了计算效率，尤其后者还提高了模型的性能。但是序列计算这个最根本的限制依然存在。&lt;/p&gt;&#xA;&lt;p&gt;注意力机制在多种序列建模和转换建模任务中占有重要地位，它能对输入输出序列中的依赖关系进行位置无关的建模。除了少数几个例子外，注意力机制通常和循环神经网络一起使用。&lt;/p&gt;&#xA;&lt;p&gt;我们提出了 Transformer，一种不使用循环神经网络、纯基于注意力来捕获输入输出全局依赖关系的模型。Transformer 显著提高了并行度，并且在 8 台 P100 GPU 上训练 12 小时后，翻译质量达到了前所未有的高度。&lt;/p&gt;</description>
    </item>
    <item>
      <title>注意力机制笔记</title>
      <link>https://luochang212.github.io/posts/attention_note/</link>
      <pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/attention_note/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~yima/&#34; target=&#34;_blank&#34;&gt;马毅&lt;/a&gt;：要想正确理解深度神经网络，就必须认识到其本质是学习高维数据中的低维结构的手段。从第一性原理出发，把目的和手段分清楚，其余的都很容易被统一、被解释。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;从 Attention 的角度理解马毅老师这句话，Embedding 的时候本来就升维了，再做 QKV 就相当于在高维里面抽低维信息。而且 Q 也是可学习的，所以就既能学到好的抽取方法；对于每一种抽取方法，又能特别高效地抽取。&lt;/p&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/rnn-note&#34; target=&#34;_blank&#34;&gt;rnn-note&lt;/a&gt; / &lt;a href=&#34;https://github.com/luochang212/attention-note&#34; target=&#34;_blank&#34;&gt;attention-note&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;一语言模型入门rnn-lstm-gru&#34;&gt;一、语言模型入门：RNN, LSTM, GRU&lt;/h3&gt;&#xA;&lt;h4 id=&#34;11-序列模型&#34;&gt;1.1 序列模型&lt;/h4&gt;&#xA;&lt;p&gt;马尔可夫假设，当前数据只跟最近 τ 个数据点相关。把最近 τ 个数据点作为特征，用 MLP 预测当前数据点的值。&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/rnn-note/blob/main/1.序列模型.ipynb&#39;)&#34;&gt;查看笔记&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h4 id=&#34;12-文本预处理&#34;&gt;1.2 文本预处理&lt;/h4&gt;&#xA;&lt;p&gt;对文本词元化 (tokenize) 并构建词表，就是把文本映射到从 0 开始的索引。&lt;/p&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/rnn-note/blob/main/2.文本预处理.ipynb&#39;)&#34;&gt;查看笔记&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h4 id=&#34;13-语言模型和数据集&#34;&gt;1.3 语言模型和数据集&lt;/h4&gt;&#xA;&lt;p&gt;对语料分批量 (batch) 处理。介绍了两种（batch 内的）采样策略：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;随机采样策略：每个 batch 内的相邻子序列是随机的&lt;/li&gt;&#xA;&lt;li&gt;顺序分区策略：每个 batch 内的相邻子序列是顺序的&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/rnn-note/blob/main/3.语言模型和数据集.ipynb&#39;)&#34;&gt;查看笔记&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h4 id=&#34;14-循环神经网络的从零开始实现&#34;&gt;1.4 循环神经网络的从零开始实现&lt;/h4&gt;&#xA;&lt;p&gt;每次输出仅由前一个隐状态和当前新输入 x 决定，是为 RNN。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://luochang212.github.io/img/rnn_flow.svg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;提及的知识点：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;独热编码&lt;/strong&gt;：文本经过词元化后，还要经过 one-hot 处理，才能进入模型&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;困惑度&lt;/strong&gt;：我们用困惑度来描述文本生成的质量，通过一个序列中所有的 n 个词元的交叉熵损失的平均值来衡量&#xA;$$\frac{1}{n} \sum_{t=1}^n-\log P\left(x_t \mid x_{t-1}, \ldots, x_1\right)$$&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;梯度裁剪&lt;/strong&gt;：对于 $T$ 长序列将产生 $O(T)$ 长矩阵乘法链。当 $T$ 较大时，可能导致数值不稳定，例如可能导致梯度爆炸或梯度消失。这种情况下优化算法可能无法收敛。下式通过将梯度 $g$ 投影回给定半径 $\theta$ 来限制梯度的大小。其中 $\frac{\theta}{|\mathbf{g}|}$ 可以理解为梯度 $g$ 的单位方向向量。&#xA;$$\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{|\mathbf{g}|}\right) \mathbf{g}$$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/rnn-note/blob/main/4.循环神经网络的从零开始实现.ipynb&#39;)&#34;&gt;查看笔记&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h4 id=&#34;15-循环神经网络的简洁实现&#34;&gt;1.5 循环神经网络的简洁实现&lt;/h4&gt;&#xA;&lt;p&gt;使用高级 API 实现循环神经网络。&lt;/p&gt;</description>
    </item>
    <item>
      <title>LSTM 家庭用电预测</title>
      <link>https://luochang212.github.io/posts/lstm_power_consumption/</link>
      <pubDate>Sun, 12 May 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/lstm_power_consumption/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;长短期记忆网络（Long Short-Term Memory，LSTM）是一种 RNN 模型，常用于序列数据建模。尤其适合需要挖掘序列中长期依赖关系的问题。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/lstm-model&#34; target=&#34;_blank&#34;&gt;luochang212/lstm-model&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;一keras-模型训练&#34;&gt;一、keras 模型训练&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;数据导入&lt;/li&gt;&#xA;&lt;li&gt;数据预处理&lt;/li&gt;&#xA;&lt;li&gt;趋势可视化&lt;/li&gt;&#xA;&lt;li&gt;分割训练集和测试集&lt;/li&gt;&#xA;&lt;li&gt;定义 &amp;amp; 训练网络&lt;/li&gt;&#xA;&lt;li&gt;预测&lt;/li&gt;&#xA;&lt;li&gt;预测整个序列&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.jupyter.org/github/luochang212/lstm-model/blob/main/1.keras模型训练.ipynb&#39;)&#34;&gt;查看示例&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;二keras-模型推理&#34;&gt;二、keras 模型推理&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;数据预处理&lt;/li&gt;&#xA;&lt;li&gt;用训练好的 keras 模型做预测&lt;/li&gt;&#xA;&lt;li&gt;预测下一个值&lt;/li&gt;&#xA;&lt;li&gt;预测下一个序列&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.jupyter.org/github/luochang212/lstm-model/blob/main/2.keras模型推理.ipynb&#39;)&#34;&gt;查看示例&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;三pytorch-模型训练&#34;&gt;三、PyTorch 模型训练&lt;/h3&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;施工中&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;参考资料：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/datasets/sagnikseal/household-power-consumption&#34; target=&#34;_blank&#34;&gt;Household_Power_consumption&lt;/a&gt; (dataset)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/code/akdagmelih/ocean-wave-prediction-with-lstm/notebook&#34; target=&#34;_blank&#34;&gt;Ocean Wave Prediction with LSTM&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/code/amirrezaeian/time-series-data-analysis-using-lstm-tutorial/notebook&#34; target=&#34;_blank&#34;&gt;Time-series data analysis using LSTM (Tutorial)&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.kanaries.net/pygwalker#a-python-library-for-exploratory-data-analysis-with-visualization---pygwalkers&#34; target=&#34;_blank&#34;&gt;PyGWalker&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/luochang212/rnn-note&#34; target=&#34;_blank&#34;&gt;luochang212/rnn-note&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div id=&#34;mini-overlay&#34; onclick=&#34;overlay_off()&#34;&gt;&lt;/div&gt;&#xA;&lt;div id=&#34;mini-window&#34;&gt;&lt;iframe id=&#34;mini-iframe&#34; frameBorder=&#34;0&#34;&gt;&lt;/iframe&gt;&lt;/div&gt;&#xA;&lt;button id=&#34;btn-close&#34; onclick=&#34;overlay_off()&#34;&gt;×&lt;/button&gt;&#xA;&lt;script src=&#34;https://luochang212.github.io/python-tips/overlay.js&#34;&gt;&lt;/script&gt;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://luochang212.github.io/python-tips/style.css&#34;&gt;</description>
    </item>
    <item>
      <title>手写深度学习</title>
      <link>https://luochang212.github.io/posts/d2l_from_scratch/</link>
      <pubDate>Fri, 09 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/d2l_from_scratch/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;与其说深度学习是一门技术，不如说深度学习是一种语言&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/AI-Project/tree/main/scratch&#34; target=&#34;_blank&#34;&gt;AI-Project/scratch&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;一自动微分&#34;&gt;一、自动微分&lt;/h3&gt;&#xA;&lt;h4 id=&#34;1-简单的例子&#34;&gt;1. 简单的例子&lt;/h4&gt;&#xA;&lt;p&gt;1.1 张量 x 的梯度&lt;/p&gt;&#xA;&lt;p&gt;张量 &lt;code&gt;$x$&lt;/code&gt; 的梯度可以存储在 &lt;code&gt;$x$&lt;/code&gt; 上。&lt;/p&gt;&#xA;&lt;p&gt;要点：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;x.grad&lt;/code&gt;: 取 &lt;code&gt;$x$&lt;/code&gt; 的梯度&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;x.requires_grad_(True)&lt;/code&gt;: 允许 tenser &lt;code&gt;$x$&lt;/code&gt; 存储自己的梯度&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;x.grad.zero_()&lt;/code&gt;: 将 &lt;code&gt;$x$&lt;/code&gt; 的梯度置零&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import torch&#xA;&#xA;# 初始化张量 x (tensor x)&#xA;x = torch.arange(4.0)&#xA;&#xA;x.requires_grad_(True)  # 允许 tensor x 存储梯度&#xA;x.grad is None  # 梯度默认为 None&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;&amp;gt; True&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;初始化带梯度的张量，下面是两个例子：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;torch.tensor([1., 2., 3.], requires_grad=True)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;&amp;gt; tensor([1., 2., 3.], requires_grad=True)&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;torch.randn((2, 5), requires_grad=True)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;&amp;gt; tensor([[ 0.4075,  1.1930,  0.5716, -1.0924,  0.0653], [-1.2869,  1.5768,  1.3445,  0.6309, -0.0484]], requires_grad=True)&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>深度学习笔记</title>
      <link>https://luochang212.github.io/posts/d2l/</link>
      <pubDate>Sat, 11 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/d2l/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;跟李沐老师学深度学习，课程见 &lt;a href=&#34;https://zh-v2.d2l.ai/index.html&#34;&gt;d2l&lt;/a&gt;，如有错误，欢迎拍砖 &lt;img src=&#34;https://luochang212.github.io/img/quyin/witty.png&#34; class=&#34;my-emoji&#34; style = &#34;height: 32px;&#34;&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/AI-Project&#34; target=&#34;_blank&#34;&gt;AI-Project&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;技术路线图&#34;&gt;〇、技术路线图&lt;/h3&gt;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;flowchart TD&#xA;    A[softmax 回归] --&gt;|无法拟合 XOR 函数| B[多层感知机]&#xA;    B --&gt; |高像素图片作为输入，模型参数爆炸| C[卷积]&#xA;    C --&gt;|数据的长宽下降太快| D[填充]&#xA;    C --&gt;|数据的长宽下降太慢| E[步幅]&#xA;    C --&gt;|缓解卷积对位置敏感| F[池化]&#xA;    C --&gt;|多模式识别与组合| G[多通道输入/输出]&#xA;&#xA;&lt;/pre&gt;&#xA;&lt;center&gt;❤️ powered by &lt;a href=&#34;https://github.com/mermaid-js/mermaid&#34; target=&#34;_blank&#34;&gt;mermaid&lt;/a&gt;&lt;/center&gt;&#xA;&lt;h3 id=&#34;一softmax-回归&#34;&gt;一、softmax 回归&lt;/h3&gt;&#xA;&lt;h4 id=&#34;1-虽然叫回归但是softmax-解决的是分类问题&#34;&gt;1. 虽然叫回归，但是softmax 解决的是分类问题&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;回归估计是一个连续值&lt;/li&gt;&#xA;&lt;li&gt;分类预测是一个离散类别&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;2-分类应用举例&#34;&gt;2. 分类应用举例&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MINIST&lt;/li&gt;&#xA;&lt;li&gt;ImageNet&lt;/li&gt;&#xA;&lt;li&gt;human-protein-atlas-image-classification (&lt;strong&gt;Kaggle&lt;/strong&gt;)&lt;/li&gt;&#xA;&lt;li&gt;malware-classification (&lt;strong&gt;Kaggle&lt;/strong&gt;)&lt;/li&gt;&#xA;&lt;li&gt;jigsaw-comment-classification (&lt;strong&gt;Kaggle&lt;/strong&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;3-从回归到多类分类--均方损失&#34;&gt;3. 从回归到多类分类 &amp;ndash; 均方损失&lt;/h4&gt;&#xA;&lt;p&gt;对分类结果做 one-hot 编码：&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$y = [y_1, y_2, , ... , y_n]^T$&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$y_i=\left\{\begin{array}{l}1 \text { if } i=y \\ 0 \text { otherwise }\end{array}\right.$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
