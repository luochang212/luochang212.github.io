<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hadoop on Chang Luo</title>
    <link>http://luochang212.github.io/categories/hadoop/</link>
    <description>Recent content in Hadoop on Chang Luo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 25 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://luochang212.github.io/categories/hadoop/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>漫谈 Hadoop Streaming</title>
      <link>http://luochang212.github.io/posts/hadoop_intro/</link>
      <pubDate>Sat, 25 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>http://luochang212.github.io/posts/hadoop_intro/</guid>
      <description>【本文 Python 向】Hadoop Streaming 是 Hadoop 提供的官方工具，它为非 Java 语言提供编程接口，让诸如 Golang, Python, Bash 等其他语言的开发者也能享受用 Hadoop MapReduce 编程的乐趣。
 随着计算机的硬件性能逐渐至达瓶颈，再通过升级硬件的方式对运算性能的提升有限，于是人们开始寻求提升性能的软件方法，分布式计算就是在这种思路下应运而生。
Hadoop 是一种分布式计算框架，它是建立在计算机集群上的一套软件，用以协同各机器共同完成任务。
分布式计算是复杂的，它要考虑机器间交互、数据存储、分布式算法等一系列问题。Hadoop 存在的意义就是约定一个编程模型，大家按模型的要求跑就行。至于实现细节，丢给 Hadoop 处理就好了。因此虽然 Hadoop 的具体实现原理很复杂，但掌握 Hadoop 的这套编程模型却不难。
1. MapReduce 拖了这么久终于进入正题了。
上文中提到的这套编程模型名叫 MapReduce。MapReduce 描述了数据在 Hadoop 中是如何被处理的，处理过程可大致分为三个阶段：Map, Shuffle 和 Reduce。中间的 Shuffle 由 Hadoop 完成，因此我们只需要编写 Map 阶段和 Reduce 阶段对应的代码就好了。
1.1 三个阶段 下图标示了数据处理的三个阶段在数据流程中的位置，一左一右两个红圈分别代表 Map 和 Reduce 阶段，中间蓝框代表 Shuffle 阶段。
* 一般而言，map 阶段的对应程序叫 mapper，reduce 阶段叫 reducer。
三个阶段的作用分别是：
 Map：预处理。Shuffle 通常是要耗费大量的计算资源，因此传给 Shuffle 的数据越精简越好，这就要求我们在 Mapper 阶段把和研究问题无关的数据都筛掉。Mapper 在输出时，会指定一个或多个字段作为主键。</description>
    </item>
    
  </channel>
</rss>