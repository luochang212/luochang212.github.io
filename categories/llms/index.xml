<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLMs on Chang Luo</title>
    <link>https://luochang212.github.io/categories/llms/</link>
    <description>Recent content in LLMs on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 26 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/categories/llms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>三种方法实现监督微调 (SFT)：LLaMA Factory, trl 和 unsloth</title>
      <link>https://luochang212.github.io/posts/sft_note/</link>
      <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/sft_note/</guid>
      <description>试了目前流行的三种微调框架，最推荐的是 unsloth，因为快！LLaMA Factory 和 trl 是在夜里跑的，显卡风扇响了一宿。第二天看日志，发现它们跑了三小时才跑完。但是 unsloth 五分钟就跑完了，快得有些夸张了。当然，这么比不是完全公平的，因为它们的量化方法、LoRA 参数都是不同的。但是 unsloth 快这一点是无可质疑的。如果在 GPU 服务器上认真微调，那么用 LLaMA Factory 没毛病；但如果只是在笔记本上随便玩玩，unsloth 的优势就太大了！&#xA;GitHub 项目地址：sft-note&#xA;一、引言 大语言模型有很强的通用能力，但在特定领域，它的表现不如领域小模型。为了让大模型适应特定任务，我们对大模型进行微调，使大模型在保持通用性的同时，兼具领域模型的专业知识、对话风格和输出格式等特质。&#xA;微调大模型有三种范式：&#xA;无监督微调：在海量数据上进行二次预训练 PT 增量预训练 监督微调 (SFT)：构造领域数据集，增强模型的指令遵循能力，并注入领域知识 指令微调 强化学习微调：通过 reward 引导模型优化 RLHF 基于人类反馈的强化学习 DPO 直接偏好优化方法 ORPO 比值比偏好优化 GRPO 群体相对策略优化 本文聚焦 监督微调 (Supervised Fine-Tuning)。监督微调是一种简单但有效的微调方式，能够快速融合业务数据、适应业务场景，因此它的性价比极高！&#xA;1. SFT 的简单介绍 监督微调的优化目标是 最小化模型生成回答与目标回答之间的差异，通常使用交叉熵损失。为避免破坏预训练阶段获得的知识，SFT 阶段通常使用 较低的学习率，并且只更新部分参数层，其他参数保持不变。与预训练阶段所需的海量数据相比，SFT 只需 较小的数据量（数千到数十万样本），即可完成微调。&#xA;2. SFT 的使用场景 为了让大家感受一下 SFT 能做什么，下面列举一些使用场景：&#xA;任务 场景举例 类型 文案生成 输入标签：红色#女士#卫衣，输出文案：女士专属红色卫衣，解锁秋冬时尚密码 任务对齐 情感分类 输入用户评论：蓝牙连接不稳定，输出情感标签：负面 任务对齐 合同审核 输入合同文本，输出潜在法律风险，并引述法条和案例 知识迁移 3.</description>
    </item>
    <item>
      <title>本地部署大模型：Ollama 和 vLLM</title>
      <link>https://luochang212.github.io/posts/llm_deploy/</link>
      <pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/llm_deploy/</guid>
      <description>2023 年本地部署大模型的报价近千万，2024 年初便骤降至百万，如今是 2025 年，只需要一行 vLLM 命令就可以部署大模型，人工成本几近于零。&#xA;GitHub 项目地址：llm-deploy&#xA;本文内容包括：&#xA;3 种方式部署 DeepSeek R1：Ollama, vLLM 和 Transformers 使用 vLLM 部署 Qwen2.5 模型 安装 Open WebUI 作为本地模型的前端聊天框 通过 vllm serve 实现一行代码启动 vLLM 推理服务 ✨ 快速部署说明在 /deploy，vLLM 服务启动脚本在 /server.&#xA;一、本地部署 DeepSeek R1 大模型本地部署依赖推理引擎，目前比较流行的推理引擎有：&#xA;推理引擎 场景 介绍 Ollama 适合个人开发者和轻量级应用 基于 llama.cpp 开发，支持 CPU 推理，安装简单，开箱即用，适合快速原型开发和测试 vLLM 适合高并发生产环境 支持多 GPU 并行、批处理、PagedAttention，吞吐量高，延迟低，适合大规模服务部署 Transformers 适合模型研究和实验 提供完整的模型训练和推理接口，支持模型微调、量化、加速，适合研究人员和需要深度定制的场景 SGLang 适合需要复杂推理流程的场景 支持结构化输出、并行推理、流式输出，特别适合需要多轮对话和复杂推理的应用 LMDeploy 适合企业级部署和边缘计算 由上海人工智能实验室开发，提供完整的模型量化、加速和部署工具链，支持多种硬件平台，特别适合资源受限场景 下面介绍如何部署 Ollama, vLLM, Transformers 这三款推理引擎，简要部署步骤见本项目的 deploy 目录。</description>
    </item>
    <item>
      <title>Langflow 实现本地知识库</title>
      <link>https://luochang212.github.io/posts/langflow_rag_app/</link>
      <pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/langflow_rag_app/</guid>
      <description>本项目将用 langflow 实现一个本地知识库。&#xA;Langflow 是大模型可视化组件编排工具。它可以为大模型赋能，比如可以为大模型应用增加对话记忆、文档检索等等的功能。它基本上站到了 LangChain 类似的生态位。开发大模型应用的需求通常比较 flexible，在功能和性能都满足的条件下，Langflow 可以快速实现原型开发和模块复用，是目前的效率之选。&#xA;GitHub 项目地址：langflow-rag-app&#xA;本文的内容包括：&#xA;介绍 RAG 的相关概念 使用 Langflow 实现简单的知识库 使用 Langflow 实现带对话记忆功能的知识库 使用 Langflow 实现代码检查 (code review) 功能 ✨ 环境部署相关的脚本，我放在这里了 deploy.&#xA;一、RAG 的概念介绍 这一节，我们先介绍 RAG 的相关概念，&#xA;知识库 就像大语言模型的“小抄”。在回答你之前，大模型先瞅一眼小抄，看有没有和你的问题相关的内容。如果有，就会从知识库中取回相应的文本片段，再结合大模型自身的能力生成最终回答。&#xA;知识库使用了一种叫 RAG（检索增强生成） 的技术。通过 RAG，大模型可以检索我们给它的文档。比如我们给它数学、法律、金融相关的文档，它可以事先进行“消化”、“吸收”。当我们对它提问时，它就能够像真正的专家一样，结合这些领域知识回答问题。&#xA;目录：&#xA;提示词模板 RAG 技术 文本向量化 向量数据库 查看笔记 二、简单的 RAG 应用 本节我们完成一个简单的 RAG 应用。我们将一个文档向量化后，存入向量数据库中，然后用 deepseek-r1:1.5b 模型，整合 RAG 取回的内容后输出回答。&#xA;最终的 langflow 工作流如下：&#xA;目录：&#xA;环境准备 安装 Ollama 安装 langflow 安装 chroma langflow 搭建工作流 创建一个新的 Flow 初始界面 本地改造计划 启动 Ollama 服务 添加 Ollama Embeddings 组件 添加 Chroma DB 组件 添加 Ollama 组件 传入文档 查看笔记 三、带对话记忆功能的 RAG 通过添加 Messsage History 组件，就可以为 RAG 添加对历史对话的记忆。</description>
    </item>
  </channel>
</rss>
