<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLMs on Chang Luo</title>
    <link>https://luochang212.github.io/categories/llms/</link>
    <description>Recent content in LLMs on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/categories/llms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>本地部署大模型：Ollama 和 vLLM</title>
      <link>https://luochang212.github.io/posts/llm_deploy/</link>
      <pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/llm_deploy/</guid>
      <description>2023 年本地部署大模型的报价近千万，2024 年初便骤降至百万，如今是 2025 年，只需要一行 vLLM 命令就可以部署大模型，人工成几近于零。&#xA;GitHub 项目地址：llm-deploy&#xA;本文内容包括：&#xA;三种方式部署 DeepSeek R1：Ollama, vLLM 和 Transformers 使用 vLLM 部署 Qwen2.5-1.5B 模型 安装 Open WebUI 作为本地部署模型的前端聊天框 通过 vllm serve 实现一行代码启动 vLLM 推理服务 ✨ 快速部署说明在 /deploy，vLLM 服务启动脚本在 /server.&#xA;一、本地部署 DeepSeek R1 大模型本地部署依赖推理引擎，目前比较流行的推理引擎有：&#xA;引擎 场景 介绍 Ollama 适合个人开发者 基于 llama.cpp 开发，故能在 CPU 机器上使用 vLLM 适合高并发、低延迟的场景 支持多 GPU 并行、批处理，是 GPU 服务器部署的不二之选 Transformers 适合科研人员 既支持推理，也支持训练，可以深度自定义推理过程，但性能一般 下面介绍如何部署这三款推理引擎，简要部署步骤见本项目的 deploy 目录。&#xA;目录：&#xA;Ollama vLLM 安装 miniconda 安装 jupyterlab 为 vllm 创建虚拟环境，并绑定到 jupyterlab 安装 vLLM 及相关依赖 下载模型文件 启动 jupyterlab 验证 vLLM Transformers 查看笔记 二、本地部署 Qwen2.</description>
    </item>
    <item>
      <title>Langflow 实现本地知识库</title>
      <link>https://luochang212.github.io/posts/langflow_rag_app/</link>
      <pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/langflow_rag_app/</guid>
      <description>本项目将用 langflow 实现一个本地知识库。&#xA;Langflow 是大模型可视化组件编排工具。它可以为大模型赋能，比如可以为大模型应用增加对话记忆、文档检索等等的功能。它基本上站到了 LangChain 类似的生态位。开发大模型应用的需求通常比较 flexible，在功能和性能都满足的条件下，Langflow 可以快速实现原型开发和模块复用，是目前的效率之选。&#xA;GitHub 项目地址：langflow-rag-app&#xA;本文的内容包括：&#xA;介绍 RAG 的相关概念 使用 Langflow 实现简单的知识库 使用 Langflow 实现带对话记忆功能的知识库 使用 Langflow 实现代码检查 (code review) 功能 ✨ 环境部署相关的脚本，我放在这里了 deploy.&#xA;一、RAG 的概念介绍 这一节，我们先介绍 RAG 的相关概念，&#xA;知识库 就像大语言模型的“小抄”。在回答你之前，大模型先瞅一眼小抄，看有没有和你的问题相关的内容。如果有，就会从知识库中取回相应的文本片段，再结合大模型自身的能力生成最终回答。&#xA;知识库使用了一种叫 RAG（检索增强生成） 的技术。通过 RAG，大模型可以检索我们给它的文档。比如我们给它数学、法律、金融相关的文档，它可以事先进行“消化”、“吸收”。当我们对它提问时，它就能够像真正的专家一样，结合这些领域知识回答问题。&#xA;目录：&#xA;提示词模板 RAG 技术 文本向量化 向量数据库 查看笔记 二、简单的 RAG 应用 本节我们完成一个简单的 RAG 应用。我们将一个文档向量化后，存入向量数据库中，然后用 deepseek-r1:1.5b 模型，整合 RAG 取回的内容后输出回答。&#xA;最终的 langflow 工作流如下：&#xA;目录：&#xA;环境准备 安装 Ollama 安装 langflow 安装 chroma langflow 搭建工作流 创建一个新的 Flow 初始界面 本地改造计划 启动 Ollama 服务 添加 Ollama Embeddings 组件 添加 Chroma DB 组件 添加 Ollama 组件 传入文档 查看笔记 三、带对话记忆功能的 RAG 通过添加 Messsage History 组件，就可以为 RAG 添加对历史对话的记忆。</description>
    </item>
  </channel>
</rss>
