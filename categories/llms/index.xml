<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLMs on Chang Luo</title>
    <link>https://luochang212.github.io/categories/llms/</link>
    <description>Recent content in LLMs on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 26 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/categories/llms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>三种方法实现监督微调：LLaMA Factory, trl 和 unsloth</title>
      <link>https://luochang212.github.io/posts/sft_note/</link>
      <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/sft_note/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;尝试了三种流行的微调框架，其中最推荐的是 unsloth，因为快！另外两种框架 LLaMA Factory 和 trl 是在夜里跑的，显卡风扇响了一宿。第二天看日志，它们都跑了三个多小时才跑完。但是同样的任务，unsloth 只需要五分钟，快得有点离谱。当然，这么比不是完全公平的，因为它们的量化方法、LoRA 参数是不同的。但是 unsloth 快这一点依然是无可质疑的。如果在 GPU 服务器上认真微调，那么用 LLaMA Factory 没毛病；但如果只是在笔记本上随便玩玩，unsloth 的优势就太大了。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/sft-note&#34; target=&#34;_blank&#34;&gt;sft-note&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;⭐ 本文的内容包括：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;大模型微调的三种范式：无监督微调、监督微调、强化学习微调&lt;/li&gt;&#xA;&lt;li&gt;介绍用于监督微调的数据格式，以及如何加载数据集&lt;/li&gt;&#xA;&lt;li&gt;如何下载 Qwen 模型，代码见 &lt;a href=&#34;https://github.com/luochang212/sft-note/blob/main/model/download_qwen.py&#34; target=&#34;_blank&#34;&gt;download_qwen.py&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;使用三种框架微调大模型：LLaMA Factory, trl, unsloth&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;一引言&#34;&gt;一、引言&lt;/h2&gt;&#xA;&lt;p&gt;大语言模型有很强的通用能力，但在特定领域，它的表现不如领域小模型。为了让大模型适应特定任务，我们对大模型进行微调，使大模型在保持通用性的同时，兼具领域模型的专业知识、对话风格和输出格式等特质。&lt;/p&gt;&#xA;&lt;p&gt;微调大模型有三种范式：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;无监督微调&lt;/strong&gt;：在海量数据上进行二次预训练&#xA;&lt;ul&gt;&#xA;&lt;li&gt;PT 增量预训练&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;监督微调 (SFT)&lt;/strong&gt;：构造领域数据集，增强模型的指令遵循能力，并注入领域知识&#xA;&lt;ul&gt;&#xA;&lt;li&gt;指令微调&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;强化学习微调&lt;/strong&gt;：通过 reward 引导模型优化&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;RLHF&lt;/a&gt; 基于人类反馈的强化学习&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.18290&#34;&gt;DPO&lt;/a&gt; 直接偏好优化方法&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07691&#34;&gt;ORPO&lt;/a&gt; 比值比偏好优化&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.03300&#34;&gt;GRPO&lt;/a&gt; 群体相对策略优化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;本文聚焦 &lt;strong&gt;监督微调 (Supervised Fine-Tuning)&lt;/strong&gt;。监督微调是一种简单但有效的微调方式，能够快速融合业务数据、适应业务场景，因此它的性价比极高！&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-sft-的简单介绍&#34;&gt;1. SFT 的简单介绍&lt;/h3&gt;&#xA;&lt;p&gt;监督微调的优化目标是 &lt;strong&gt;最小化模型生成回答与目标回答之间的差异&lt;/strong&gt;，通常使用交叉熵损失。为避免破坏预训练阶段获得的知识，SFT 阶段通常使用 &lt;strong&gt;较低的学习率&lt;/strong&gt;，并且只更新部分参数层，其他参数保持不变。与预训练阶段所需的海量数据相比，SFT 只需 &lt;strong&gt;较小的数据量&lt;/strong&gt;（数千到数十万样本），即可完成微调。&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-sft-的使用场景&#34;&gt;2. SFT 的使用场景&lt;/h3&gt;&#xA;&lt;p&gt;为了让大家感受一下 SFT 能做什么，下面列举一些使用场景：&lt;/p&gt;</description>
    </item>
    <item>
      <title>本地部署大模型：Ollama 和 vLLM</title>
      <link>https://luochang212.github.io/posts/llm_deploy/</link>
      <pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/llm_deploy/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;2023 年本地部署大模型的报价近千万，2024 年初便骤降至百万，如今是 2025 年，只需要一行 vLLM 命令就可以部署大模型，人工成本几近于零。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/llm-deploy&#34; target=&#34;_blank&#34;&gt;llm-deploy&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;本文内容包括：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;3 种方式部署 DeepSeek R1：Ollama, vLLM 和 Transformers&lt;/li&gt;&#xA;&lt;li&gt;使用 vLLM 部署 Qwen2.5 模型&lt;/li&gt;&#xA;&lt;li&gt;安装 Open WebUI 作为本地模型的前端聊天框&lt;/li&gt;&#xA;&lt;li&gt;通过 &lt;code&gt;vllm serve&lt;/code&gt; 实现一行代码启动 vLLM 推理服务&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;✨ 快速部署说明在 &lt;a href=&#34;https://github.com/luochang212/llm-deploy/tree/main/deploy&#34; target=&#34;_blank&#34;&gt;/deploy&lt;/a&gt;，vLLM 服务启动脚本在 &lt;a href=&#34;https://github.com/luochang212/llm-deploy/tree/main/server&#34; target=&#34;_blank&#34;&gt;/server&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;一本地部署-deepseek-r1&#34;&gt;一、本地部署 DeepSeek R1&lt;/h2&gt;&#xA;&lt;p&gt;大模型本地部署依赖推理引擎，目前比较流行的推理引擎有：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;推理引擎&lt;/th&gt;&#xA;          &lt;th&gt;场景&lt;/th&gt;&#xA;          &lt;th&gt;介绍&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;适合个人开发者和轻量级应用&lt;/td&gt;&#xA;          &lt;td&gt;基于 &lt;a href=&#34;https://github.com/ggml-org/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; 开发，支持 CPU 推理，安装简单，开箱即用，适合快速原型开发和测试&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;适合高并发生产环境&lt;/td&gt;&#xA;          &lt;td&gt;支持多 GPU 并行、批处理、PagedAttention，吞吐量高，延迟低，适合大规模服务部署&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;适合模型研究和实验&lt;/td&gt;&#xA;          &lt;td&gt;提供完整的模型训练和推理接口，支持模型微调、量化、加速，适合研究人员和需要深度定制的场景&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;SGLang&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;适合需要复杂推理流程的场景&lt;/td&gt;&#xA;          &lt;td&gt;支持结构化输出、并行推理、流式输出，特别适合需要多轮对话和复杂推理的应用&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/InternLM/lmdeploy&#34;&gt;LMDeploy&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;适合企业级部署和边缘计算&lt;/td&gt;&#xA;          &lt;td&gt;由上海人工智能实验室开发，提供完整的模型量化、加速和部署工具链，支持多种硬件平台，特别适合资源受限场景&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;下面介绍如何部署 Ollama, vLLM, Transformers 这三款推理引擎，简要部署步骤见本项目的 &lt;a href=&#34;https://github.com/luochang212/llm-deploy/tree/main/deploy&#34;&gt;deploy&lt;/a&gt; 目录。&lt;/p&gt;&#xA;&lt;p&gt;目录：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Langflow 实现本地知识库</title>
      <link>https://luochang212.github.io/posts/langflow_rag_app/</link>
      <pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/langflow_rag_app/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本项目将用 &lt;a href=&#34;https://github.com/langflow-ai/langflow&#34; target=&#34;_blank&#34;&gt;langflow&lt;/a&gt; 实现一个本地知识库。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;Langflow 是大模型可视化组件编排工具。它可以为大模型赋能，比如可以为大模型应用增加对话记忆、文档检索等等的功能。它基本上站到了 LangChain 类似的生态位。开发大模型应用的需求通常比较 flexible，在功能和性能都满足的条件下，Langflow 可以快速实现原型开发和模块复用，是目前的效率之选。&lt;/p&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/langflow-rag-app&#34; target=&#34;_blank&#34;&gt;langflow-rag-app&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;本文的内容包括：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;介绍 RAG 的相关概念&lt;/li&gt;&#xA;&lt;li&gt;使用 Langflow 实现简单的知识库&lt;/li&gt;&#xA;&lt;li&gt;使用 Langflow 实现带对话记忆功能的知识库&lt;/li&gt;&#xA;&lt;li&gt;使用 Langflow 实现代码检查 (code review) 功能&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;✨ 环境部署相关的脚本，我放在这里了 &lt;a href=&#34;https://github.com/luochang212/langflow-rag-app/tree/main/deploy&#34; target=&#34;_blank&#34;&gt;deploy&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;一rag-的概念介绍&#34;&gt;一、RAG 的概念介绍&lt;/h2&gt;&#xA;&lt;p&gt;这一节，我们先介绍 RAG 的相关概念，&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;知识库&lt;/strong&gt; 就像大语言模型的“小抄”。在回答你之前，大模型先瞅一眼小抄，看有没有和你的问题相关的内容。如果有，就会从知识库中取回相应的文本片段，再结合大模型自身的能力生成最终回答。&lt;/p&gt;&#xA;&lt;p&gt;知识库使用了一种叫 &lt;strong&gt;RAG（检索增强生成）&lt;/strong&gt; 的技术。通过 RAG，大模型可以检索我们给它的文档。比如我们给它数学、法律、金融相关的文档，它可以事先进行“消化”、“吸收”。当我们对它提问时，它就能够像真正的专家一样，结合这些领域知识回答问题。&lt;/p&gt;&#xA;&lt;p&gt;目录：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;提示词模板&lt;/li&gt;&#xA;&lt;li&gt;RAG 技术&#xA;&lt;ul&gt;&#xA;&lt;li&gt;文本向量化&lt;/li&gt;&#xA;&lt;li&gt;向量数据库&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/langflow-rag-app/blob/main/1.road_map.ipynb&#39;)&#34;&gt;查看笔记&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h2 id=&#34;二简单的-rag-应用&#34;&gt;二、简单的 RAG 应用&lt;/h2&gt;&#xA;&lt;p&gt;本节我们完成一个简单的 RAG 应用。我们将一个文档向量化后，存入向量数据库中，然后用 deepseek-r1:1.5b 模型，整合 RAG 取回的内容后输出回答。&lt;/p&gt;&#xA;&lt;p&gt;最终的 langflow 工作流如下：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://luochang212.github.io/img/simple_rag_app.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;目录：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;环境准备&#xA;&lt;ul&gt;&#xA;&lt;li&gt;安装 Ollama&lt;/li&gt;&#xA;&lt;li&gt;安装 langflow&lt;/li&gt;&#xA;&lt;li&gt;安装 chroma&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;langflow 搭建工作流&#xA;&lt;ul&gt;&#xA;&lt;li&gt;创建一个新的 Flow&lt;/li&gt;&#xA;&lt;li&gt;初始界面&lt;/li&gt;&#xA;&lt;li&gt;本地改造计划&lt;/li&gt;&#xA;&lt;li&gt;启动 Ollama 服务&lt;/li&gt;&#xA;&lt;li&gt;添加 Ollama Embeddings 组件&lt;/li&gt;&#xA;&lt;li&gt;添加 Chroma DB 组件&lt;/li&gt;&#xA;&lt;li&gt;添加 Ollama 组件&lt;/li&gt;&#xA;&lt;li&gt;传入文档&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;br&gt;&#xA;&lt;center&gt;&#xA;&lt;button class=&#34;demo-btn&#34; onclick=&#34;window_on(&#39;https://nbviewer.org/github/luochang212/langflow-rag-app/blob/main/2.simple_rag_app.ipynb&#39;)&#34;&gt;查看笔记&lt;/button&gt;&#xA;&lt;/center&gt;&#xA;&lt;br&gt;&#xA;&lt;h2 id=&#34;三带对话记忆功能的-rag&#34;&gt;三、带对话记忆功能的 RAG&lt;/h2&gt;&#xA;&lt;p&gt;通过添加 Messsage History 组件，就可以为 RAG 添加对历史对话的记忆。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
