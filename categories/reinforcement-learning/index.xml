<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning on Chang Luo</title>
    <link>http://luochang212.github.io/categories/reinforcement-learning/</link>
    <description>Recent content in Reinforcement Learning on Chang Luo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 21 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="http://luochang212.github.io/categories/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>强化学习笔记</title>
      <link>http://luochang212.github.io/posts/reinforcement_learning/</link>
      <pubDate>Sat, 21 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>http://luochang212.github.io/posts/reinforcement_learning/</guid>
      <description>课程地址：李宏毅深度强化学习(国语)课程(2018)
一、Policy Gradient (Review) 1）Proximal Policy Optimization (PPO) Policy Gradient On-policy =&amp;gt; Off-policy Add constraint 2）Basic Components Actor: 角色，采取什么行动？ Env: 环境，当前状态下提供给Actor什么信息？ Reward Function: 给当前行为什么奖励？这个是难点 3）Policy of Actor Policy is a network
input: 游戏画面 output: 每个行为对应输出层中的一个神经元 一场游戏叫做一个 episode
4）Expected Reward 每个 episode 出现的概率：
$p\left(s_1\right) = \prod_{t=1}^T p_\theta\left(a_t \mid s_t\right) p\left(s_{t+1} \mid s_t, a_t\right)$
每个 episode 的奖励 R = episode 所有 action 的 reward 之和。公式表达为：
$R(\tau) = \sum_{t=1}^{T} r_t$
因为给定 Action 和 Env 时，给出的下一个 Env 有随机性，所以 R 是一个随机变量，无法计算 R 的真实值，但可以计算 R 的期望。</description>
    </item>
    
  </channel>
</rss>
