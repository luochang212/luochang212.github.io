<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on Chang Luo</title>
    <link>http://localhost:1313/categories/llm/</link>
    <description>Recent content in LLM on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Langflow 实现本地知识库</title>
      <link>http://localhost:1313/posts/langflow_rag_app/</link>
      <pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/langflow_rag_app/</guid>
      <description>本项目将用 langflow 实现一个本地知识库。&#xA;Langflow 是大模型可视化组件编排工具。它可以为大模型赋能，比如可以为大模型应用增加对话记忆、文档检索等等的功能。它基本上站到了 LangChain 类似的生态位。开发大模型应用的需求通常比较 flexible，在功能和性能都满足的条件下，Langflow 可以快速实现原型开发和模块复用，是目前的效率之选。&#xA;GitHub 项目地址：langflow-rag-app&#xA;本文的内容包括：&#xA;介绍 RAG 的相关概念 使用 Langflow 实现简单的知识库 使用 Langflow 实现带对话记忆功能的知识库 使用 Langflow 实现代码检查 (code review) 功能 ✨ 环境部署相关的脚本，我放在这里了 deploy.&#xA;一、RAG 的概念介绍 这一节，我们先介绍 RAG 的相关概念，&#xA;知识库 就像大语言模型的“小抄”。在回答你之前，大模型先瞅一眼小抄，看有没有和你的问题相关的内容。如果有，就会从知识库中取回相应的文本片段，再结合大模型自身的能力生成最终回答。&#xA;知识库使用了一种叫 RAG（检索增强生成） 的技术。通过 RAG，大模型可以检索我们给它的文档。比如我们给它数学、法律、金融相关的文档，它可以事先进行“消化”、“吸收”。当我们对它提问时，它就能够像真正的专家一样，结合这些领域知识回答问题。&#xA;目录：&#xA;提示词模板 RAG 技术 文本向量化 向量数据库 查看笔记 二、简单的 RAG 应用 本节我们完成一个简单的 RAG 应用。我们将一个文档向量化后，存入向量数据库中，然后用 deepseek-r1:1.5b 模型，整合 RAG 取回的内容后输出回答。&#xA;最终的 langflow 工作流如下：&#xA;目录：&#xA;环境准备 安装 Ollama 安装 langflow 安装 chroma langflow 搭建工作流 创建一个新的 Flow 初始界面 本地改造计划 启动 Ollama 服务 添加 Ollama Embeddings 组件 添加 Chroma DB 组件 添加 Ollama 组件 传入文档 查看笔记 三、带对话记忆功能的 RAG 通过添加 Messsage History 组件，就可以为 RAG 添加对历史对话的记忆。</description>
    </item>
  </channel>
</rss>
