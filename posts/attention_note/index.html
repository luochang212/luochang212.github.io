<!DOCTYPE html>
<html lang="zh-cn">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    
    <link rel="canonical" href="https://luochang212.github.io/posts/attention_note/">
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.148.2">

    
    
    

<title>注意力机制笔记 • Chang Luo</title>



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="注意力机制笔记">
  <meta name="twitter:description" content="智能产生的根源：并行、查询、压缩">
      <meta name="twitter:site" content="@_stellar_tide">

<meta property="og:url" content="https://luochang212.github.io/posts/attention_note/">
  <meta property="og:site_name" content="Chang Luo">
  <meta property="og:title" content="注意力机制笔记">
  <meta property="og:description" content="智能产生的根源：并行、查询、压缩">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-09T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-06-09T00:00:00+00:00">
    <meta property="article:tag" content="MultiHeadAttention">
    <meta property="article:tag" content="PositionalEncoding">


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.1354666f7c97c95b395dd180ab9cf1aca1ff0408c93996eed446576738e01579.css" integrity="sha256-E1Rmb3yXyVs5XdGAq5zxrKH/BAjJOZbu1EZXZzjgFXk=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">




<link rel="stylesheet" href="/scss/tocbot.5ef07cebc3c477b54270456f149ee02922479bb7555fd344b2c69f953b0e7e5e.css" integrity="sha256-XvB868PEd7VCcEVvFJ7gKSJHm7dVX9NEssaflTsOfl4=">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    
    

</head>


    <body class=" ">
    <head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://upcdn.b0.upaiyun.com/libs/jquery/jquery-1.9.0.min.js"></script>

  <style>
    body {
      transition: background-color .5s;
    }

    #overlay {
      position: fixed;
      display: none;
      width: calc(100vw - 580px);
      height: 100%;
      top: 0;
      left: 0;
       
      z-index: 2;
      cursor: pointer;
    }

    .sidenav {
      height: 100%;
      width: 0;
      position: fixed;
      z-index: 1000;
      top: 0;
      right: 0;
      background-color: #FFF;
      overflow: hidden;
      transition: 0.5s;
    }

    #main {
      transition: margin-left .5s;
       
      padding: 16px;
      position: absolute;
      top: 0;
      right: 0;
      bottom: 0;
      left: 0;
      overflow: hidden;
    }

     
    .tab {
      overflow: hidden;
      border: 1px solid #ccc;
      background-color: #f1f1f1;
    }

     
    .tab button {
      background-color: inherit;
      float: left;
      border: none;
      outline: none;
      cursor: pointer;
      padding: 14px 16px;
      transition: 0.3s;
      font-size: 17px;
    }

     
    .tab button:hover {
      background-color: #ddd;
    }

     
    .tab button.active {
      background-color: #ccc;
    }

     
    .tabcontent {
      display: flex;
      padding: 12px 12px;
      margin: 0;
      border: 1px solid #ccc;
      border-top: none;
      overflow: hidden;
      height: calc(100vh - 90px);
    }

    .dropbtn {
      background-color: #3498DB;
      color: white;
      padding: 16px;
      font-size: 16px;
      border: none;
    }

    .dropup {
      position: relative;
      display: inline-block;
    }

    .dropup-content {
      display: none;
      position: absolute;
      background-color: #f1f1f1;
      min-width: 160px;
      bottom: 50px;
      z-index: 1;
    }

    .dropup-content a {
      color: black;
      padding: 12px 16px;
      text-decoration: none;
      display: block;
    }

    .dropup-content a:hover {
      background-color: #ccc
    }

    .dropup:hover .dropup-content {
      display: block;
    }

    .dropup:hover .dropbtn {
      background-color: #2980B9;
    }

    #video2,
    #video3 {
      display: none;
    }

    .myiframe {
      height: 100%;
      width: 100%;
    }
  </style>

</head>

<body>

  
  <div class="sidebar">
    <div class="container ">
      <div class="sidebar-about">
        <span class="site__title">
          <a href="https://luochang212.github.io/">Chang Luo</a>
        </span>
        
        
        
        <div class="author-image">
          <a href="#" class="load-iframe" onclick="openNav()"><img src="https://luochang212.github.io//images/profile.webp" id="author-iamge"
              alt="Author Image" class="img--circle img--headshot element--center"></a>
        </div>
        
        <p class="site__description">
          
        </p>
      </div>
      <div class="collapsible-menu">
        <input type="checkbox" id="menuToggle">
        <label for="menuToggle">Chang Luo</label>
        <div class="menu-content">
          <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/portfolio/">
						<span>Projects</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/about/">
						<span>About Me</span>
					</a>
				</li>
			 
		
	</ul>
</div>

          <section class="social">
	
	<a href="https://twitter.com/_stellar_tide" rel="me"><i class="fab fa-x-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	<a href="https://github.com/luochang212" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	<a href="https://instagram.com/suphenshin" rel="me"><i class="fab fa-instagram fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	<a href="https://www.zhihu.com/people/Fashionable" rel="me">
		<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="2" 
			 stroke-linecap="round" stroke-linejoin="round" style="width: 1.25em; height: 1.25em; vertical-align: -0.25em;">
			<path d="m13.3334,3.60098l0,17.1471l1.79582,0l0.75424,2.13703l3.18459,-2.13703l3.93584,0l0,-17.1471l-9.6705,0zm7.41375,14.87538l-1.79283,0l-2.24777,1.50849l-0.53276,-1.50849l-0.53875,0l0,-12.53483l5.10911,0l0,12.53483l0.00299,0zm-8.56906,-7.18927l-3.97475,0c0.06285,-1.34387 0.1287,-3.12174 0.19754,-5.17496l3.91788,0l-0.00299,-0.24244c0,-0.01796 -0.00599,-0.43998 -0.06884,-0.87097c-0.06285,-0.44896 -0.19754,-1.04457 -0.62854,-1.04457l-6.5727,0c0.13169,-0.61657 0.46991,-2.08615 0.87995,-2.80747l0.19155,-0.33522l-0.3861,-0.02095c-0.02394,0 -0.58663,-0.02694 -1.23912,0.31726c-1.06851,0.56868 -1.5474,1.68807 -1.75691,2.52612c-0.55072,2.18791 -1.33489,3.70837 -1.66712,4.35786c-0.09877,0.19155 -0.15863,0.30529 -0.18557,0.38311c-0.05387,0.14666 -0.02394,0.29332 0.0838,0.38909c0.31427,0.28434 1.14334,-0.0868 1.15232,-0.08979c0.01796,-0.00898 0.03891,-0.01796 0.06585,-0.02993c0.41603,-0.18856 1.64916,-0.74826 2.08914,-2.52911l1.69705,0c0.02095,0.96376 0.09278,4.14236 0.0868,5.17496l-4.22018,0l-0.06285,0.0449c-0.69139,0.50582 -0.91288,1.8916 -0.92185,1.95146l-0.0419,0.27536l4.99837,0c-0.36814,2.34355 -0.79315,3.3941 -1.01763,3.81313c-0.11074,0.20951 -0.21849,0.41902 -0.32025,0.62255c-0.63752,1.26306 -1.29898,2.56802 -3.7802,4.5973c-0.10775,0.0838 -0.20951,0.23944 -0.14367,0.41005c0.07183,0.18856 0.27835,0.27237 0.73629,0.27237c0.16162,0 0.35318,-0.00898 0.58065,-0.02993c1.49352,-0.13169 3.01698,-0.53875 4.04359,-2.6219c0.50882,-1.05056 0.94879,-2.14601 1.31394,-3.25941l4.08549,4.78886l0.14965,-0.35916c0.02394,-0.05687 0.56868,-1.38578 0.15264,-2.87032l-0.01497,-0.05387l-3.23547,-3.68143l-0.65847,0.49684c0.19155,-0.78118 0.31726,-1.49352 0.37413,-2.12805l4.74995,0l0,-0.23944c0,-1.20021 -0.55371,-1.91255 -0.57466,-1.94248l-0.07183,-0.08979z" />
		</svg>
	</a>
	
	
	
	
	
	
	
	
	
	
	
	
	
</section>

        </div>
      </div>
      
<div class="copyright">
  &copy; 2025 Chang Luo
  
</div>



    </div>
  </div>

  
  

  <div id="overlay" onclick="off()"></div>

  <div id="mySidenav" class="sidenav">

    <div id="main">
      <div class="tab">
        <button id="default-tab" class="tablinks" onclick="openItem(event, 'Resources')">Resources</button>
        <button class="tablinks" onclick="openItem(event, 'netease-player')">Music</button>
        <button class="tablinks" onclick="openItem(event, 'video-player')">Video</button>
        <button class="tablinks" onclick="openItem(event, 'gallery')">Gallery</button>
        <button class="tablinks" onclick="openItem(event, 'gadget')">Gadget</button>
        <button class="tablinks" onclick="openItem(event, 'categories')">Cats</button>
      </div>

      <div id="Resources" class="tabcontent">
        <iframe class="myiframe"
          src="about:blank"
          data-src="/resources/"
          frameborder="0"
          marginheight="0"
          marginwidth="0"
          scrolling="auto"></iframe>
      </div>

      <div id="netease-player" class="tabcontent">
        <iframe class="myiframe"
          src="about:blank"
          data-src="//music.163.com/outchain/player?type=0&id=2250585392&auto=0&height=430"
          frameborder="no"
          marginheight="0"
          marginwidth="0"
          scrolling="auto"></iframe>
      </div>

      <div id="video-player" class="tabcontent">
        <div id="video1">
          <iframe src="about:blank"
            data-src="//player.bilibili.com/player.html?bvid=BV1Ln4y1d7Z9&p=1&high_quality=1&danmaku=0&autoplay=0"
            style="width:100%; height: 500px"
            scrolling="no"
            frameborder="no"
            framespacing="0"
            allowfullscreen="true"></iframe>
        </div>
        <div id="video2">
          <iframe src="about:blank"
            data-src="//player.bilibili.com/player.html?bvid=BV1rp4y1L7i4&p=1&high_quality=1&danmaku=0&autoplay=0"
            style="width:100%; height: 500px"
            scrolling="no"
            frameborder="no"
            framespacing="0"
            allowfullscreen="true"></iframe>
        </div>
        <div id="video3">
          <iframe src="about:blank"
            data-src="//player.bilibili.com/player.html?bvid=BV1Av4y147QE&p=1&high_quality=1&danmaku=0&autoplay=0"
            style="width:100%; height:500px"
            scrolling="no"
            frameborder="no"
            framespacing="0"
            allowfullscreen="true"></iframe>
        </div>

        

        

        <br>
        <div class="dropup">
          <button class="dropbtn">更多视频</button>
          <div class="dropup-content">
            <a onclick="link1()">ZUTOMYAO</a>
            <a onclick="link2()">岁岁恋</a>
            <a onclick="link3()">真物论</a>
          </div>
        </div>
      </div>

      <div id="gallery" class="tabcontent">
        <iframe class="myiframe"
        src="about:blank"
        data-src="/gadget/gallery/#two"
        frameborder="0"
        marginheight="0"
        marginwidth="0"
        scrolling="auto"></iframe>
      </div>

      <div id="gadget" class="tabcontent">
        <iframe class="myiframe"
        src="about:blank"
        data-src="/tool/"
        frameborder="0"
        marginheight="0"
        marginwidth="0"
        scrolling="auto"></iframe>
      </div>

      <div id="categories" class="tabcontent">
        <iframe class="myiframe"
        src="about:blank"
        data-src="/categories/"
        frameborder="0"
        marginheight="0"
        marginwidth="0"
        scrolling="auto"></iframe>
      </div>
    </div>
  </div>

  
  <script>
    'use strict'

    function openNav() {
      on();
      document.getElementById("mySidenav").style.width = "580px";
      document.body.style.backgroundColor = "rgba(0,0,0,0.5)";
    }

    function closeNav() {
      document.getElementById("mySidenav").style.width = "0";
      document.body.style.backgroundColor = "white";
    }

    function on() {
      document.getElementById("overlay").style.display = "block";
    }

    function off() {
      closeNav();
      document.getElementById("overlay").style.display = "none";
    }

    
    document.getElementById('default-tab').click();

    function openItem(evt, itemName) {
      var i, tabcontent, tablinks;
      tabcontent = document.getElementsByClassName("tabcontent");
      for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
      }
      tablinks = document.getElementsByClassName("tablinks");
      for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
      }
      document.getElementById(itemName).style.display = "block";
      evt.currentTarget.className += " active";
    }

    function link1() {
      document.getElementById("video1").style.display = "block";
      document.getElementById("video2").style.display = "none";
      document.getElementById("video3").style.display = "none";
    }

    function link2() {
      document.getElementById("video1").style.display = "none";
      document.getElementById("video2").style.display = "block";
      document.getElementById("video3").style.display = "none";
    }

    function link3() {
      document.getElementById("video1").style.display = "none";
      document.getElementById("video2").style.display = "none";
      document.getElementById("video3").style.display = "block";
    }

     
    var flag = 0;

     
    $(".load-iframe").click(function () {
      if (flag < 1) {
        $("iframe").each(function () {
          $(this).attr("src", $(this).data("src"));
        });
        flag = flag + 1;
      }
    });
  </script>

</body>



        <div class="content container">
            
    <style>
  .my-emoji {
    height: 25px;
    display: inline-block;
    margin: 0;
    vertical-align: text-bottom;
  }

  .social-links a {
    color: rgba(120,120,120,1);
    text-decoration: none;
    outline: none !important;
    cursor: pointer;
    transition-property: padding, text-shadow, line-height, color, background, opacity, transform;
    transition-duration: .25s;
    transition-timing-function: ease-in-out;
    display: inline-block;
    white-space: nowrap;
    position: relative;
  }

  .social-links a:after {
    content: "";
    display: block;
    height: .125rem;
    margin: -.125rem 0 0;
    background: rgba(0,0,0,.5);
    border-radius: 1px;
    transform: scaleX(0);
    transition: transform .25s ease-in-out;
  }

  .social-links a:hover:after {
    transform: scaleX(1);
  }

  .social-links a:hover {
    color: rgba(120,120,120,1);
  }

  .post {
    margin-bottom: 0;
  }

  .post-footer {
    margin-top: 0;
    padding-top: 0.5rem;
  }
</style>



<article>
  <header>
    <h1>注意力机制笔记</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Jun 9, 2024
    
    
    
      
      
          in
          
          
              <a class="badge badge-category" href="/categories/deeplearning">DEEPLEARNING</a>
              
          
      
    
    
    
      
      
          <br/>
           <i class="fas fa-tags"></i>
          
          <a class="badge badge-tag" href="/tags/multiheadattention">multiheadattention</a>
           
      
          <a class="badge badge-tag" href="/tags/positionalencoding">positionalencoding</a>
          
      
    
    
    <br/>
    <i class="fas fa-clock"></i> 2 min read
</div>


  </header>
  
  
  
  <div class="toc-wrapper">
    <input type="checkbox" id="tocToggle">
    <label for="tocToggle">目录</label>
    
    <div class="toc" id="TableOfContents"></div>
    
  </div>
  
  
  <div class="post">
    <blockquote>
<p><a href="https://people.eecs.berkeley.edu/~yima/" target="_blank">马毅</a>：要想正确理解深度神经网络，就必须认识到其本质是学习高维数据中的低维结构的手段。从第一性原理出发，把目的和手段分清楚，其余的都很容易被统一、被解释。</p></blockquote>
<p>从 Attention 的角度理解马毅老师这句话，Embedding 的时候本来就升维了，再做 QKV 就相当于在高维里面抽低维信息。而且 Q 也是可学习的，所以就既能学到好的抽取方法；对于每一种抽取方法，又能特别高效地抽取。</p>
<p>GitHub 项目地址：<a href="https://github.com/luochang212/rnn-note" target="_blank">rnn-note</a> / <a href="https://github.com/luochang212/attention-note" target="_blank">attention-note</a></p>
<h3 id="一语言模型入门rnn-lstm-gru">一、语言模型入门：RNN, LSTM, GRU</h3>
<h4 id="11-序列模型">1.1 序列模型</h4>
<p>马尔可夫假设，当前数据只跟最近 τ 个数据点相关。把最近 τ 个数据点作为特征，用 MLP 预测当前数据点的值。</p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/rnn-note/blob/main/1.序列模型.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="12-文本预处理">1.2 文本预处理</h4>
<p>对文本词元化 (tokenize) 并构建词表，就是把文本映射到从 0 开始的索引。</p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/rnn-note/blob/main/2.文本预处理.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="13-语言模型和数据集">1.3 语言模型和数据集</h4>
<p>对语料分批量 (batch) 处理。介绍了两种（batch 内的）采样策略：</p>
<ul>
<li>随机采样策略：每个 batch 内的相邻子序列是随机的</li>
<li>顺序分区策略：每个 batch 内的相邻子序列是顺序的</li>
</ul>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/rnn-note/blob/main/3.语言模型和数据集.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="14-循环神经网络的从零开始实现">1.4 循环神经网络的从零开始实现</h4>
<p>每次输出仅由前一个隐状态和当前新输入 x 决定，是为 RNN。</p>
<p><img src="/img/rnn_flow.svg" alt=""></p>
<p>提及的知识点：</p>
<ul>
<li><strong>独热编码</strong>：文本经过词元化后，还要经过 one-hot 处理，才能进入模型</li>
<li><strong>困惑度</strong>：我们用困惑度来描述文本生成的质量，通过一个序列中所有的 n 个词元的交叉熵损失的平均值来衡量
$$\frac{1}{n} \sum_{t=1}^n-\log P\left(x_t \mid x_{t-1}, \ldots, x_1\right)$$</li>
<li><strong>梯度裁剪</strong>：对于 $T$ 长序列将产生 $O(T)$ 长矩阵乘法链。当 $T$ 较大时，可能导致数值不稳定，例如可能导致梯度爆炸或梯度消失。这种情况下优化算法可能无法收敛。下式通过将梯度 $g$ 投影回给定半径 $\theta$ 来限制梯度的大小。其中 $\frac{\theta}{|\mathbf{g}|}$ 可以理解为梯度 $g$ 的单位方向向量。
$$\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{|\mathbf{g}|}\right) \mathbf{g}$$</li>
</ul>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/rnn-note/blob/main/4.循环神经网络的从零开始实现.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="15-循环神经网络的简洁实现">1.5 循环神经网络的简洁实现</h4>
<p>使用高级 API 实现循环神经网络。</p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/rnn-note/blob/main/5.循环神经网络的简洁实现.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="16-门控循环单元">1.6 门控循环单元</h4>
<p>”门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控。 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 这些机制是可学习的。“</p>
<p><img src="/img/gru_flow.svg" alt=""></p>
<p>为了解决长期记忆依赖的问题，使用两个门和一个候选隐状态：</p>
<ul>
<li>重置门 $\mathbf{R}_t$</li>
<li>更新门 $\mathbf{Z}_t$</li>
<li>候选隐状态 $\mathbf{H}_t$</li>
</ul>
<p>重置门、更新门由上一轮隐状态和当前新输入 x 决定；候选隐状态由上一轮隐状态、当前新输入 x 和重置门决定；本轮产出的隐状态由更新门、上一轮隐状态、候选隐状态决定。</p>
<p>公式表示如下：
<code>$$\begin{aligned} \mathbf{R}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r)\\ \mathbf{Z}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z)\\ \tilde{\mathbf{H}}_t &amp;= \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h)\\ \mathbf{H}_t &amp;= \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t \end{aligned}$$</code></p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/rnn-note/blob/main/6.门控循环单元.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="17-长短期记忆网络">1.7 长短期记忆网络</h4>
<p>长短期记忆网络 LSTM 比 GRU 复杂一点，效果也好一点。</p>
<p><img src="/img/lstm_flow.svg" alt=""></p>
<ul>
<li>输入门：和候选记忆元一起更新 记忆元</li>
<li>遗忘门：更新 记忆元</li>
<li>输出门：和 记忆元 一起更新 隐状态</li>
</ul>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/rnn-note/blob/main/7.长短期记忆网络.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="18深度循环神经网络">1.8.深度循环神经网络</h4>
<p>在输入和输出之间，添加多层隐状态。</p>
<p><img src="/img/deep_rnn_flow.svg" alt=""></p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/rnn-note/blob/main/8.深度循环神经网络.ipynb')">查看笔记</button>
</center>
<br>
<h3 id="二注意力机制">二、注意力机制</h3>
<h4 id="21-机器翻译数据集">2.1 机器翻译数据集</h4>
<p>“通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。”</p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/attention-note/blob/main/1.机器翻译数据集.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="22-编码器-解码器架构">2.2 编码器-解码器架构</h4>
<p>从不定长序列到不定长序列：</p>
<ul>
<li>编码器（encoder）： 它接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。</li>
<li>解码器（decoder）： 它将固定形状的编码状态映射到长度可变的序列。</li>
</ul>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/attention-note/blob/main/2.编码器-解码器架构.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="23-序列到序列学习">2.3 序列到序列学习</h4>
<p>使用两个循环神经网络设计一个序列到序列模型。</p>
<p>知识点：</p>
<ul>
<li>掩蔽填充词，将填充词元的预测排除在损失函数的计算之外</li>
<li>使用 BLEU 衡量翻译质量的好坏</li>
</ul>
<p><img src="/img/seq2seq_flow.svg" alt=""></p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/attention-note/blob/main/3.序列到序列学习.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="24-注意力提示">2.4 注意力提示</h4>
<p>对于一个有杯子、书本、窗户的场景</p>
<ul>
<li>非自主性提示：杯子特别红</li>
<li>自主性提示：想看书、想看看窗外</li>
</ul>
<p>其实就是不依赖数据本身的统计信息，而是主动查询来获取信息。本节引出 QKV 的概念。</p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/attention-note/blob/main/4.注意力提示.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="25-注意力汇聚nadaraya-watson-核回归">2.5 注意力汇聚：Nadaraya-Watson 核回归</h4>
<p>使用注意力机制解回归问题。分别试验了非参模型和带参模型的效果。</p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/attention-note/blob/main/5.注意力汇聚：Nadaraya-Watson核回归.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="26-注意力评分函数">2.6 注意力评分函数</h4>
<p>注意力评分函数是 q 和 k 的函数，本质是对查询和键之间的关系建模。</p>
<p>本节介绍了两个注意力评分函数：</p>
<ul>
<li>加性注意力：用 MLP 做</li>
<li>缩放点积注意力：用点积做，因此要求查询和键具有相同长度 $d$</li>
</ul>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/attention-note/blob/main/6.注意力评分函数.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="27-bahdanau-注意力">2.7 Bahdanau 注意力</h4>
<p>受对齐思想启发，Bahdanau 注意力将加性注意力加入解码器。</p>
<p>每个解码时间步，模型会根据当前解码器状态计算一个权重分布，用于加权求和编码器的所有隐藏状态，生成一个上下文向量。这意味着每次生成新词时，模型可以关注输入序列的不同部分。</p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/attention-note/blob/main/7.Bahdanau注意力.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="28-多头注意力">2.8 多头注意力</h4>
<p>为了使用多个注意力汇聚，多头注意力块用 h 组不同的线性投影 (linear projections) 来变换查询、键和值。然后用变换后的 QKV 将并行地进行注意力汇聚。最后，将 h 个注意力汇聚的输出 concat 在一起，并通过另一个可以学习的线性投影 (逐位前馈网络, FFN) 进行变换，以产生最终输出。</p>
<p><img src="/img/multi_head_flow.svg" alt=""></p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/attention-note/blob/main/8.多头注意力.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="29-自注意力和位置编码">2.9 自注意力和位置编码</h4>
<p>当注意力汇聚的 QKV 均来自同一组输入时，这种注意力被称为自注意力。为了让神经网络既能并行地处理词元，又能学到序列信息，Transformer 通过在输入中添加位置编码来注入绝对或者相对信息。</p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/attention-note/blob/main/9.自注意力和位置编码.ipynb')">查看笔记</button>
</center>
<br>
<h4 id="210-transformer">2.10 Transformer</h4>
<p>Transformer 作为编码器-解码器架构的一种实现，其整体架构如下所示。注意到图中出现了三个注意力块，接近输入的两个，是自注意力。连接编码器和解码器的，是编码器-解码器注意力。</p>
<p><img src="/img/transformer_flow.svg" alt=""></p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/attention-note/blob/main/10.transformer.ipynb')">查看笔记</button>
</center>
<br>
<h3 id="问题列表">问题列表</h3>
<ul>
<li>transformer架构中都有哪些层？</li>
<li>自注意力机制在transformer中的作用？</li>
<li>自注意力在transformer架构中的位置？</li>
<li>transformer架构中，都有哪些构件用到了注意力机制？</li>
<li>缩放点积注意力做的是点积，加性注意力做的数学运算是什么？不是点积吗？</li>
<li>在加性注意力中q和k的维度可以不同吗？在点积放缩注意力中q和k的维度可以不同吗？</li>
<li>多头自注意力机制如何平衡多个头的输出信息？</li>
<li>可以把多头注意力跟CNN中的通道做类比吗？两者有什么差别？</li>
<li>自注意力的输入是否有固定的长度限制？</li>
<li>实际训练时，样本长度总会有一个限制，一般限制是多少？</li>
<li>长距离依赖最远能多远，由什么参数决定的？</li>
<li>相邻样本的位置编码是否也比较相近？这会导致采样策略影响模型效果吗？</li>
<li>自注意力计算的时间复杂度是多少？</li>
<li>TPU相比GPU有什么优势？</li>
<li>时至今日，深度学习本质还是学习线性变换w吗？数学上是否有更精妙的设计？</li>
<li>transformer做多头注意力时，多个头的qkv共用线性变换 W_q W_k W_v 吗？</li>
</ul>
<p>参考：</p>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a></li>
<li><a href="http://zh-v2.d2l.ai/chapter_attention-mechanisms/index.html" target="_blank">zh-v2.d2l.ai</a></li>
<li><a href="https://cvpr2024-tutorial-low-dim-models.github.io/?continueFlag=c5a6aa2d606ab528733a659adc47aaf0" target="_blank">Learning Deep Low-Dimensional Models from High-Dimensional Data: From Theory to Practice</a></li>
<li><a href="https://github.com/ridgerchu/matmulfreellm" target="_blank">ridgerchu / matmulfreellm</a></li>
</ul>
<div id="mini-overlay" onclick="overlay_off()"></div>
<div id="mini-window"><iframe id="mini-iframe" frameBorder="0"></iframe></div>
<button id="btn-close" onclick="overlay_off()">×</button>
<script src="/python-tips/overlay.js"></script>
<link rel="stylesheet" href="/python-tips/style.css">

  </div>

  <div class="post-footer">
    <p style="margin-bottom: 1rem;">欢迎关注我的其它发布渠道：</p>
    <div class="social-links" style="display: flex; flex-direction: column; gap: 10px;">
      
      <a href="https://twitter.com/_stellar_tide" target="_blank" rel="noopener noreferrer">
        <i class="fab fa-twitter"></i> X
      </a>
      
      
      <a href="https://github.com/luochang212" target="_blank" rel="noopener noreferrer">
        <i class="fab fa-github"></i> GitHub
      </a>
      
      
      <a href="https://www.luochang.ink/images/wechat.jpg" target="_blank" rel="noopener noreferrer">
        <i class="fab fa-weixin"></i> 公众号
      </a>
      
      <a href="https://www.zhihu.com/people/Fashionable" target="_blank" rel="noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="2" 
             stroke-linecap="round" stroke-linejoin="round" style="width: 1em; height: 1em; vertical-align: -0.125em; display: inline-block;">
          <path d="m13.3334,3.60098l0,17.1471l1.79582,0l0.75424,2.13703l3.18459,-2.13703l3.93584,0l0,-17.1471l-9.6705,0zm7.41375,14.87538l-1.79283,0l-2.24777,1.50849l-0.53276,-1.50849l-0.53875,0l0,-12.53483l5.10911,0l0,12.53483l0.00299,0zm-8.56906,-7.18927l-3.97475,0c0.06285,-1.34387 0.1287,-3.12174 0.19754,-5.17496l3.91788,0l-0.00299,-0.24244c0,-0.01796 -0.00599,-0.43998 -0.06884,-0.87097c-0.06285,-0.44896 -0.19754,-1.04457 -0.62854,-1.04457l-6.5727,0c0.13169,-0.61657 0.46991,-2.08615 0.87995,-2.80747l0.19155,-0.33522l-0.3861,-0.02095c-0.02394,0 -0.58663,-0.02694 -1.23912,0.31726c-1.06851,0.56868 -1.5474,1.68807 -1.75691,2.52612c-0.55072,2.18791 -1.33489,3.70837 -1.66712,4.35786c-0.09877,0.19155 -0.15863,0.30529 -0.18557,0.38311c-0.05387,0.14666 -0.02394,0.29332 0.0838,0.38909c0.31427,0.28434 1.14334,-0.0868 1.15232,-0.08979c0.01796,-0.00898 0.03891,-0.01796 0.06585,-0.02993c0.41603,-0.18856 1.64916,-0.74826 2.08914,-2.52911l1.69705,0c0.02095,0.96376 0.09278,4.14236 0.0868,5.17496l-4.22018,0l-0.06285,0.0449c-0.69139,0.50582 -0.91288,1.8916 -0.92185,1.95146l-0.0419,0.27536l4.99837,0c-0.36814,2.34355 -0.79315,3.3941 -1.01763,3.81313c-0.11074,0.20951 -0.21849,0.41902 -0.32025,0.62255c-0.63752,1.26306 -1.29898,2.56802 -3.7802,4.5973c-0.10775,0.0838 -0.20951,0.23944 -0.14367,0.41005c0.07183,0.18856 0.27835,0.27237 0.73629,0.27237c0.16162,0 0.35318,-0.00898 0.58065,-0.02993c1.49352,-0.13169 3.01698,-0.53875 4.04359,-2.6219c0.50882,-1.05056 0.94879,-2.14601 1.31394,-3.25941l4.08549,4.78886l0.14965,-0.35916c0.02394,-0.05687 0.56868,-1.38578 0.15264,-2.87032l-0.01497,-0.05387l-3.23547,-3.68143l-0.65847,0.49684c0.19155,-0.78118 0.31726,-1.49352 0.37413,-2.12805l4.74995,0l0,-0.23944c0,-1.20021 -0.55371,-1.91255 -0.57466,-1.94248l-0.07183,-0.08979z" />
        </svg> 知乎
      </a>

      <a href="/index.xml" target="_blank" rel="noopener noreferrer">
        <i class="fas fa-rss"></i> RSS
      </a>

    </div>
    <div style="margin-bottom: 5rem;"></div>
  </div>

  

<div class="navigation navigation-single">
    
    <a href="/posts/process_manager/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">后台管理工具介绍</span>
    </a>
    
    
    <a href="/posts/transformer_arxiv/" class="navigation-next">
      <span class="navigation-tittle">Attention Is All You Need 论文精读</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>

        </div>
        
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>





<script src="https://kit.fontawesome.com/2134995a39.js" crossorigin="anonymous"></script>

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    



<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.js"></script>
<script type="text/javascript">
  if (tocbot) {
    tocbot.init({
      
      tocSelector: '.toc',
      
      contentSelector: '.post',
      
      headingSelector: 'h2, h3, h4',
      collapseDepth: 4
    });
  }
</script>



    



    </body>
</html>
