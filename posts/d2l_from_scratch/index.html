<!DOCTYPE html>
<html lang="zh-cn">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    
    <link rel="canonical" href="https://luochang212.github.io/posts/d2l_from_scratch/">
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.148.2">

    
    
    

<title>手写深度学习 • Chang Luo</title>



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="手写深度学习">
  <meta name="twitter:description" content="冬天就是要多做手部运动，让手暖起来">
      <meta name="twitter:site" content="@_stellar_tide">

<meta property="og:url" content="https://luochang212.github.io/posts/d2l_from_scratch/">
  <meta property="og:site_name" content="Chang Luo">
  <meta property="og:title" content="手写深度学习">
  <meta property="og:description" content="冬天就是要多做手部运动，让手暖起来">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-02-09T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-02-09T00:00:00+00:00">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="D2L">


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.1354666f7c97c95b395dd180ab9cf1aca1ff0408c93996eed446576738e01579.css" integrity="sha256-E1Rmb3yXyVs5XdGAq5zxrKH/BAjJOZbu1EZXZzjgFXk=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">




<link rel="stylesheet" href="/scss/tocbot.5ef07cebc3c477b54270456f149ee02922479bb7555fd344b2c69f953b0e7e5e.css" integrity="sha256-XvB868PEd7VCcEVvFJ7gKSJHm7dVX9NEssaflTsOfl4=">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    
    

</head>


    <body class=" ">
    <head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://upcdn.b0.upaiyun.com/libs/jquery/jquery-1.9.0.min.js"></script>

  <style>
    body {
      transition: background-color .5s;
    }

    #overlay {
      position: fixed;
      display: none;
      width: calc(100vw - 580px);
      height: 100%;
      top: 0;
      left: 0;
       
      z-index: 2;
      cursor: pointer;
    }

    .sidenav {
      height: 100%;
      width: 0;
      position: fixed;
      z-index: 1000;
      top: 0;
      right: 0;
      background-color: #FFF;
      overflow: hidden;
      transition: 0.5s;
    }

    #main {
      transition: margin-left .5s;
       
      padding: 16px;
      position: absolute;
      top: 0;
      right: 0;
      bottom: 0;
      left: 0;
      overflow: hidden;
    }

     
    .tab {
      overflow: hidden;
      border: 1px solid #ccc;
      background-color: #f1f1f1;
    }

     
    .tab button {
      background-color: inherit;
      float: left;
      border: none;
      outline: none;
      cursor: pointer;
      padding: 14px 16px;
      transition: 0.3s;
      font-size: 17px;
    }

     
    .tab button:hover {
      background-color: #ddd;
    }

     
    .tab button.active {
      background-color: #ccc;
    }

     
    .tabcontent {
      display: flex;
      padding: 12px 12px;
      margin: 0;
      border: 1px solid #ccc;
      border-top: none;
      overflow: hidden;
      height: calc(100vh - 90px);
    }

    .dropbtn {
      background-color: #3498DB;
      color: white;
      padding: 16px;
      font-size: 16px;
      border: none;
    }

    .dropup {
      position: relative;
      display: inline-block;
    }

    .dropup-content {
      display: none;
      position: absolute;
      background-color: #f1f1f1;
      min-width: 160px;
      bottom: 50px;
      z-index: 1;
    }

    .dropup-content a {
      color: black;
      padding: 12px 16px;
      text-decoration: none;
      display: block;
    }

    .dropup-content a:hover {
      background-color: #ccc
    }

    .dropup:hover .dropup-content {
      display: block;
    }

    .dropup:hover .dropbtn {
      background-color: #2980B9;
    }

    #video2,
    #video3 {
      display: none;
    }

    .myiframe {
      height: 100%;
      width: 100%;
    }
  </style>

</head>

<body>

  
  <div class="sidebar">
    <div class="container ">
      <div class="sidebar-about">
        <span class="site__title">
          <a href="https://luochang212.github.io/">Chang Luo</a>
        </span>
        
        
        
        <div class="author-image">
          <a href="#" class="load-iframe" onclick="openNav()"><img src="https://luochang212.github.io//images/profile.webp" id="author-iamge"
              alt="Author Image" class="img--circle img--headshot element--center"></a>
        </div>
        
        <p class="site__description">
          
        </p>
      </div>
      <div class="collapsible-menu">
        <input type="checkbox" id="menuToggle">
        <label for="menuToggle">Chang Luo</label>
        <div class="menu-content">
          <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/portfolio/">
						<span>Projects</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/about/">
						<span>About Me</span>
					</a>
				</li>
			 
		
	</ul>
</div>

          <section class="social">
	
	<a href="https://twitter.com/_stellar_tide" rel="me"><i class="fab fa-x-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	<a href="https://github.com/luochang212" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	<a href="https://instagram.com/suphenshin" rel="me"><i class="fab fa-instagram fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	<a href="https://www.zhihu.com/people/Fashionable" rel="me">
		<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="2" 
			 stroke-linecap="round" stroke-linejoin="round" style="width: 1.25em; height: 1.25em; vertical-align: -0.25em;">
			<path d="m13.3334,3.60098l0,17.1471l1.79582,0l0.75424,2.13703l3.18459,-2.13703l3.93584,0l0,-17.1471l-9.6705,0zm7.41375,14.87538l-1.79283,0l-2.24777,1.50849l-0.53276,-1.50849l-0.53875,0l0,-12.53483l5.10911,0l0,12.53483l0.00299,0zm-8.56906,-7.18927l-3.97475,0c0.06285,-1.34387 0.1287,-3.12174 0.19754,-5.17496l3.91788,0l-0.00299,-0.24244c0,-0.01796 -0.00599,-0.43998 -0.06884,-0.87097c-0.06285,-0.44896 -0.19754,-1.04457 -0.62854,-1.04457l-6.5727,0c0.13169,-0.61657 0.46991,-2.08615 0.87995,-2.80747l0.19155,-0.33522l-0.3861,-0.02095c-0.02394,0 -0.58663,-0.02694 -1.23912,0.31726c-1.06851,0.56868 -1.5474,1.68807 -1.75691,2.52612c-0.55072,2.18791 -1.33489,3.70837 -1.66712,4.35786c-0.09877,0.19155 -0.15863,0.30529 -0.18557,0.38311c-0.05387,0.14666 -0.02394,0.29332 0.0838,0.38909c0.31427,0.28434 1.14334,-0.0868 1.15232,-0.08979c0.01796,-0.00898 0.03891,-0.01796 0.06585,-0.02993c0.41603,-0.18856 1.64916,-0.74826 2.08914,-2.52911l1.69705,0c0.02095,0.96376 0.09278,4.14236 0.0868,5.17496l-4.22018,0l-0.06285,0.0449c-0.69139,0.50582 -0.91288,1.8916 -0.92185,1.95146l-0.0419,0.27536l4.99837,0c-0.36814,2.34355 -0.79315,3.3941 -1.01763,3.81313c-0.11074,0.20951 -0.21849,0.41902 -0.32025,0.62255c-0.63752,1.26306 -1.29898,2.56802 -3.7802,4.5973c-0.10775,0.0838 -0.20951,0.23944 -0.14367,0.41005c0.07183,0.18856 0.27835,0.27237 0.73629,0.27237c0.16162,0 0.35318,-0.00898 0.58065,-0.02993c1.49352,-0.13169 3.01698,-0.53875 4.04359,-2.6219c0.50882,-1.05056 0.94879,-2.14601 1.31394,-3.25941l4.08549,4.78886l0.14965,-0.35916c0.02394,-0.05687 0.56868,-1.38578 0.15264,-2.87032l-0.01497,-0.05387l-3.23547,-3.68143l-0.65847,0.49684c0.19155,-0.78118 0.31726,-1.49352 0.37413,-2.12805l4.74995,0l0,-0.23944c0,-1.20021 -0.55371,-1.91255 -0.57466,-1.94248l-0.07183,-0.08979z" />
		</svg>
	</a>
	
	
	
	
	
	
	
	
	
	
	
	
	
</section>

        </div>
      </div>
      
<div class="copyright">
  &copy; 2025 Chang Luo
  
</div>



    </div>
  </div>

  
  

  <div id="overlay" onclick="off()"></div>

  <div id="mySidenav" class="sidenav">

    <div id="main">
      <div class="tab">
        <button id="default-tab" class="tablinks" onclick="openItem(event, 'Resources')">Resources</button>
        <button class="tablinks" onclick="openItem(event, 'netease-player')">Music</button>
        <button class="tablinks" onclick="openItem(event, 'video-player')">Video</button>
        <button class="tablinks" onclick="openItem(event, 'gallery')">Gallery</button>
        <button class="tablinks" onclick="openItem(event, 'gadget')">Gadget</button>
        <button class="tablinks" onclick="openItem(event, 'categories')">Cats</button>
      </div>

      <div id="Resources" class="tabcontent">
        <iframe class="myiframe"
          src="about:blank"
          data-src="/resources/"
          frameborder="0"
          marginheight="0"
          marginwidth="0"
          scrolling="auto"></iframe>
      </div>

      <div id="netease-player" class="tabcontent">
        <iframe class="myiframe"
          src="about:blank"
          data-src="//music.163.com/outchain/player?type=0&id=2250585392&auto=0&height=430"
          frameborder="no"
          marginheight="0"
          marginwidth="0"
          scrolling="auto"></iframe>
      </div>

      <div id="video-player" class="tabcontent">
        <div id="video1">
          <iframe src="about:blank"
            data-src="//player.bilibili.com/player.html?bvid=BV1Ln4y1d7Z9&p=1&high_quality=1&danmaku=0&autoplay=0"
            style="width:100%; height: 500px"
            scrolling="no"
            frameborder="no"
            framespacing="0"
            allowfullscreen="true"></iframe>
        </div>
        <div id="video2">
          <iframe src="about:blank"
            data-src="//player.bilibili.com/player.html?bvid=BV1rp4y1L7i4&p=1&high_quality=1&danmaku=0&autoplay=0"
            style="width:100%; height: 500px"
            scrolling="no"
            frameborder="no"
            framespacing="0"
            allowfullscreen="true"></iframe>
        </div>
        <div id="video3">
          <iframe src="about:blank"
            data-src="//player.bilibili.com/player.html?bvid=BV1Av4y147QE&p=1&high_quality=1&danmaku=0&autoplay=0"
            style="width:100%; height:500px"
            scrolling="no"
            frameborder="no"
            framespacing="0"
            allowfullscreen="true"></iframe>
        </div>

        

        

        <br>
        <div class="dropup">
          <button class="dropbtn">更多视频</button>
          <div class="dropup-content">
            <a onclick="link1()">ZUTOMYAO</a>
            <a onclick="link2()">岁岁恋</a>
            <a onclick="link3()">真物论</a>
          </div>
        </div>
      </div>

      <div id="gallery" class="tabcontent">
        <iframe class="myiframe"
        src="about:blank"
        data-src="/gadget/gallery/#two"
        frameborder="0"
        marginheight="0"
        marginwidth="0"
        scrolling="auto"></iframe>
      </div>

      <div id="gadget" class="tabcontent">
        <iframe class="myiframe"
        src="about:blank"
        data-src="/tool/"
        frameborder="0"
        marginheight="0"
        marginwidth="0"
        scrolling="auto"></iframe>
      </div>

      <div id="categories" class="tabcontent">
        <iframe class="myiframe"
        src="about:blank"
        data-src="/categories/"
        frameborder="0"
        marginheight="0"
        marginwidth="0"
        scrolling="auto"></iframe>
      </div>
    </div>
  </div>

  
  <script>
    'use strict'

    function openNav() {
      on();
      document.getElementById("mySidenav").style.width = "580px";
      document.body.style.backgroundColor = "rgba(0,0,0,0.5)";
    }

    function closeNav() {
      document.getElementById("mySidenav").style.width = "0";
      document.body.style.backgroundColor = "white";
    }

    function on() {
      document.getElementById("overlay").style.display = "block";
    }

    function off() {
      closeNav();
      document.getElementById("overlay").style.display = "none";
    }

    
    document.getElementById('default-tab').click();

    function openItem(evt, itemName) {
      var i, tabcontent, tablinks;
      tabcontent = document.getElementsByClassName("tabcontent");
      for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
      }
      tablinks = document.getElementsByClassName("tablinks");
      for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
      }
      document.getElementById(itemName).style.display = "block";
      evt.currentTarget.className += " active";
    }

    function link1() {
      document.getElementById("video1").style.display = "block";
      document.getElementById("video2").style.display = "none";
      document.getElementById("video3").style.display = "none";
    }

    function link2() {
      document.getElementById("video1").style.display = "none";
      document.getElementById("video2").style.display = "block";
      document.getElementById("video3").style.display = "none";
    }

    function link3() {
      document.getElementById("video1").style.display = "none";
      document.getElementById("video2").style.display = "none";
      document.getElementById("video3").style.display = "block";
    }

     
    var flag = 0;

     
    $(".load-iframe").click(function () {
      if (flag < 1) {
        $("iframe").each(function () {
          $(this).attr("src", $(this).data("src"));
        });
        flag = flag + 1;
      }
    });
  </script>

</body>



        <div class="content container">
            
    <style>
  .my-emoji {
    height: 25px;
    display: inline-block;
    margin: 0;
    vertical-align: text-bottom;
  }

  .social-links a {
    color: rgba(120,120,120,1);
    text-decoration: none;
    outline: none !important;
    cursor: pointer;
    transition-property: padding, text-shadow, line-height, color, background, opacity, transform;
    transition-duration: .25s;
    transition-timing-function: ease-in-out;
    display: inline-block;
    white-space: nowrap;
    position: relative;
  }

  .social-links a:after {
    content: "";
    display: block;
    height: .125rem;
    margin: -.125rem 0 0;
    background: rgba(0,0,0,.5);
    border-radius: 1px;
    transform: scaleX(0);
    transition: transform .25s ease-in-out;
  }

  .social-links a:hover:after {
    transform: scaleX(1);
  }

  .social-links a:hover {
    color: rgba(120,120,120,1);
  }

  .post {
    margin-bottom: 0;
  }

  .post-footer {
    margin-top: 0;
    padding-top: 0.5rem;
  }
</style>



<article>
  <header>
    <h1>手写深度学习</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Feb 9, 2024
    
    
    
      
      
          in
          
          
              <a class="badge badge-category" href="/categories/deeplearning">DEEPLEARNING</a>
              
          
      
    
    
    
      
      
          <br/>
           <i class="fas fa-tags"></i>
          
          <a class="badge badge-tag" href="/tags/python">python</a>
           
      
          <a class="badge badge-tag" href="/tags/d2l">d2l</a>
          
      
    
    
    <br/>
    <i class="fas fa-clock"></i> 25 min read
</div>


  </header>
  
  
  
  <div class="toc-wrapper">
    <input type="checkbox" id="tocToggle">
    <label for="tocToggle">目录</label>
    
    <div class="toc" id="TableOfContents"></div>
    
  </div>
  
  
  <div class="post">
    <blockquote>
<p>与其说深度学习是一门技术，不如说深度学习是一种语言</p></blockquote>
<p>GitHub 项目地址：<a href="https://github.com/luochang212/AI-Project/tree/main/scratch" target="_blank">AI-Project/scratch</a></p>
<h3 id="一自动微分">一、自动微分</h3>
<h4 id="1-简单的例子">1. 简单的例子</h4>
<p>1.1 张量 x 的梯度</p>
<p>张量 <code>$x$</code> 的梯度可以存储在 <code>$x$</code> 上。</p>
<p>要点：</p>
<ul>
<li><code>x.grad</code>: 取 <code>$x$</code> 的梯度</li>
<li><code>x.requires_grad_(True)</code>: 允许 tenser <code>$x$</code> 存储自己的梯度</li>
<li><code>x.grad.zero_()</code>: 将 <code>$x$</code> 的梯度置零</li>
</ul>
<pre tabindex="0"><code>import torch

# 初始化张量 x (tensor x)
x = torch.arange(4.0)

x.requires_grad_(True)  # 允许 tensor x 存储梯度
x.grad is None  # 梯度默认为 None
</code></pre><p><code>&gt; True</code></p>
<p>初始化带梯度的张量，下面是两个例子：</p>
<pre tabindex="0"><code>torch.tensor([1., 2., 3.], requires_grad=True)
</code></pre><p><code>&gt; tensor([1., 2., 3.], requires_grad=True)</code></p>
<pre tabindex="0"><code>torch.randn((2, 5), requires_grad=True)
</code></pre><p><code>&gt; tensor([[ 0.4075,  1.1930,  0.5716, -1.0924,  0.0653], [-1.2869,  1.5768,  1.3445,  0.6309, -0.0484]], requires_grad=True)</code></p>
<p>1.2 损失函数 及 反向传播</p>
<p>我们约定：</p>
<ul>
<li>将 <strong>损失</strong> 记为 <code>$y$</code></li>
<li>设 <strong>损失函数</strong> 为：<code>$y = 2 * x \cdot x$</code>（注意是点乘）</li>
</ul>
<p>计算 $y$ 关于 $x$ 每个分量的梯度，步骤如下：</p>
<ol>
<li>定义损失函数：<code>y = 2 * torch.dot(x, x)</code></li>
<li>计算 <code>$y$</code> 关于 <code>$x$</code> 的梯度，即反向传播：<code>y.backward()</code></li>
<li>获取更新后 <code>$x$</code> 的梯度：<code>x.grad</code></li>
</ol>
<pre tabindex="0"><code>y = 2 * torch.dot(x, x)
y.backward()  # 用反向传播自动计算 y 关于 x 每个分量的梯度

x.grad
</code></pre><p><code>&gt; tensor([ 0.,  4.,  8., 12.])</code></p>
<p>函数 $y = 2x^{T}x$ 关于 $x$ 的梯度应为 $4x$，验证是否正确：</p>
<pre tabindex="0"><code>x.grad == 4 * x
</code></pre><p><code>&gt; tensor([True, True, True, True])</code></p>
<h4 id="2-当损失为向量时">2. 当损失为向量时</h4>
<p>梯度的“形状”：</p>
<ul>
<li>当损失 <code>$y$</code> 为 <strong>标量</strong> 时，梯度是 <strong>向量</strong>，且与 <code>$x$</code> 维度相同</li>
<li>当损失 <code>$y$</code> 为 <strong>向量</strong> 时，梯度是 <strong>矩阵</strong></li>
</ul>
<p>注意，当损失 $y$ 为向量时。我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。也就是说：</p>
<p>在反向传播代码里要多加一个 <code>sum()</code> 函数，写成 <code>y.sum().backward()</code></p>
<pre tabindex="0"><code>x.grad.zero_()  # 将张量 x 的梯度置零
x, x.grad
</code></pre><p><code>&gt; (tensor([0., 1., 2., 3.], requires_grad=True), tensor([0., 0., 0., 0.]))</code></p>
<p>定义损失函数：<code>$y = 2 * x \times x$</code>（注意是叉乘）</p>
<pre tabindex="0"><code># 定义损失函数
y = 2 * x * x
y  # 注意 y 在这里是向量
</code></pre><p><code>&gt; tensor([ 0.,  2.,  8., 18.], grad_fn=&lt;MulBackward0&gt;)</code></p>
<pre tabindex="0"><code>y.sum().backward()
x.grad
</code></pre><p><code>&gt; tensor([ 0.,  4.,  8., 12.])</code></p>
<h4 id="3-with-torchno_grad">3. <code>with torch.no_grad()</code></h4>
<p>在 PyTorch 中，如果一个张量的 <code>requires_grad</code> 参数设为 <code>True</code>。则所有依赖它的张量的 <code>requires_grad</code> 参数将被设置为 <code>True</code></p>
<p>但在 <code>with torch.no_grad()</code> 块中的张量，依赖它的张量的 <code>requires_grad</code> 参数将被设为 <code>False</code></p>
<p>参见：<a href="https://pytorch.org/docs/stable/generated/torch.no_grad.html" target="_blank">pytorch.org</a></p>
<p>下面是一个例子：</p>
<pre tabindex="0"><code>x = torch.tensor([1.], requires_grad=True)
with torch.no_grad():
    y = x * 2
y.requires_grad
</code></pre><p><code>&gt; False</code></p>
<p>作为对照：</p>
<pre tabindex="0"><code>x = torch.tensor([1.], requires_grad=True)
y = x * 2
y.requires_grad
</code></pre><p><code>&gt; True</code></p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/AI-Project/blob/main/scratch/1.auto_diff.ipynb')">查看示例</button>
</center>
<br>
<h3 id="二加载数据">二、加载数据</h3>
<p>本节学习将 图像分类数据集 Fashion-MNIST 分割成训练集和测试集，并构造一个数据加载器 Generators 使得数据可以被多次迭代使用。</p>
<p>教程链接：<a href="https://zh-v2.d2l.ai/chapter_linear-networks/image-classification-dataset.html">image-classification-datasetv</a></p>
<pre tabindex="0"><code>%matplotlib inline
import time
import torch
import torchvision
from torch.utils import data
from torchvision import transforms
import matplotlib.pyplot as plt
</code></pre><h4 id="1-读取数据集">1. 读取数据集</h4>
<p>读取 Fashion-MNIST 数据集 (Xiao et al., 2017)</p>
<pre tabindex="0"><code># 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，
# 并除以255使得所有像素的数值均在0～1之间
trans = transforms.ToTensor()
mnist_train = torchvision.datasets.FashionMNIST(
    root=&#34;../data&#34;, train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root=&#34;../data&#34;, train=False, transform=trans, download=True)
</code></pre><pre tabindex="0"><code>def get_fashion_mnist_labels(labels):  #@save
    &#34;&#34;&#34;返回Fashion-MNIST数据集的文本标签&#34;&#34;&#34;
    text_labels = [&#39;t-shirt&#39;, &#39;trouser&#39;, &#39;pullover&#39;, &#39;dress&#39;, &#39;coat&#39;,
                   &#39;sandal&#39;, &#39;shirt&#39;, &#39;sneaker&#39;, &#39;bag&#39;, &#39;ankle boot&#39;]
    return [text_labels[int(i)] for i in labels]
</code></pre><pre tabindex="0"><code>def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save
    &#34;&#34;&#34;绘制图像列表&#34;&#34;&#34;
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        if torch.is_tensor(img):
            # 图片张量
            ax.imshow(img.numpy())
        else:
            # PIL图片
            ax.imshow(img)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes
</code></pre><pre tabindex="0"><code>X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
X.shape, y.shape
</code></pre><p><code>&gt; (torch.Size([18, 1, 28, 28]), torch.Size([18]))</code></p>
<pre tabindex="0"><code>show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y))
</code></pre><p><img src="/img/dataloader_mnist.png" alt=""></p>
<h4 id="2-读取小批量">2. 读取小批量</h4>
<p>在每次迭代中，数据加载器每次都会读取一小批量数据，大小为batch_size。 通过内置数据迭代器，我们可以随机打乱了所有样本，从而无偏见地读取小批量。</p>
<pre tabindex="0"><code>batch_size = 256

def get_dataloader_workers():  #@save
    &#34;&#34;&#34;使用4个进程来读取数据&#34;&#34;&#34;
    return 4

train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,
                             num_workers=get_dataloader_workers())
</code></pre><pre tabindex="0"><code>class Timer:
    &#34;&#34;&#34;Record multiple running times.&#34;&#34;&#34;
    def __init__(self):
        &#34;&#34;&#34;Defined in :numref:`sec_minibatch_sgd`&#34;&#34;&#34;
        self.times = []
        self.start()

    def start(self):
        &#34;&#34;&#34;Start the timer.&#34;&#34;&#34;
        self.tik = time.time()

    def stop(self):
        &#34;&#34;&#34;Stop the timer and record the time in a list.&#34;&#34;&#34;
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        &#34;&#34;&#34;Return the average time.&#34;&#34;&#34;
        return sum(self.times) / len(self.times)

    def sum(self):
        &#34;&#34;&#34;Return the sum of time.&#34;&#34;&#34;
        return sum(self.times)

    def cumsum(self):
        &#34;&#34;&#34;Return the accumulated time.&#34;&#34;&#34;
        return np.array(self.times).cumsum().tolist()
</code></pre><pre tabindex="0"><code># 看一下读取训练数据所需的时间
timer = Timer()
for X, y in train_iter:
    continue
f&#39;{timer.stop():.2f} sec&#39;
</code></pre><h4 id="3-整合所有组件">3. 整合所有组件</h4>
<pre tabindex="0"><code>def load_data_fashion_mnist(batch_size, resize=None):
    &#34;&#34;&#34;下载Fashion-MNIST数据集，然后将其加载到内存中&#34;&#34;&#34;
    trans = [transforms.ToTensor()]
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(
        root=&#34;../data&#34;, train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root=&#34;../data&#34;, train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers()))
</code></pre><p>通过指定resize参数来测试load_data_fashion_mnist函数的图像大小调整功能</p>
<pre tabindex="0"><code>for X, y in train_iter:
    print(X.shape, X.dtype, y.shape, y.dtype)
    break
</code></pre><p><code>&gt; torch.Size([256, 1, 28, 28]) torch.float32 torch.Size([256]) torch.int64</code></p>
<pre tabindex="0"><code>train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
for X, y in train_iter:
    print(f&#34;X.shape: {X.shape}&#34;)
    print(f&#34;X.dtype: {X.dtype}&#34;)
    print(f&#34;y.shape: {y.shape}&#34;)
    print(f&#34;y.dtype: {y.dtype}&#34;)
    break
</code></pre><br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/AI-Project/blob/main/scratch/2.dataloader.ipynb')">查看示例</button>
</center>
<br>
<h3 id="三logistic-回归">三、Logistic 回归</h3>
<pre tabindex="0"><code>%matplotlib inline
import random
import math
import numpy as np
import collections
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
</code></pre><h4 id="1-生成样本和标号">1. 生成样本和标号</h4>
<pre tabindex="0"><code># 画图参数
COLORS = [&#39;b&#39;, &#39;g&#39;, &#39;r&#39;, &#39;c&#39;, &#39;m&#39;, &#39;y&#39;, &#39;k&#39;]
MARKERS = [&#39;o&#39;, &#39;v&#39;, &#39;^&#39;, &#39;s&#39;, &#39;P&#39;]

# 业务参数
DIM = 3
CLUSTR_NUM = 10


def generate_sample(clustr_num, width=30, std=3, smin=600, smax=700, gen_seed=9603602):
    &#34;&#34;&#34;生成样本
    clustr_num: 簇数
    width: 空间点位于长宽高均为 width 的正方体内
    std: 生成样本时，样本与样本中心距离的标准差
    smin: 生成样本量时，样本量的最小值
    smax: 生成样本量时，样本量的最大值
    &#34;&#34;&#34;
    if clustr_num &gt; len(COLORS) * len(MARKERS):
        raise Exception(&#34;Error: clustr_num &lt;= len(COLORS) * len(MARKERS)&#34;)
    dim = DIM
    res = collections.defaultdict(list)
    
    random.seed(gen_seed)
    for i in range(clustr_num):
        mean = [random.random() * width for _ in range(dim)]
        sample_num = round((smax - smin) * random.random()) + smin
        for r in np.random.normal(0, std, sample_num):
            deg = [random.random() * math.pi * 2 for _ in range(2)]
            node = [mean[0] + r * math.cos(deg[0]) * math.cos(deg[1]),
                    mean[1] + r * math.cos(deg[0]) * math.sin(deg[1]),
                    mean[2] + r * math.sin(deg[0])]
        
            res[i].append(node)

    return res
</code></pre><pre tabindex="0"><code>samples = dict(generate_sample(clustr_num=CLUSTR_NUM))
# samples
</code></pre><pre tabindex="0"><code>def _3d_plot(samples):

    fig = plt.figure()
    ax = fig.add_subplot(projection=&#39;3d&#39;)

    mix = [(c, m) for c in COLORS for m in MARKERS]
    np.random.seed(19680801)
    np.random.shuffle(mix)
    for i, f in enumerate(samples.items()):
        k, v = f
        color, marker = mix[i]

        xs = [e[0] for e in v]
        ys = [e[1] for e in v]
        zs = [e[2] for e in v]
        ax.scatter(xs, ys, zs, color=color, marker=marker, label=k)

    ax.set_xlabel(&#39;X Label&#39;)
    ax.set_ylabel(&#39;Y Label&#39;)
    ax.set_zlabel(&#39;Z Label&#39;)
    ax.legend = ax.legend(bbox_to_anchor=(.5, .30, .85, .5))

    plt.show()
</code></pre><pre tabindex="0"><code>_3d_plot(samples)
</code></pre><p><img src="/img/samples.png" alt=""></p>
<h4 id="2-分割-训练集-和-预测集">2. 分割 训练集 和 预测集</h4>
<pre tabindex="0"><code>def load_data(samples, div_rate=0.8):
    X, Y = [], []

    # 把输入整成适合sklearn输入的形状
    for lable, vlist in samples.items():
        for v in vlist:
            X.append(v)
            Y.append(lable)

    # 校验
    if not (div_rate &lt;= 0.95 and div_rate &gt; 0):
        raise Exception(&#34;Error: div_rate &lt;= 0.95 and div_rate &gt; 0&#34;)
    if len(X) != len(Y):
        raise Exception(&#34;Error: len(X) != len(Y)&#34;)

    # shuffle
    data = list(zip(X, Y))
    random.shuffle(data)
    X = [e[0] for e in data]
    Y = [e[1] for e in data]

    # 分割训练集和预测集
    div_num = math.floor(len(X) * div_rate)
    train_X, train_Y = X[:div_num], Y[:div_num]
    test_X, test_Y = X[div_num:], Y[div_num:]

    return train_X, train_Y, test_X, test_Y
</code></pre><pre tabindex="0"><code>train_X, train_Y, test_X, test_Y = load_data(samples)
sum(test_Y), len(test_Y)
</code></pre><p><code>&gt; (5920, 1294)</code></p>
<pre tabindex="0"><code>train_X[:3], train_Y[:3], test_X[:3], test_Y[:3]
</code></pre><p><code>&gt; ([[20.837554579645627, 18.961845823741736, 16.633726090847148], [2.3794870779388098, 28.838366976092825, 13.376753958471658], [4.851394029530915, 8.603495446318274, 17.186267813538862]], [3, 6, 7], [[29.404596908032293, 21.33132439225346, 6.65602456625498], [27.167331038537665, 27.64980912165818, 7.986085486917643], [19.193191406802836, 15.638041952359082, 7.5837088545045495]], [0, 5, 3])</code></p>
<h4 id="3-借助-sklearn-的简单实现">3. 借助 sklearn 的简单实现</h4>
<p>sklearn链接：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">sklearn.linear_model.LogisticRegression</a></p>
<pre tabindex="0"><code>logreg = LogisticRegression(C=1e5, max_iter=100000)
logreg.fit(train_X, train_Y)
</code></pre><pre tabindex="0"><code>pred_Y = logreg.predict(test_X)
pred_Y
</code></pre><p><code>&gt; array([0, 5, 3, ..., 5, 2, 1])</code></p>
<pre tabindex="0"><code>def simple_accuracy(pred_Y, test_Y):
    l = [int(y==y_hat) for y, y_hat in zip(test_Y, pred_Y)]
    l_sum = sum(l)
    return l_sum, len(l), round(l_sum / len(l), 4)
</code></pre><pre tabindex="0"><code>pos, s, acc = simple_accuracy(pred_Y, test_Y)
print(&#34;预测正确的个数:&#34;, pos)
print(&#34;总数:&#34;, s)
print(&#34;精确率:&#34;, acc)
</code></pre><p><code>&gt; 预测正确的个数: 1223 总数: 1294 精确率: 0.9451</code></p>
<pre tabindex="0"><code>pred_samples = collections.defaultdict(list)
for xp, yp in zip(test_X, pred_Y):
    pred_samples[yp].append(xp)

_3d_plot(pred_samples)
</code></pre><p><img src="/img/logistic_pred.png" alt=""></p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/AI-Project/blob/main/scratch/3.logistic_regression.ipynb')">查看示例</button>
</center>
<br>
<h3 id="四softmax-回归">四、Softmax 回归</h3>
<p>从零开始写 softmax 回归</p>
<p>softmax 回归其实是分类不是回归</p>
<p>softmax 回归的步骤：</p>
<ul>
<li>初始 样本 与 样本标号</li>
<li>将样本分割成 训练集 和 测试集</li>
<li>初始化 矩阵 W 和 向量 b（线性回归 Y = W * X + b）</li>
<li>线性回归的结果过 softmax</li>
<li>定义 loss function，在这里是 交叉熵</li>
<li>定义优化器，做反向传播，更新参数</li>
<li>计算分类精度</li>
</ul>
<p>重要构件：</p>
<ul>
<li>网络模型（前向传播）：线性方程 + softmax 算子</li>
<li>loss 函数：用于计算梯度</li>
</ul>
<pre tabindex="0"><code>%matplotlib inline
import random
import math
import numpy as np
import collections
import matplotlib.pyplot as plt
from IPython import display
import torch
</code></pre><h4 id="1-生成-样本-和-样本标号">1. 生成 样本 和 样本标号</h4>
<p>生成 <code>clustr_num</code> 个簇，每个簇有 <code>sample_num</code> 个样本</p>
<pre tabindex="0"><code># 画图参数
COLORS = [&#39;b&#39;, &#39;g&#39;, &#39;r&#39;, &#39;c&#39;, &#39;m&#39;, &#39;y&#39;, &#39;k&#39;]
MARKERS = [&#39;o&#39;, &#39;v&#39;, &#39;^&#39;, &#39;s&#39;, &#39;P&#39;]

# 业务参数
DIM = 3
CLUSTR_NUM = 10


def generate_sample(clustr_num, width=30, std=3, smin=600, smax=700, gen_seed=9603602):
    &#34;&#34;&#34;生成样本
    clustr_num: 簇数
    width: 空间点位于长宽高均为 width 的正方体内
    std: 生成样本时，样本与样本中心距离的标准差
    smin: 生成样本量时，样本量的最小值
    smax: 生成样本量时，样本量的最大值
    &#34;&#34;&#34;
    if clustr_num &gt; len(COLORS) * len(MARKERS):
        raise Exception(&#34;Error: clustr_num &lt;= len(COLORS) * len(MARKERS)&#34;)
    dim = DIM
    res = collections.defaultdict(list)
    
    random.seed(gen_seed)
    for i in range(clustr_num):
        mean = [random.random() * width for _ in range(dim)]
        sample_num = round((smax - smin) * random.random()) + smin
        for r in np.random.normal(0, std, sample_num):
            deg = [random.random() * math.pi * 2 for _ in range(2)]
            node = [mean[0] + r * math.cos(deg[0]) * math.cos(deg[1]),
                    mean[1] + r * math.cos(deg[0]) * math.sin(deg[1]),
                    mean[2] + r * math.sin(deg[0])]
        
            res[i].append(node)

    return res
</code></pre><pre tabindex="0"><code>samples = dict(generate_sample(clustr_num=CLUSTR_NUM))
# samples
</code></pre><pre tabindex="0"><code>def _3d_plot(samples):
    
    fig = plt.figure()
    ax = fig.add_subplot(projection=&#39;3d&#39;)

    mix = [(c, m) for c in COLORS for m in MARKERS]
    np.random.seed(19680801)
    np.random.shuffle(mix)
    for i, f in enumerate(samples.items()):
        k, v = f
        color, marker = mix[i]

        xs = [e[0] for e in v]
        ys = [e[1] for e in v]
        zs = [e[2] for e in v]
        ax.scatter(xs, ys, zs, color=color, marker=marker, label=k)

    ax.set_xlabel(&#39;X Label&#39;)
    ax.set_ylabel(&#39;Y Label&#39;)
    ax.set_zlabel(&#39;Z Label&#39;)
    ax.legend = ax.legend(bbox_to_anchor=(.5, .30, .85, .5))

    plt.show()
</code></pre><pre tabindex="0"><code>_3d_plot(samples)
</code></pre><p><img src="/img/softmax_samples.png" alt=""></p>
<h4 id="2-分割-训练集-和-测试集">2. 分割 训练集 和 测试集</h4>
<p>将样本字典整成元组并打乱，分成训练集和测试集</p>
<pre tabindex="0"><code>def load_sample(samples, train_rate=0.8):
    sample_list = []
    for k, v in samples.items():
        for s in v:
            e = (s, k)
            sample_list.append(e)
    
    random.shuffle(sample_list)
    train_num = math.floor(len(sample_list) * train_rate)

    train = sample_list[:train_num]
    test = sample_list[train_num:]

    return train, test
</code></pre><pre tabindex="0"><code>train_data, test_data = load_sample(samples)
# train_data, test_data
</code></pre><pre tabindex="0"><code>len(train_data), len(test_data)
</code></pre><p><code>&gt; (5174, 1294)</code></p>
<h4 id="3-初始化模型参数">3. 初始化模型参数</h4>
<pre tabindex="0"><code>nums_input = DIM  # 输入数据的维度
nums_output = CLUSTR_NUM  # CLUSTR_NUM 是簇数，也是标签数

W = torch.normal(0, 0.01, size=(nums_input, nums_output), requires_grad=True, dtype=torch.float64)
b = torch.zeros(nums_output, requires_grad=True, dtype=torch.float64)
# W, b
</code></pre><pre tabindex="0"><code>print(&#34;W.shape:&#34;, W.shape)
print(&#34;b.shape:&#34;, b.shape)
print(&#34;W.dtype:&#34;, W.dtype)
print(&#34;b.dtype:&#34;, b.dtype)
</code></pre><p><code>&gt; W.shape: torch.Size([3, 10]) b.shape: torch.Size([10]) W.dtype: torch.float64 b.dtype: torch.float64</code></p>
<h4 id="4-定义-softmax-操作">4. 定义 softmax 操作</h4>
<p>经过 softmax 操作，每个元素变成非负数，且每行总和为 1</p>
<p>你可以说 softmax 操作是在做标准化</p>
<p>但我觉得 softmax 操作更像是把原始数据做变换后，往概率那边硬凑</p>
<pre tabindex="0"><code>def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition  # 利用了广播机制
</code></pre><h4 id="5-定义-网络模型">5. 定义 网络模型</h4>
<pre tabindex="0"><code>def net(X):
    return softmax(torch.matmul(X.reshape(-1, W.shape[0]), W) + b)
</code></pre><h4 id="6-定义-损失函数">6. 定义 损失函数</h4>
<p>把 交叉熵函数 作为 损失函数</p>
<p>注意作为输入的 y_hat 和 y 的形状不同，本质上不是一个东西。它俩的关系是：</p>
<p><code>$y = j = argmax \hat y_{j}$</code></p>
<pre tabindex="0"><code>def cross_entropy(y_hat, y):
    return -torch.log(y_hat[range(len(y_hat)), y])
</code></pre><h4 id="7-优化器">7. 优化器</h4>
<p><code>sgd()</code>：小批量随机梯度下降函数</p>
<p>参见：<a href="https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#sec-linear-scratch">linear-regression-scratch</a></p>
<pre tabindex="0"><code>def sgd(params, lr, batch_size):
    &#34;&#34;&#34;Minibatch stochastic gradient descent.

    Defined in :numref:`sec_utils`&#34;&#34;&#34;
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
</code></pre><pre tabindex="0"><code>def sgd_updater(batch_size, lr=0.1):
    &#34;&#34;&#34;优化器
    batch_size: 批量大小
    lr: 学习率
    &#34;&#34;&#34;
    return sgd([W, b], lr, batch_size)
</code></pre><h4 id="8-两个工具类">8. 两个工具类</h4>
<p>本节有两个工具类：</p>
<ul>
<li>累加器</li>
<li>训练图作图器</li>
</ul>
<pre tabindex="0"><code># 累加器
class Accumulator:
    &#34;&#34;&#34;在n个变量上累加&#34;&#34;&#34;
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
</code></pre><pre tabindex="0"><code># 训练图作图器
class Animator:
    &#34;&#34;&#34;在动画中绘制数据&#34;&#34;&#34;
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale=&#39;linear&#39;, yscale=&#39;linear&#39;,
                 fmts=(&#39;-&#39;, &#39;m--&#39;, &#39;g-.&#39;, &#39;r:&#39;), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        self.use_svg_display()
        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: self.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts
        
    @staticmethod
    def use_svg_display():
        &#34;&#34;&#34;Use the svg format to display a plot in Jupyter.

        Defined in :numref:`sec_calculus`&#34;&#34;&#34;
        from matplotlib_inline import backend_inline
        backend_inline.set_matplotlib_formats(&#39;svg&#39;)
    
    @staticmethod
    def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
        &#34;&#34;&#34;Set the axes for matplotlib.

        Defined in :numref:`sec_calculus`&#34;&#34;&#34;
        axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)
        axes.set_xscale(xscale), axes.set_yscale(yscale)
        axes.set_xlim(xlim),     axes.set_ylim(ylim)
        if legend:
            axes.legend(legend)
        axes.grid()

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, &#34;__len__&#34;):
            y = [y]
        n = len(y)
        if not hasattr(x, &#34;__len__&#34;):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
</code></pre><h4 id="9-计算-分类精度">9. 计算 分类精度</h4>
<p>计算预测正确的数量：</p>
<pre tabindex="0"><code>def accuracy(y_hat, y):
    &#34;&#34;&#34;计算预测正确的数量&#34;&#34;&#34;
    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
</code></pre><p>计算预测正确的比例：</p>
<pre tabindex="0"><code>def cal_accuracy_rate(y_hat, y):
    &#34;&#34;&#34;计算预测正确的比例&#34;&#34;&#34;
    return accuracy(y_hat, y) / len(y)
</code></pre><p>一个复合函数，用于计算 在指定数据集 上，网络 net 的精度</p>
<pre tabindex="0"><code>def evaluate_accuracy(net, data_iter):  #@save
    &#34;&#34;&#34;计算在指定数据集上模型的精度&#34;&#34;&#34;
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
</code></pre><h4 id="10-数据迭代器">10. 数据迭代器</h4>
<p>我的野生实现，不是官方的实现方法</p>
<p>每 <code>batch_size</code> 行输入 concat 在一起，对应的 <code>y</code> 也 concat 在一起</p>
<pre tabindex="0"><code>class MyDataLoader:
    def __init__(self):
        self._lst: list
        self.i = 0

    @property
    def lst(self):
        return self._lst
    
    @lst.setter
    def lst(self, value):
        self._lst = value
    
    def __iter__(self):
        return self
    
    def __next__(self):
        if self.i &lt; len(self._lst):
            i = self.i
            self.i += 1
            return self._lst[i]
        else:
            self.i = 0
            raise StopIteration


def data_iter(data, batch_size):
    &#34;&#34;&#34;将 原始输入向量 拼接成 批量矩阵&#34;&#34;&#34;
    res = []
    batch_num = math.floor(len(data) / batch_size)
    for i in range(batch_num):
        start, end = batch_size * i, batch_size * (i + 1)
        # X = torch.tensor([e[0] for e in data[start:end]], requires_grad=True)
        X = torch.tensor([e[0] for e in data[start:end]])
        y = torch.tensor([e[1] for e in data[start:end]])
        res.append((X, y))
    
    dl = MyDataLoader()
    dl.lst = res
    
    return dl
</code></pre><h4 id="11-训练">11. 训练</h4>
<p>注意，学习率 lr 和 迭代周期数 num_epochs 都是可调的超参数</p>
<pre tabindex="0"><code># 单批量训练
def train_epoch(train_data, net, loss, updater):
    metric = Accumulator(3)  # 用于存储 (训练损失总和、训练准确度总和、样本数)
    for X, y in train_data:
        # 前向传播
        y_hat = net(X)

        # 计算梯度
        l = loss(y_hat, y)

        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # 反向传播
            l.sum().backward()

            # 使用优化器，更新参数
            updater(X.shape[0])  # X.shape[0] 是批量大小 (batch_size)

        # 记录当前状态
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())

    # 返回训练损失和训练精度
    return metric[0] / metric[2],  metric[1] / metric[2]
</code></pre><pre tabindex="0"><code># 多批量训练
def train(num_epochs, net, train_data, test_data, loss, updater, ylim=[0.3, 1.0], need_assert=True):
    &#34;&#34;&#34;
    num_epochs: 迭代周期个数
    net: 网络模型函数
    train_data: 训练集
    test_data: 测试集
    loss: 损失函数
    updater: 优化器
    &#34;&#34;&#34;
    animator = Animator(xlabel=&#39;epoch&#39;, xlim=[1, num_epochs], ylim=ylim,
                        legend=[&#39;train loss&#39;, &#39;train acc&#39;, &#39;test acc&#39;])

    for epoch in range(num_epochs):
        train_metrics = train_epoch(train_data, net, loss, updater)
        test_acc = evaluate_accuracy(net, test_data)
        animator.add(epoch + 1, train_metrics + (test_acc,))  # train_metrics + (test_acc,) 是有三个元素的元组
    train_loss, train_acc = train_metrics
    if need_assert:
        assert train_loss &lt; 5, train_loss
        assert train_acc &lt;= 1 and train_acc &gt; 0.5, train_acc
        assert test_acc &lt;= 1 and test_acc &gt; 0.5, test_acc
    print(&#34;train_loss:&#34;, round(train_loss, 5))
    print(&#34;train_acc:&#34;, round(train_acc, 5))
    print(&#34;test_acc:&#34;, round(test_acc, 5))
</code></pre><pre tabindex="0"><code>batch_size = 20  # 输入 X 的 X.shape[0]
num_epochs = 10  # 迭代周期数

train_dt = data_iter(train_data, batch_size)
test_dt = data_iter(test_data, batch_size)
type(train_dt), type(test_dt)
</code></pre><pre tabindex="0"><code>loss = cross_entropy
# updater=torch.optim.SGD([W, b], lr=0.1, momentum=0.9)
updater = sgd_updater

train(num_epochs, net, train_dt, test_dt, loss, updater)
</code></pre><p><code>&gt; train_loss: 1.51878 train_acc: 0.83004 test_acc: 0.80156</code></p>
<p><img src="/img/softmax_acc.png" alt=""></p>
<h4 id="12-预测">12. 预测</h4>
<pre tabindex="0"><code>def predict(net, test_data, n=6):
    &#34;&#34;&#34;预测标签&#34;&#34;&#34;
    for X, y in test_data:
        break
    trues = y
    preds = net(X).argmax(axis=1)

    indexs = list(range(len(trues)))
    random.shuffle(indexs)
    for i in indexs[:n]:
        print(&#34;true:&#34;, y[i])
        print(&#34;pred:&#34;, preds[i])
        print(&#34;=&#34; * 15)
</code></pre><pre tabindex="0"><code>predict(net, test_dt, n=5)
</code></pre><p>输出：</p>
<pre tabindex="0"><code>true: tensor(4)
pred: tensor(4)
===============
true: tensor(2)
pred: tensor(2)
===============
true: tensor(1)
pred: tensor(1)
===============
true: tensor(8)
pred: tensor(8)
===============
true: tensor(9)
pred: tensor(6)
===============
</code></pre><pre tabindex="0"><code>def predict_plot(net, test_data):
    &#34;&#34;&#34;预测标签3D图&#34;&#34;&#34;
    for X, y in test_data:
        break
    pred_samples = collections.defaultdict(list)
    for a, b in zip(X, y):
        pred_samples[int(b)].append(a.tolist())
    sd = sorted(pred_samples.items(), key=lambda e: e[0])
    _3d_plot(collections.OrderedDict(sd))
</code></pre><pre tabindex="0"><code>predict_plot(net, test_dt)
</code></pre><p><img src="/img/softmax_pred.png" alt=""></p>
<h4 id="13-xor问题-minsky--papert-1969">13. XOR问题 (Minsky &amp; Papert, 1969)</h4>
<blockquote>
<p><strong>Q</strong>: 能不能描述一下XOR问题 (Minsky &amp; Papert, 1969)</p></blockquote>
<blockquote>
<p><strong>文心一言</strong>：在1969年，Marvin Minsky和Seymour Papert提出了一个关于单层神经网络（也称为感知机）的功能及局限性的重要观点，并以XOR问题为例进行了阐述。</p>
<p>XOR问题是一个简单的逻辑运算问题，其规则是：当两个输入中有一个为1且另一个为0时，输出为1；否则，输出为0。这个问题对于人类来说很简单，但对于单层神经网络来说却是一个挑战。</p>
<p>Minsky和Papert通过分析单层神经网络的工作原理，证明了单层神经网络无法准确解决XOR问题。单层神经网络只能产生线性分割面，而XOR问题是一个非线性问题，无法用单一的线性分割面来解决。因此，单层神经网络无法拟合XOR函数。</p>
<p>这个发现对神经网络的发展产生了深远的影响。它揭示了单层神经网络的局限性，并推动了多层神经网络（如深度神经网络）的研究和发展。多层神经网络通过引入隐藏层，可以产生更复杂的非线性分割面，从而解决XOR等非线性问题。</p>
<p>总的来说，Minsky和Papert在1969年提出的XOR问题是对单层神经网络功能及局限性的一个重要探讨，它揭示了单层神经网络的局限性，并推动了神经网络领域的发展。</p></blockquote>
<p>我们构作一个类 XOR 样本集。生成 12 个簇，并把其中任意 4 个簇合并，使用同一种标号，此时 12 个簇一共产生 <code>12 / 4 = 3</code> 种标号</p>
<pre tabindex="0"><code>def gen_xor_samples(clustr_num, partition):
    samples = dict(generate_sample(clustr_num))

    indexs = [i for i in range(clustr_num)]
    random.shuffle(indexs)

    xor_samples = collections.defaultdict(list)
    for i, label in enumerate(indexs):
        xor_samples[math.floor(i / partition)] += samples[label]
    return xor_samples
</code></pre><pre tabindex="0"><code>clustr_num = 16
partition = 2
xor_samples = gen_xor_samples(clustr_num, partition)
# xor_samples
</code></pre><pre tabindex="0"><code>%matplotlib inline
_3d_plot(xor_samples)
</code></pre><p><img src="/img/softmax_xor_samples.png" alt=""></p>
<pre tabindex="0"><code>xor_train_data, xor_test_data = load_sample(xor_samples)
# xor_train_data, xor_test_data
</code></pre><pre tabindex="0"><code>len(xor_train_data), len(xor_test_data)
</code></pre><p><code>&gt; (8274, 2069)</code></p>
<pre tabindex="0"><code>nums_input = DIM  # 输入数据的维度
nums_output = math.floor(clustr_num / partition)  # CLUSTR_NUM 是标签数

W = torch.normal(0, 0.01, size=(nums_input, nums_output), requires_grad=True, dtype=torch.float64)
b = torch.zeros(nums_output, requires_grad=True, dtype=torch.float64)
</code></pre><pre tabindex="0"><code>batch_size = 32  # 输入 X 的 X.shape[0]
num_epochs = 10  # 迭代周期数

xor_train_dt = data_iter(xor_train_data, batch_size)
xor_test_dt = data_iter(xor_test_data, batch_size)
</code></pre><pre tabindex="0"><code>loss = cross_entropy
# updater=torch.optim.SGD([W, b], lr=0.1, momentum=0.9)
updater = sgd_updater

train(num_epochs, net, xor_train_dt, xor_test_dt, loss, updater, ylim=[0.0, 1.0], need_assert=False)
</code></pre><p><code>&gt; train_loss: 5.7881 train_acc: 0.44816 test_acc: 0.42188</code></p>
<p><img src="/img/softmax_xor_acc.png" alt=""></p>
<p>结论就是，softmax 回归拟合不了类 XOR 函数，这需要引入 <strong>多层感知机</strong> 解决。</p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/AI-Project/blob/main/scratch/4.softmax_regression.ipynb')">查看示例</button>
</center>
<br>
<h3 id="五softmax-回归的简洁实现">五、softmax 回归的简洁实现</h3>
<p>通过深度学习框架的高级API能够使实现同样的功能，并且能让我们拥有更高的编码效率。</p>
<p>简化了以下构建的编写：</p>
<ul>
<li>网络模型（前向传播）：线性方程 + softmax 算子</li>
<li>参数初始化</li>
<li>loss 函数：用于计算梯度</li>
<li>优化器：用梯度更新参数</li>
</ul>
<p>可以看到，主要构件的编写流程都被简化了</p>
<pre tabindex="0"><code>%matplotlib inline
import torch
from torch import nn
import torchvision
from torchvision import transforms
import random
import math
import numpy as np
import collections
import matplotlib.pyplot as plt
from IPython import display
</code></pre><h4 id="1-导入数据">1. 导入数据</h4>
<pre tabindex="0"><code>def get_dataloader_workers():
    &#34;&#34;&#34;Use 4 processes to read the data.

    Defined in :numref:`sec_utils`&#34;&#34;&#34;
    return 4


def load_data_fashion_mnist(batch_size, resize=None):
    &#34;&#34;&#34;Download the Fashion-MNIST dataset and then load it into memory.

    Defined in :numref:`sec_utils`&#34;&#34;&#34;
    trans = [transforms.ToTensor()]
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(
        root=&#34;../data&#34;, train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root=&#34;../data&#34;, train=False, transform=trans, download=True)
    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,
                                        num_workers=get_dataloader_workers()),
            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,
                                        num_workers=get_dataloader_workers()))
</code></pre><pre tabindex="0"><code>batch_size = 256
train_iter, test_iter = load_data_fashion_mnist(batch_size)
type(train_iter), type(test_iter)
</code></pre><p><code>&gt; (torch.utils.data.dataloader.DataLoader, torch.utils.data.dataloader.DataLoader)</code></p>
<pre tabindex="0"><code>len([e for e in train_iter])
</code></pre><p><code>&gt; 235</code></p>
<h4 id="2-定义网络模型">2. 定义网络模型</h4>
<pre tabindex="0"><code># PyTorch不会隐式地调整输入的形状。因此，
# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

l = [e for e in net.parameters()]
# l
</code></pre><pre tabindex="0"><code>l[0].shape, l[1].shape
</code></pre><p><code>&gt; (torch.Size([10, 784]), torch.Size([10]))</code></p>
<h4 id="3-初始化模型参数-1">3. 初始化模型参数</h4>
<pre tabindex="0"><code>def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights)
</code></pre><h4 id="4-重新审视softmax的实现">4. 重新审视Softmax的实现</h4>
<p>参见：<a href="https://zh-v2.d2l.ai/chapter_linear-networks/softmax-regression-concise.html#subsec-softmax-implementation-revisited">softmax-regression-concise</a></p>
<pre tabindex="0"><code>loss = nn.CrossEntropyLoss(reduction=&#39;none&#39;)
</code></pre><h4 id="5-优化算法">5. 优化算法</h4>
<pre tabindex="0"><code>trainer = torch.optim.SGD(net.parameters(), lr=0.1)
</code></pre><h4 id="6-必要构件">6. 必要构件</h4>
<pre tabindex="0"><code># 累加器
class Accumulator:
    &#34;&#34;&#34;在n个变量上累加&#34;&#34;&#34;
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
</code></pre><pre tabindex="0"><code># 训练图作图器
class Animator:
    &#34;&#34;&#34;在动画中绘制数据&#34;&#34;&#34;
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale=&#39;linear&#39;, yscale=&#39;linear&#39;,
                 fmts=(&#39;-&#39;, &#39;m--&#39;, &#39;g-.&#39;, &#39;r:&#39;), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        self.use_svg_display()
        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: self.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts
        
    @staticmethod
    def use_svg_display():
        &#34;&#34;&#34;Use the svg format to display a plot in Jupyter.

        Defined in :numref:`sec_calculus`&#34;&#34;&#34;
        from matplotlib_inline import backend_inline
        backend_inline.set_matplotlib_formats(&#39;svg&#39;)
    
    @staticmethod
    def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
        &#34;&#34;&#34;Set the axes for matplotlib.

        Defined in :numref:`sec_calculus`&#34;&#34;&#34;
        axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)
        axes.set_xscale(xscale), axes.set_yscale(yscale)
        axes.set_xlim(xlim),     axes.set_ylim(ylim)
        if legend:
            axes.legend(legend)
        axes.grid()

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, &#34;__len__&#34;):
            y = [y]
        n = len(y)
        if not hasattr(x, &#34;__len__&#34;):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
</code></pre><pre tabindex="0"><code>def accuracy(y_hat, y):
    &#34;&#34;&#34;计算预测正确的数量&#34;&#34;&#34;
    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
</code></pre><pre tabindex="0"><code>def evaluate_accuracy(net, data_iter):  #@save
    &#34;&#34;&#34;计算在指定数据集上模型的精度&#34;&#34;&#34;
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
</code></pre><h4 id="7-训练">7. 训练</h4>
<pre tabindex="0"><code># 单批量训练
def train_epoch(train_data, net, loss, updater):
    metric = Accumulator(4)  # 用于存储 (训练损失总和、训练准确度总和、样本数)
    for X, y in train_data:
        # print(X, y)
        # 正向传播
        y_hat = net(X)
        
        # 计算梯度
        l = loss(y_hat, y)

        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # 反向传播
            l.sum().backward()

            # 使用优化器，更新参数
            # print(&#34;X.grad&#34;, X.grad)
            updater(X.shape[0])  # X.shape[0] 是批量大小 (batch_size)

        # 记录当前状态
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel(), 1)

    # 返回训练损失和训练精度
    # print(metric[:], )
    return metric[0] / metric[2],  metric[1] / metric[2], metric[3]
</code></pre><pre tabindex="0"><code># 多批量训练
def train(num_epochs, net, train_data, test_data, loss, updater, ylim=[0.3, 1.0], need_assert=True):
    &#34;&#34;&#34;
    num_epochs: 迭代周期个数
    net: 网络模型函数
    train_data: 训练集
    test_data: 测试集
    loss: 损失函数
    updater: 优化器
    &#34;&#34;&#34;
    animator = Animator(xlabel=&#39;epoch&#39;, xlim=[1, num_epochs], ylim=ylim,
                        legend=[&#39;train loss&#39;, &#39;train acc&#39;, &#39;test acc&#39;])
    
    batch_num = 0
    for epoch in range(num_epochs):
        train_metrics = train_epoch(train_data, net, loss, updater)
        test_acc = evaluate_accuracy(net, test_data)
        animator.add(epoch + 1, train_metrics[:2] + (test_acc,))  # train_metrics + (test_acc,) 是有三个元素的元组
        batch_num += train_metrics[2]
    train_loss, train_acc = train_metrics[:2]
    if need_assert:
        assert train_loss &lt; 5, train_loss
        assert train_acc &lt;= 1 and train_acc &gt; 0.5, train_acc
        assert test_acc &lt;= 1 and test_acc &gt; 0.5, test_acc
    print(&#34;train_loss:&#34;, round(train_loss, 5))
    print(&#34;train_acc:&#34;, round(train_acc, 5))
    print(&#34;test_acc:&#34;, round(test_acc, 5))
    print(&#34;batch_num:&#34;, batch_num)
</code></pre><pre tabindex="0"><code>num_epochs = 10
train(num_epochs, net, train_iter, test_iter, loss, trainer)
</code></pre><p><code>&gt; train_loss: 0.44763 train_acc: 0.84878 test_acc: 0.8349 batch_num: 2350.0</code></p>
<p><img src="/img/softmax_consise_acc.png" alt=""></p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/AI-Project/blob/main/scratch/5.softmax_regresion_consise.ipynb')">查看示例</button>
</center>
<br>
<h3 id="六多层感知机">六、多层感知机</h3>
<p>教程链接：<a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html">mlp-scratch</a></p>
<pre tabindex="0"><code>%matplotlib inline
import random
import math
import numpy as np
import collections
import matplotlib.pyplot as plt
from IPython import display
import torchvision
from torchvision import transforms
import torch
from torch import nn
</code></pre><h4 id="1-导入-fashion-mnist-数据集">1. 导入 Fashion-MNIST 数据集</h4>
<pre tabindex="0"><code>def get_dataloader_workers():
    &#34;&#34;&#34;Use 4 processes to read the data.

    Defined in :numref:`sec_utils`&#34;&#34;&#34;
    return 4


def load_data_fashion_mnist(batch_size, resize=None):
    &#34;&#34;&#34;Download the Fashion-MNIST dataset and then load it into memory.

    Defined in :numref:`sec_utils`&#34;&#34;&#34;
    trans = [transforms.ToTensor()]
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(
        root=&#34;../data&#34;, train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root=&#34;../data&#34;, train=False, transform=trans, download=True)
    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,
                                        num_workers=get_dataloader_workers()),
            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,
                                        num_workers=get_dataloader_workers()))
</code></pre><pre tabindex="0"><code>batch_size = 256
train_iter, test_iter = load_data_fashion_mnist(batch_size)
type(train_iter)
</code></pre><p><code>&gt; torch.utils.data.dataloader.DataLoader</code></p>
<h4 id="2-初始化模型参数">2. 初始化模型参数</h4>
<pre tabindex="0"><code>num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
</code></pre><h4 id="3-激活函数">3. 激活函数</h4>
<pre tabindex="0"><code>def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
</code></pre><h4 id="4-模型">4. 模型</h4>
<pre tabindex="0"><code>def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    return (H@W2 + b2)
</code></pre><h4 id="5-损失函数">5. 损失函数</h4>
<pre tabindex="0"><code>loss = nn.CrossEntropyLoss(reduction=&#39;none&#39;)
</code></pre><h4 id="6-必要构件-1">6. 必要构件</h4>
<pre tabindex="0"><code># 累加器
class Accumulator:
    &#34;&#34;&#34;在n个变量上累加&#34;&#34;&#34;
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
</code></pre><pre tabindex="0"><code># 训练图作图器
class Animator:
    &#34;&#34;&#34;在动画中绘制数据&#34;&#34;&#34;
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale=&#39;linear&#39;, yscale=&#39;linear&#39;,
                 fmts=(&#39;-&#39;, &#39;m--&#39;, &#39;g-.&#39;, &#39;r:&#39;), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        self.use_svg_display()
        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: self.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts
        
    @staticmethod
    def use_svg_display():
        &#34;&#34;&#34;Use the svg format to display a plot in Jupyter.

        Defined in :numref:`sec_calculus`&#34;&#34;&#34;
        from matplotlib_inline import backend_inline
        backend_inline.set_matplotlib_formats(&#39;svg&#39;)
    
    @staticmethod
    def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
        &#34;&#34;&#34;Set the axes for matplotlib.

        Defined in :numref:`sec_calculus`&#34;&#34;&#34;
        axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)
        axes.set_xscale(xscale), axes.set_yscale(yscale)
        axes.set_xlim(xlim),     axes.set_ylim(ylim)
        if legend:
            axes.legend(legend)
        axes.grid()

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, &#34;__len__&#34;):
            y = [y]
        n = len(y)
        if not hasattr(x, &#34;__len__&#34;):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
</code></pre><pre tabindex="0"><code>def accuracy(y_hat, y):
    &#34;&#34;&#34;计算预测正确的数量&#34;&#34;&#34;
    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
</code></pre><pre tabindex="0"><code>def evaluate_accuracy(net, data_iter):  #@save
    &#34;&#34;&#34;计算在指定数据集上模型的精度&#34;&#34;&#34;
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
</code></pre><h4 id="7-训练-1">7. 训练</h4>
<pre tabindex="0"><code># 单批量训练
def train_epoch(train_data, net, loss, updater):
    metric = Accumulator(4)  # 用于存储 (训练损失总和、训练准确度总和、样本数)
    for X, y in train_data:
        # print(X, y)
        # 正向传播
        y_hat = net(X)
        
        # 计算梯度
        l = loss(y_hat, y)

        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # 反向传播
            l.sum().backward()

            # 使用优化器，更新参数
            # print(&#34;X.grad&#34;, X.grad)
            updater(X.shape[0])  # X.shape[0] 是批量大小 (batch_size)

        # 记录当前状态
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel(), 1)

    # 返回训练损失和训练精度
    # print(metric[:], )
    return metric[0] / metric[2],  metric[1] / metric[2], metric[3]
</code></pre><pre tabindex="0"><code># 多批量训练
def train(num_epochs, net, train_data, test_data, loss, updater, ylim=[0.3, 1.0], need_assert=True):
    &#34;&#34;&#34;
    num_epochs: 迭代周期个数
    net: 网络模型函数
    train_data: 训练集
    test_data: 测试集
    loss: 损失函数
    updater: 优化器
    &#34;&#34;&#34;
    animator = Animator(xlabel=&#39;epoch&#39;, xlim=[1, num_epochs], ylim=ylim,
                        legend=[&#39;train loss&#39;, &#39;train acc&#39;, &#39;test acc&#39;])
    
    batch_num = 0
    for epoch in range(num_epochs):
        train_metrics = train_epoch(train_data, net, loss, updater)
        test_acc = evaluate_accuracy(net, test_data)
        animator.add(epoch + 1, train_metrics[:2] + (test_acc,))  # train_metrics + (test_acc,) 是有三个元素的元组
        batch_num += train_metrics[2]
    train_loss, train_acc = train_metrics[:2]
    if need_assert:
        assert train_loss &lt; 5, train_loss
        assert train_acc &lt;= 1 and train_acc &gt; 0.5, train_acc
        assert test_acc &lt;= 1 and test_acc &gt; 0.5, test_acc
    print(&#34;train_loss:&#34;, round(train_loss, 5))
    print(&#34;train_acc:&#34;, round(train_acc, 5))
    print(&#34;test_acc:&#34;, round(test_acc, 5))
    print(&#34;batch_num:&#34;, batch_num)
</code></pre><pre tabindex="0"><code>num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
train(num_epochs, net, train_iter, test_iter, loss, updater)
</code></pre><p><code>&gt; train_loss: 0.38572 train_acc: 0.86397 test_acc: 0.8497 batch_num: 2350.0</code></p>
<p><img src="/img/mlp_acc.png" alt=""></p>
<h4 id="8--预测">8.  预测</h4>
<pre tabindex="0"><code>def predict(net, test_data, n=6):
    &#34;&#34;&#34;预测标签&#34;&#34;&#34;
    for X, y in test_data:
        break
    trues = y
    preds = net(X).argmax(axis=1)

    indexs = list(range(len(trues)))
    random.shuffle(indexs)
    for i in indexs[:n]:
        print(&#34;true:&#34;, y[i])
        print(&#34;pred:&#34;, preds[i])
        print(&#34;=&#34; * 15)
</code></pre><pre tabindex="0"><code>predict(net, test_iter, n=6)
</code></pre><p>输出：</p>
<pre tabindex="0"><code>true: tensor(0)
pred: tensor(0)
===============
true: tensor(1)
pred: tensor(1)
===============
true: tensor(3)
pred: tensor(3)
===============
true: tensor(3)
pred: tensor(3)
===============
true: tensor(0)
pred: tensor(0)
===============
true: tensor(1)
pred: tensor(1)
===============
</code></pre><p>⚠️ 接着我们来解决 softmax 回归一节中引入的 XOR 问题</p>
<h4 id="9-生成-样本-和-样本标号">9. 生成 样本 和 样本标号</h4>
<pre tabindex="0"><code># 画图参数
COLORS = [&#39;b&#39;, &#39;g&#39;, &#39;r&#39;, &#39;c&#39;, &#39;m&#39;, &#39;y&#39;, &#39;k&#39;]
MARKERS = [&#39;o&#39;, &#39;v&#39;, &#39;^&#39;, &#39;s&#39;, &#39;P&#39;]

# 业务参数
DIM = 3
CLUSTR_NUM = 10


def generate_sample(clustr_num, width=30, std=3, smin=600, smax=700, gen_seed=9603602):
    &#34;&#34;&#34;生成样本
    clustr_num: 簇数
    width: 空间点位于长宽高均为 width 的正方体内
    std: 生成样本时，样本与样本中心距离的标准差
    smin: 生成样本量时，样本量的最小值
    smax: 生成样本量时，样本量的最大值
    &#34;&#34;&#34;
    if clustr_num &gt; len(COLORS) * len(MARKERS):
        raise Exception(&#34;Error: clustr_num &lt;= len(COLORS) * len(MARKERS)&#34;)
    dim = DIM
    res = collections.defaultdict(list)
    
    random.seed(gen_seed)
    for i in range(clustr_num):
        mean = [random.random() * width for _ in range(dim)]
        sample_num = round((smax - smin) * random.random()) + smin
        for r in np.random.normal(0, std, sample_num):
            deg = [random.random() * math.pi * 2 for _ in range(2)]
            node = [mean[0] + r * math.cos(deg[0]) * math.cos(deg[1]),
                    mean[1] + r * math.cos(deg[0]) * math.sin(deg[1]),
                    mean[2] + r * math.sin(deg[0])]
        
            res[i].append(node)

    return res
</code></pre><pre tabindex="0"><code>def _3d_plot(samples):
    
    fig = plt.figure()
    ax = fig.add_subplot(projection=&#39;3d&#39;)

    mix = [(c, m) for c in COLORS for m in MARKERS]
    np.random.seed(19680801)
    np.random.shuffle(mix)
    for i, f in enumerate(samples.items()):
        k, v = f
        color, marker = mix[i]

        xs = [e[0] for e in v]
        ys = [e[1] for e in v]
        zs = [e[2] for e in v]
        ax.scatter(xs, ys, zs, color=color, marker=marker, label=k)

    ax.set_xlabel(&#39;X Label&#39;)
    ax.set_ylabel(&#39;Y Label&#39;)
    ax.set_zlabel(&#39;Z Label&#39;)
    ax.legend = ax.legend(bbox_to_anchor=(.5, .30, .85, .5))

    plt.show()
</code></pre><pre tabindex="0"><code>def gen_xor_samples(clustr_num, partition):
    samples = dict(generate_sample(clustr_num))

    indexs = [i for i in range(clustr_num)]
    random.shuffle(indexs)

    xor_samples = collections.defaultdict(list)
    for i, label in enumerate(indexs):
        xor_samples[math.floor(i / partition)] += samples[label]
    return xor_samples

clustr_num = 16
partition = 2
xor_samples = gen_xor_samples(clustr_num, partition)
# xor_samples
</code></pre><pre tabindex="0"><code>_3d_plot(xor_samples)
</code></pre><p><img src="/img/mlp_xor_samples.png" alt=""></p>
<h4 id="10-分割-训练集-和-测试集">10. 分割 训练集 和 测试集</h4>
<p>将样本字典整成元组并打乱，分成训练集和测试集</p>
<pre tabindex="0"><code>def load_sample(samples, train_rate=0.8):
    lst = []
    for y, Xlist in samples.items():
        for X in Xlist:
            lst.append((X, y))

    random.shuffle(lst)
    train_num = math.floor(len(lst) * train_rate)

    train = lst[:train_num]
    test = lst[train_num:]

    return train, test
</code></pre><pre tabindex="0"><code>class MyDataLoader:
    def __init__(self):
        self._lst: list
        self.i = 0

    @property
    def lst(self):
        return self._lst
    
    @lst.setter
    def lst(self, value):
        self._lst = value
    
    def __iter__(self):
        return self
    
    def __next__(self):
        if self.i &lt; len(self._lst):
            i = self.i
            self.i += 1
            return self._lst[i]
        else:
            self.i = 0
            raise StopIteration


def data_iter(data, batch_size):
    &#34;&#34;&#34;将 原始输入向量 拼接成 批量矩阵&#34;&#34;&#34;
    res = []
    batch_num = math.floor(len(data) / batch_size)
    for i in range(batch_num):
        start, end = batch_size * i, batch_size * (i + 1)
        # X = torch.tensor([e[0] for e in data[start:end]], requires_grad=True)
        X = torch.tensor([e[0] for e in data[start:end]])
        y = torch.tensor([e[1] for e in data[start:end]])
        res.append((X, y))
    
    dl = MyDataLoader()
    dl.lst = res
    
    return dl
</code></pre><pre tabindex="0"><code>xor_train_data, xor_test_data = load_sample(xor_samples)
# xor_train_data, xor_test_data
</code></pre><pre tabindex="0"><code>batch_size = 32
xor_train_iter = data_iter(xor_train_data, batch_size)
xor_test_iter = data_iter(xor_test_data, batch_size)
type(xor_train_iter), type(xor_test_iter)
</code></pre><pre tabindex="0"><code>num_inputs, num_outputs, num_hiddens = DIM, len(xor_samples.items()), 128
num_inputs, num_outputs, num_hiddens
</code></pre><p><code>&gt; (3, 8, 512)</code></p>
<pre tabindex="0"><code># 初始化参数
W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True, dtype=torch.float64) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))

W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True, dtype=torch.float64) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
</code></pre><pre tabindex="0"><code># 激活函数
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
</code></pre><pre tabindex="0"><code># 网络模型
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)
    return (H@W2 + b2)
</code></pre><pre tabindex="0"><code># 损失函数
loss = nn.CrossEntropyLoss(reduction=&#39;none&#39;)
</code></pre><pre tabindex="0"><code>num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
train(num_epochs, net, xor_train_iter, xor_test_iter, loss, updater)
</code></pre><p><code>&gt; train_loss: 0.61771 train_acc: 0.79227 test_acc: 0.78125 batch_num: 2580.0</code></p>
<p><img src="/img/mlp_xor_acc.png" alt=""></p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/AI-Project/blob/main/scratch/6.mlp.ipynb')">查看示例</button>
</center>
<br>
<h3 id="七多层感知机的简洁实现">七、多层感知机的简洁实现</h3>
<pre tabindex="0"><code>%matplotlib inline
import random
import math
import numpy as np
import collections
import matplotlib.pyplot as plt
from IPython import display
import torchvision
from torchvision import transforms
import torch
from torch import nn
</code></pre><h4 id="1-模型">1. 模型</h4>
<pre tabindex="0"><code>net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights)
</code></pre><pre tabindex="0"><code>def get_dataloader_workers():
    &#34;&#34;&#34;Use 4 processes to read the data.

    Defined in :numref:`sec_utils`&#34;&#34;&#34;
    return 4


def load_data_fashion_mnist(batch_size, resize=None):
    &#34;&#34;&#34;Download the Fashion-MNIST dataset and then load it into memory.

    Defined in :numref:`sec_utils`&#34;&#34;&#34;
    trans = [transforms.ToTensor()]
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(
        root=&#34;../data&#34;, train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root=&#34;../data&#34;, train=False, transform=trans, download=True)
    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,
                                        num_workers=get_dataloader_workers()),
            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,
                                        num_workers=get_dataloader_workers()))


# 累加器
class Accumulator:
    &#34;&#34;&#34;在n个变量上累加&#34;&#34;&#34;
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

    
# 训练图作图器
class Animator:
    &#34;&#34;&#34;在动画中绘制数据&#34;&#34;&#34;
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale=&#39;linear&#39;, yscale=&#39;linear&#39;,
                 fmts=(&#39;-&#39;, &#39;m--&#39;, &#39;g-.&#39;, &#39;r:&#39;), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        self.use_svg_display()
        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: self.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts
        
    @staticmethod
    def use_svg_display():
        &#34;&#34;&#34;Use the svg format to display a plot in Jupyter.

        Defined in :numref:`sec_calculus`&#34;&#34;&#34;
        from matplotlib_inline import backend_inline
        backend_inline.set_matplotlib_formats(&#39;svg&#39;)
    
    @staticmethod
    def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
        &#34;&#34;&#34;Set the axes for matplotlib.

        Defined in :numref:`sec_calculus`&#34;&#34;&#34;
        axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)
        axes.set_xscale(xscale), axes.set_yscale(yscale)
        axes.set_xlim(xlim),     axes.set_ylim(ylim)
        if legend:
            axes.legend(legend)
        axes.grid()

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, &#34;__len__&#34;):
            y = [y]
        n = len(y)
        if not hasattr(x, &#34;__len__&#34;):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
        

def accuracy(y_hat, y):
    &#34;&#34;&#34;计算预测正确的数量&#34;&#34;&#34;
    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())


def evaluate_accuracy(net, data_iter):  #@save
    &#34;&#34;&#34;计算在指定数据集上模型的精度&#34;&#34;&#34;
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
</code></pre><h4 id="2-训练">2. 训练</h4>
<pre tabindex="0"><code># 单批量训练
def train_epoch(train_data, net, loss, updater):
    metric = Accumulator(4)  # 用于存储 (训练损失总和、训练准确度总和、样本数)
    for X, y in train_data:
        # print(X, y)
        # 正向传播
        y_hat = net(X)
        
        # 计算梯度
        l = loss(y_hat, y)

        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # 反向传播
            l.sum().backward()

            # 使用优化器，更新参数
            # print(&#34;X.grad&#34;, X.grad)
            updater(X.shape[0])  # X.shape[0] 是批量大小 (batch_size)

        # 记录当前状态
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel(), 1)

    # 返回训练损失和训练精度
    # print(metric[:], )
    return metric[0] / metric[2],  metric[1] / metric[2], metric[3]


# 多批量训练
def train(num_epochs, net, train_data, test_data, loss, updater, ylim=[0.3, 1.0], need_assert=True):
    &#34;&#34;&#34;
    num_epochs: 迭代周期个数
    net: 网络模型函数
    train_data: 训练集
    test_data: 测试集
    loss: 损失函数
    updater: 优化器
    &#34;&#34;&#34;
    animator = Animator(xlabel=&#39;epoch&#39;, xlim=[1, num_epochs], ylim=ylim,
                        legend=[&#39;train loss&#39;, &#39;train acc&#39;, &#39;test acc&#39;])
    
    batch_num = 0
    for epoch in range(num_epochs):
        train_metrics = train_epoch(train_data, net, loss, updater)
        test_acc = evaluate_accuracy(net, test_data)
        animator.add(epoch + 1, train_metrics[:2] + (test_acc,))  # train_metrics + (test_acc,) 是有三个元素的元组
        batch_num += train_metrics[2]
    train_loss, train_acc = train_metrics[:2]
    if need_assert:
        assert train_loss &lt; 5, train_loss
        assert train_acc &lt;= 1 and train_acc &gt; 0.5, train_acc
        assert test_acc &lt;= 1 and test_acc &gt; 0.5, test_acc
    print(&#34;train_loss:&#34;, round(train_loss, 5))
    print(&#34;train_acc:&#34;, round(train_acc, 5))
    print(&#34;test_acc:&#34;, round(test_acc, 5))
    print(&#34;batch_num:&#34;, batch_num)
</code></pre><pre tabindex="0"><code>batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction=&#39;none&#39;)
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = load_data_fashion_mnist(batch_size)
train(num_epochs, net, train_iter, test_iter, loss, trainer)
</code></pre><p><code>&gt; train_loss: 0.38334 train_acc: 0.86343 test_acc: 0.849 batch_num: 2350.0</code></p>
<p><img src="/img/mlp_consise_acc.png" alt=""></p>
<br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/AI-Project/blob/main/scratch/7.mlp_consise.ipynb')">查看示例</button>
</center>
<br>
<h3 id="八automl-h2o">八、AutoML: h2o</h3>
<p>h2o 是一个 AutoML 框架，本节我们尝试用 <code>h2o.automl.H2OAutoML</code> 完成 Kaggle 竞赛 <a href="https://www.kaggle.com/competitions/titanic" target="_blank">titanic</a>.</p>
<pre tabindex="0"><code>DIRECTORY = &#39;./data&#39;
TRAIN_FILE=&#39;titanic/train.csv&#39;
TEST_FILE=&#39;titanic/test.csv&#39;
MODEL_FILE=&#39;model&#39;
PREDICT_FILE=&#39;res.csv&#39;

LABEL_COL=&#39;Survived&#39;
</code></pre><pre tabindex="0"><code>import h2o
import pandas as pd
import warnings

import util
</code></pre><pre tabindex="0"><code># 隐藏 warning，请谨慎使用
warnings.filterwarnings(&#34;ignore&#34;)
</code></pre><h4 id="1-训练">1. 训练</h4>
<pre tabindex="0"><code>h2o.init()
</code></pre><pre tabindex="0"><code>train_path = util.gen_abspath(DIRECTORY, TRAIN_FILE)
data = h2o.import_file(train_path)
data
</code></pre><pre tabindex="0"><code># 处理类别变量
factor_cols = [&#39;Name&#39;, &#39;Sex&#39;]
for col in factor_cols:
    data[col] = data[col].asfactor()
</code></pre><pre tabindex="0"><code># 分割训练集、验证集
train, valid = data.split_frame(ratios=[0.8], seed=377)
</code></pre><pre tabindex="0"><code>y = LABEL_COL
x = data.columns
x.remove(&#39;PassengerId&#39;)
x.remove(&#39;Survived&#39;)
</code></pre><pre tabindex="0"><code>aml = h2o.automl.H2OAutoML(max_runtime_secs=1000)
aml.train(x=x, y=y, training_frame=data)
</code></pre><pre tabindex="0"><code># 预测
y_pred = aml.predict(data).as_data_frame()[&#39;predict&#39;].tolist()
y_true = data.as_data_frame()[&#39;Survived&#39;].tolist()
</code></pre><pre tabindex="0"><code>y_label, threshold = util.eval_binary(y_true=y_true, y_pred=y_pred, n_trials=1000, ret=True)
</code></pre><p>输出：</p>
<pre tabindex="0"><code>threshold: 0.69676
accuracy: 1.00000
precision: 1.00000
recall: 1.00000
f1_score: 1.00000
auc: 1.00000
cross-entropy loss: 0.06207
True Positive (TP): 342
True Negative (TN): 549
False Positive (FP): 0
False Negative (FN): 0
confusion matrix:
[[549   0]
 [  0 342]]
</code></pre><h4 id="2-评估">2. 评估</h4>
<pre tabindex="0"><code># 最佳模型的表现
aml.leader.model_performance(valid)
</code></pre><p>输出：</p>
<pre tabindex="0"><code>MSE: 0.01228063484339922
RMSE: 0.11081802580536805
MAE: 0.09042077792156883
RMSLE: 0.07910391398523471
Mean Residual Deviance: 0.01228063484339922
R^2: 0.947051240897029
Null degrees of freedom: 185
Residual degrees of freedom: 183
Null deviance: 43.20171411080499
Residual deviance: 2.284198080872255
AIC: -282.5049544551409
</code></pre><pre tabindex="0"><code># 最佳模型的摘要信息
aml.leader.summary()
</code></pre><pre tabindex="0"><code># 特征的重要程度
aml.varimp()
</code></pre><pre tabindex="0"><code># 模型排行榜
aml.leaderboard
</code></pre><h4 id="3-保存">3. 保存</h4>
<pre tabindex="0"><code># save model DIRECTORY
model_dir = util.gen_abspath(DIRECTORY, MODEL_FILE)
model_path = h2o.save_model(model=aml.leader, path=model_dir, force=True)
</code></pre><pre tabindex="0"><code># load model
saved_model = h2o.load_model(model_path)
</code></pre><h4 id="4-预测">4. 预测</h4>
<pre tabindex="0"><code>test_path = util.gen_abspath(DIRECTORY, TEST_FILE)
test_data = h2o.import_file(test_path)
test_data
</code></pre><pre tabindex="0"><code>factor_cols = [&#39;Name&#39;, &#39;Sex&#39;]
for col in factor_cols:
    test_data[col] = test_data[col].asfactor()
tmp = test_data.drop([&#39;PassengerId&#39;], axis=1)

df_predict = aml.predict(tmp)
</code></pre><pre tabindex="0"><code>res = pd.DataFrame({
    &#39;PassengerId&#39;: test_data.as_data_frame()[&#39;PassengerId&#39;].tolist(),
    &#39;Survived&#39;: [1 if e &gt; threshold else 0 for e in df_predict.as_data_frame()[&#39;predict&#39;].tolist()]
})

res
</code></pre><pre tabindex="0"><code>res_path = util.gen_abspath(DIRECTORY, PREDICT_FILE)
res.to_csv(res_path, index=False)
</code></pre><br>
<center>
<button class="demo-btn" onclick="window_on('https://nbviewer.org/github/luochang212/AI-Project/blob/main/scratch/8.automl.ipynb')">查看示例</button>
</center>
<br>
<div id="mini-overlay" onclick="overlay_off()"></div>
<div id="mini-window"><iframe id="mini-iframe" frameBorder="0"></iframe></div>
<button id="btn-close" onclick="overlay_off()">×</button>
<script src="/python-tips/overlay.js"></script>
<link rel="stylesheet" href="/python-tips/style.css">

  </div>

  <div class="post-footer">
    <p style="margin-bottom: 1rem;">欢迎关注我的其它发布渠道：</p>
    <div class="social-links" style="display: flex; flex-direction: column; gap: 10px;">
      
      <a href="https://twitter.com/_stellar_tide" target="_blank" rel="noopener noreferrer">
        <i class="fab fa-twitter"></i> X
      </a>
      
      
      <a href="https://github.com/luochang212" target="_blank" rel="noopener noreferrer">
        <i class="fab fa-github"></i> GitHub
      </a>
      
      
      <a href="https://www.luochang.ink/images/wechat.jpg" target="_blank" rel="noopener noreferrer">
        <i class="fab fa-weixin"></i> 公众号
      </a>
      
      <a href="https://www.zhihu.com/people/Fashionable" target="_blank" rel="noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="2" 
             stroke-linecap="round" stroke-linejoin="round" style="width: 1em; height: 1em; vertical-align: -0.125em; display: inline-block;">
          <path d="m13.3334,3.60098l0,17.1471l1.79582,0l0.75424,2.13703l3.18459,-2.13703l3.93584,0l0,-17.1471l-9.6705,0zm7.41375,14.87538l-1.79283,0l-2.24777,1.50849l-0.53276,-1.50849l-0.53875,0l0,-12.53483l5.10911,0l0,12.53483l0.00299,0zm-8.56906,-7.18927l-3.97475,0c0.06285,-1.34387 0.1287,-3.12174 0.19754,-5.17496l3.91788,0l-0.00299,-0.24244c0,-0.01796 -0.00599,-0.43998 -0.06884,-0.87097c-0.06285,-0.44896 -0.19754,-1.04457 -0.62854,-1.04457l-6.5727,0c0.13169,-0.61657 0.46991,-2.08615 0.87995,-2.80747l0.19155,-0.33522l-0.3861,-0.02095c-0.02394,0 -0.58663,-0.02694 -1.23912,0.31726c-1.06851,0.56868 -1.5474,1.68807 -1.75691,2.52612c-0.55072,2.18791 -1.33489,3.70837 -1.66712,4.35786c-0.09877,0.19155 -0.15863,0.30529 -0.18557,0.38311c-0.05387,0.14666 -0.02394,0.29332 0.0838,0.38909c0.31427,0.28434 1.14334,-0.0868 1.15232,-0.08979c0.01796,-0.00898 0.03891,-0.01796 0.06585,-0.02993c0.41603,-0.18856 1.64916,-0.74826 2.08914,-2.52911l1.69705,0c0.02095,0.96376 0.09278,4.14236 0.0868,5.17496l-4.22018,0l-0.06285,0.0449c-0.69139,0.50582 -0.91288,1.8916 -0.92185,1.95146l-0.0419,0.27536l4.99837,0c-0.36814,2.34355 -0.79315,3.3941 -1.01763,3.81313c-0.11074,0.20951 -0.21849,0.41902 -0.32025,0.62255c-0.63752,1.26306 -1.29898,2.56802 -3.7802,4.5973c-0.10775,0.0838 -0.20951,0.23944 -0.14367,0.41005c0.07183,0.18856 0.27835,0.27237 0.73629,0.27237c0.16162,0 0.35318,-0.00898 0.58065,-0.02993c1.49352,-0.13169 3.01698,-0.53875 4.04359,-2.6219c0.50882,-1.05056 0.94879,-2.14601 1.31394,-3.25941l4.08549,4.78886l0.14965,-0.35916c0.02394,-0.05687 0.56868,-1.38578 0.15264,-2.87032l-0.01497,-0.05387l-3.23547,-3.68143l-0.65847,0.49684c0.19155,-0.78118 0.31726,-1.49352 0.37413,-2.12805l4.74995,0l0,-0.23944c0,-1.20021 -0.55371,-1.91255 -0.57466,-1.94248l-0.07183,-0.08979z" />
        </svg> 知乎
      </a>

      <a href="/index.xml" target="_blank" rel="noopener noreferrer">
        <i class="fas fa-rss"></i> RSS
      </a>

    </div>
    <div style="margin-bottom: 5rem;"></div>
  </div>

  

<div class="navigation navigation-single">
    
    <a href="/posts/linux_handbook/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Linux 运维手册</span>
    </a>
    
    
    <a href="/posts/raspberry_pi_5/" class="navigation-next">
      <span class="navigation-tittle">树莓派 5 装机指南</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>

        </div>
        
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>





<script src="https://kit.fontawesome.com/2134995a39.js" crossorigin="anonymous"></script>

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    



<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.js"></script>
<script type="text/javascript">
  if (tocbot) {
    tocbot.init({
      
      tocSelector: '.toc',
      
      contentSelector: '.post',
      
      headingSelector: 'h2, h3, h4',
      collapseDepth: 4
    });
  }
</script>



    



    </body>
</html>
