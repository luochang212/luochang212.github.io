{"version":"1","records":[{"hierarchy":{"lvl1":"快速入门"},"type":"lvl1","url":"/quickstart","position":0},{"hierarchy":{"lvl1":"快速入门"},"content":"本 Notebook 基于 LangChain v1.0、LangGraph v1.0 撰写。\n\n!pip show langchain\n\n!pip show langgraph\n\n","type":"content","url":"/quickstart","position":1},{"hierarchy":{"lvl1":"快速入门","lvl2":"一、环境配置"},"type":"lvl2","url":"/quickstart#id","position":2},{"hierarchy":{"lvl1":"快速入门","lvl2":"一、环境配置"},"content":"1）安装依赖\n\n在命令行中安装 Python 依赖：pip install -r requirement.txt\n\n2）导入依赖\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\n\n# 加载模型配置\n# 请事先在 .env 中配置 DASHSCOPE_API_KEY\n_ = load_dotenv()\n\n","type":"content","url":"/quickstart#id","position":3},{"hierarchy":{"lvl1":"快速入门","lvl2":"二、简单的 Agent"},"type":"lvl2","url":"/quickstart#id-agent","position":4},{"hierarchy":{"lvl1":"快速入门","lvl2":"二、简单的 Agent"},"content":"\n\n# 配置大模型服务\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n)\n\n# 创建一个简单的Agent\nagent = create_agent(\n    model=llm,\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# 运行Agent\nresponse = agent.invoke({'messages': '你好'})\n\nresponse['messages'][-1].content\n\n# 可视化 Agent\nagent\n\n","type":"content","url":"/quickstart#id-agent","position":5},{"hierarchy":{"lvl1":"快速入门","lvl2":"三、带工具调用的 Agent"},"type":"lvl2","url":"/quickstart#id-agent-1","position":6},{"hierarchy":{"lvl1":"快速入门","lvl2":"三、带工具调用的 Agent"},"content":"\n\n# 一个工具函数\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\n# 创建带工具调用的Agent\ntool_agent = create_agent(\n    model=llm,\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# 运行Agent\nresponse = tool_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n\nresponse['messages'][-1].content\n\n# 可视化 Agent\ntool_agent\n\n","type":"content","url":"/quickstart#id-agent-1","position":7},{"hierarchy":{"lvl1":"快速入门","lvl2":"四、使用 ToolRuntime 控制工具权限"},"type":"lvl2","url":"/quickstart#id-toolruntime","position":8},{"hierarchy":{"lvl1":"快速入门","lvl2":"四、使用 ToolRuntime 控制工具权限"},"content":"\n\nfrom typing import Literal, Any\nfrom pydantic import BaseModel\nfrom langchain.tools import tool, ToolRuntime\n\nclass Context(BaseModel):\n    authority: Literal[\"admin\", \"user\"]\n\n# 创建带权限控制的tool，依赖ToolRuntime的内容进行判断\n@tool\ndef math_add(runtime: ToolRuntime[Context, Any], a: int, b: int) -> int:\n    \"\"\"Add two numbers together.\"\"\"\n    authority = runtime.context.authority\n    # 只有admin用户可以访问加法工具\n    if authority != \"admin\":\n        raise PermissionError(\"User does not have permission to add numbers\")\n    return a + b\n\n# 创建带工具调用的Agent\ntool_agent = create_agent(\n    model=llm,\n    tools=[get_weather, math_add],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# 在运行Agent时注入context\nresponse = tool_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"请计算 8234783 + 94123832 = ?\"}]},\n    config={\"configurable\": {\"thread_id\": \"1\"}},\n    context=Context(authority=\"admin\"),\n)\n\nfor message in response['messages']:\n    message.pretty_print()\n\n# 验证计算结果是否正确\n8234783 + 94123832\n\n","type":"content","url":"/quickstart#id-toolruntime","position":9},{"hierarchy":{"lvl1":"快速入门","lvl2":"五、结构化输出"},"type":"lvl2","url":"/quickstart#id-1","position":10},{"hierarchy":{"lvl1":"快速入门","lvl2":"五、结构化输出"},"content":"如果我们希望获得结构化输出（structured output），可以在 create_agent 函数中增加 response_format 参数。create_agent 会自动处理结构化输出。\n\n首先约定输出格式。对于上面两数相加的例子，我们只需要输出一个字段 output，输出的数据结构如下：\n\nfrom pydantic import BaseModel, Field\n\nclass CalcInfo(BaseModel):\n    \"\"\"Calculation information.\"\"\"\n    output: int = Field(description=\"The calculation result\")\n\n# 创建带结构化输出的Agent\nstructured_agent = create_agent(\n    model=llm,\n    tools=[get_weather, math_add],\n    system_prompt=\"You are a helpful assistant\",\n    response_format=CalcInfo,\n)\n\nresponse = structured_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"请计算 8234783 + 94123832 = ?\"}]},\n    config={\"configurable\": {\"thread_id\": \"1\"}},\n    context=Context(authority=\"admin\"),\n)\n\nfor message in response['messages']:\n    message.pretty_print()\n\nresponse['messages'][-1]\n\n","type":"content","url":"/quickstart#id-1","position":11},{"hierarchy":{"lvl1":"快速入门","lvl2":"六、流式输出"},"type":"lvl2","url":"/quickstart#id-2","position":12},{"hierarchy":{"lvl1":"快速入门","lvl2":"六、流式输出"},"content":"更多信息请查看 \n\nstreaming.\n\nagent = create_agent(\n    model=llm,\n    tools=[get_weather],\n)\n\nfor chunk in agent.stream(  \n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"updates\",\n):\n    for step, data in chunk.items():\n        print(f\"step: {step}\")\n        print(f\"content: {data['messages'][-1].content_blocks}\")\n\n扩展阅读：\n\nLangChain\n\nLangGraph\n\nDeep Agents\n\nLangMem\n\nlanggraph-101","type":"content","url":"/quickstart#id-2","position":13},{"hierarchy":{"lvl1":"Deep Agents"},"type":"lvl1","url":"/deep-agents","position":0},{"hierarchy":{"lvl1":"Deep Agents"},"content":"你可以把 \n\ndeepagents 看作 LangChain 团队出品的 DeepResearch.\n\n由于本 tutorial 写到现在已经又臭又长了，所以这里就 quickstart 一下。剩下的部分请大家去官网看吧！\n\n# !pip install deepagents ddgs\n\n1）加载模型\n\nimport os\nfrom datetime import datetime\nfrom dotenv import load_dotenv\nfrom ddgs import DDGS\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\nfrom deepagents import create_deep_agent\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 加载模型\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n    temperature=0.7,\n)\n\n2）创建搜索工具\n\nDDGS 是我找到的一款免费的、无需 API_KEY即可直接使用的网络搜索服务。我们用它替代官方推荐的 Tavily.\n\n# 创建 ddgs 客户端\nddgs = DDGS()\n\ndef internet_search(\n    query: str,\n    max_results: int = 3,\n) -> str:\n    \"\"\"\n    使用互联网搜索指定关键词并返回格式化结果。\n\n    Args:\n        query: 搜索关键词或问题。\n        max_results: 返回的最大结果条数。\n\n    Returns:\n        包含每条搜索结果的标题、摘要与链接的字符串。\n    \"\"\"\n    results = list(\n        ddgs.text(\n            query=query,\n            region=\"wt-wt\",  # wt-wt zh\n            timelimit='y',\n            safesearch='off',  # moderate off\n            page=1,\n            backend='auto',\n            max_results=max_results,\n        )\n    )\n\n    content = \"\"\n    for i, r in enumerate(results, 1):\n        content += f\"【结果 {i}】\\n\"\n        content += f\"标题: {r['title']}\\n\"\n        content += f\"摘要: {r['body']}\\n\"\n        content += f\"链接: {r['href']}\\n\\n\"\n\n    return content\n\n# 测试\nres = internet_search(query=\"美食 四川\")\nprint(res)\n\n3）创建深度代理\n\n# System prompt to steer the agent to be an expert researcher\nresearch_instructions = \"\"\"你是一名资深研究员。你的工作是进行全面深入的研究，并撰写一份精炼的报告。\n\n你可以使用互联网搜索工具作为获取信息的主要方式。\n\n## `internet_search`\n\n使用该工具对指定查询进行互联网搜索。你可以设置返回结果的最大数量。\n\n对于该工具的 query 参数，每次最多输入 **2个** 关键词。且关键词之间必须用空格分开。若不遵守此条规定，工具将返回无意义内容。\n\n正确用例：\n\n- 南京\n- 美食 四川\n\n错误用例：\n\n- 美食 四川 2025\n\n注意，当关键词数量为2个的时候，必须将2个词中更重要的那个放在前面。\n\"\"\"\n\n@tool\ndef get_today_date() -> str:\n    \"\"\"获取今天的日期\"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d\")\n\nagent = create_deep_agent(\n    model=llm,\n    tools=[internet_search, get_today_date],\n    system_prompt=research_instructions\n)\n\n4）运行 Agent\n\n我们的问题是：新上任的玻利维亚总统是谁？请介绍一下这位总统。\n\n因为昨天（2025 年 11 月 8 日）玻利维亚总统 罗德里戈・帕斯・佩雷拉 刚刚上任，这条信息肯定不在预训练数据里。所以可以用这个问题验证 Deep Agents 是否真的联网了。\n\nresult = agent.invoke({\"messages\": [\n    {\"role\": \"user\", \"content\": \"新上任的玻利维亚总统是谁？请介绍一下这位总统。\"}\n]})\n\n# 最终回复\nprint(result[\"messages\"][-1].content)\n\n# 思考过程\nfor message in result[\"messages\"]:\n    message.pretty_print()","type":"content","url":"/deep-agents","position":1},{"hierarchy":{"lvl1":"调试界面"},"type":"lvl1","url":"/langgraph-cli","position":0},{"hierarchy":{"lvl1":"调试界面"},"content":"LangGraph 还贴心地为大家提供了方便的调试界面 \n\nLangGraph CLI。\n\n但这部分功能不是端侧的。也就是说，它必须联网使用。而且你的数据会经过 LangChain 团队的服务器，所以 请不要在该页面上使用敏感数据。\n\n该调试界面由 LangSmith 提供，它长这样：\n\n","type":"content","url":"/langgraph-cli","position":1},{"hierarchy":{"lvl1":"调试界面","lvl2":"一、langsmith"},"type":"lvl2","url":"/langgraph-cli#id-langsmith","position":2},{"hierarchy":{"lvl1":"调试界面","lvl2":"一、langsmith"},"content":"LangSmith 是由 LangChain 团队推出的 LLM 应用的数据分析平台，它可以帮助开发者可视化地管理和优化整个应用开发流程。LangSmith 是一个付费的 PaaS，但提供了部分免费的功能。\n\n","type":"content","url":"/langgraph-cli#id-langsmith","position":3},{"hierarchy":{"lvl1":"调试界面","lvl2":"二、langgraph-cli"},"type":"lvl2","url":"/langgraph-cli#id-langgraph-cli","position":4},{"hierarchy":{"lvl1":"调试界面","lvl2":"二、langgraph-cli"},"content":"","type":"content","url":"/langgraph-cli#id-langgraph-cli","position":5},{"hierarchy":{"lvl1":"调试界面","lvl3":"1）安装依赖","lvl2":"二、langgraph-cli"},"type":"lvl3","url":"/langgraph-cli#id-1","position":6},{"hierarchy":{"lvl1":"调试界面","lvl3":"1）安装依赖","lvl2":"二、langgraph-cli"},"content":"在使用该调试界面前，需要先安装依赖：pip install \"langgraph-cli[inmem]\"","type":"content","url":"/langgraph-cli#id-1","position":7},{"hierarchy":{"lvl1":"调试界面","lvl3":"2）开发 Agent 后端","lvl2":"二、langgraph-cli"},"type":"lvl3","url":"/langgraph-cli#id-2-agent","position":8},{"hierarchy":{"lvl1":"调试界面","lvl3":"2）开发 Agent 后端","lvl2":"二、langgraph-cli"},"content":"这里，我们开发一个简单的 Agent 作为该调试界面的后端。代码如下：import os\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 配置大模型服务\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n)\n\n# 创建Agent\nagent = create_agent(model=llm)\n\n# langgraph-cli 入口函数\ndef get_app():\n    return agent\n\n\nSource: \n\nsimple_agent.py","type":"content","url":"/langgraph-cli#id-2-agent","position":9},{"hierarchy":{"lvl1":"调试界面","lvl3":"3）编写配置文件","lvl2":"二、langgraph-cli"},"type":"lvl3","url":"/langgraph-cli#id-3","position":10},{"hierarchy":{"lvl1":"调试界面","lvl3":"3）编写配置文件","lvl2":"二、langgraph-cli"},"content":"langgraph-cli 默认的配置文件名叫 langgraph.json. 当然你也可以不叫这个名字，但是要在接下来的步骤，使用 --config 参数指定该 json 文件。{\n    \"dependencies\": [\n        \"./\"\n    ],\n    \"graphs\": {\n        \"supervisor\": \"./simple_agent.py:get_app\"\n    },\n    \"env\": \"./.env\"\n}","type":"content","url":"/langgraph-cli#id-3","position":11},{"hierarchy":{"lvl1":"调试界面","lvl3":"4）启动 langgraph-cli","lvl2":"二、langgraph-cli"},"type":"lvl3","url":"/langgraph-cli#id-4-langgraph-cli","position":12},{"hierarchy":{"lvl1":"调试界面","lvl3":"4）启动 langgraph-cli","lvl2":"二、langgraph-cli"},"content":"在命令行启动它：# 如果你的配置文件是默认的 langgraph.json\nlanggraph dev\n\n# 如果你的配置文件是 [your_agent].json\nlanggraph dev --config [your_agent].json\n\n运行后会自动打开浏览器，跳转到调试页面。\n\n最后再友情提示一下，该页面并非自建页面，使用它将受到 隐私 和 联网 的限制。它不是开源的，这也是我将该篇介绍放到最末尾的原因。\n\n如果对这方面有顾虑的朋友，不妨参考我 \n\nclickhouse-chatbi 项目的解决方案。该项目使用了 Next.js 的 \n\nnextjs-ai-chatbot 模板。此模板不仅有简洁且现代的外观，还兼顾了易用性和拓展性，实乃快捷开发的不二之选。","type":"content","url":"/langgraph-cli#id-4-langgraph-cli","position":13},{"hierarchy":{"lvl1":"StateGraph"},"type":"lvl1","url":"/stategraph","position":0},{"hierarchy":{"lvl1":"StateGraph"},"content":"使用 StateGraph 构建有向循环图\n\n上一节，我们演示了如何使用 LangGraph 创建 Agent。然而，使用 Agent 调用 LLM 终究是隔了一层。我们将工具调用、流程控制的权力交给 Agent 的同时，也失去了对具体行为的掌控。\n\n现在我们回到 LangGraph 底层，使用 StateGraph 直接控制代码执行。\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langchain_core.messages import SystemMessage, HumanMessage\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import ToolNode\nfrom langchain_core.runnables import RunnableConfig\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 加载模型\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n    temperature=0.7,\n)\n\n# 工具函数\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\n# 创建工具节点\ntools = [get_weather]\ntool_node = ToolNode(tools)\n\n# 创建助手节点\ndef assistant(state: MessagesState, config: RunnableConfig):\n    system_prompt = 'You are a helpful assistant that can check weather.'\n    all_messages = [SystemMessage(system_prompt)] + state['messages']\n    model = llm.bind_tools(tools)\n    return {'messages': [model.invoke(all_messages)]}\n\n# 创建条件边\ndef should_continue(state: MessagesState, config: RunnableConfig):\n    messages = state['messages']\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return 'continue'\n    return 'end'\n\n# 创建图\nbuilder = StateGraph(MessagesState)\n\n# 添加节点\nbuilder.add_node('assistant', assistant)\nbuilder.add_node('tool', tool_node)\n\n# 添加边\nbuilder.add_edge(START, 'assistant')\n\n# 添加条件边\nbuilder.add_conditional_edges(\n    'assistant',\n    should_continue,\n    {\n        'continue': 'tool',\n        'end': END,\n    },\n)\n\n# 添加边：调用工具节点后回到assistant\nbuilder.add_edge('tool', 'assistant')\n\n# 编译图\nmy_graph = builder.compile(name='my-graph')\nmy_graph\n\n有向图分为：\n\n有向无环图（DAG）\n\n有向循环图（DCG）\n\n从上图可以看出，我们使用 StateGraph 创建的是一个有向循环图。\n\n# 调用图\nresponse = my_graph.invoke({'messages': [HumanMessage(content='上海天气怎么样？')]})\nfor message in response['messages']:\n    message.pretty_print()","type":"content","url":"/stategraph","position":1},{"hierarchy":{"lvl1":"中间件"},"type":"lvl1","url":"/middleware","position":0},{"hierarchy":{"lvl1":"中间件"},"content":"","type":"content","url":"/middleware","position":1},{"hierarchy":{"lvl1":"中间件","lvl2":"一、预算控制"},"type":"lvl2","url":"/middleware#id","position":2},{"hierarchy":{"lvl1":"中间件","lvl2":"一、预算控制"},"content":"利用运行时（runtime）和上下文（context），我们可以动态地更改模型配置。这种动态性能帮助我们更好地控制模型。\n\n一个实用的场景是「预算控制」。随着对话轮次增加，积累的历史对话越来越多，每次请求的费用也随之增加。为了控制预算，我们可以设定在对话超过某个轮次之后，切换到费率较低的模型。该功能可以通过中间件实现。\n\n事实上，中间件（middleware）能实现的功能有很多，可以将它视为 Agent 的万能控制接口。LangChain v1.0 通过引入中间件，极大增强了对 Agent 的掌控力。\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import MessagesState\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 低费率模型\nbasic_model = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n)\n\n# 高费率模型\nadvanced_model = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-max\",\n)\n\n具体来讲，使用 \n\n 装饰器可以创建控制模型的中间件。\n\n@wrap_model_call\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n    \"\"\"Choose model based on conversation complexity.\"\"\"\n    message_count = len(request.state[\"messages\"])\n\n    if message_count > 5:\n        # Use an advanced model for longer conversations\n        model = advanced_model\n    else:\n        model = basic_model\n\n    request.model = model\n    print(f\"message_count: {message_count}\")\n    print(f\"model_name: {model.model_name}\")\n\n    return handler(request)\n\nagent = create_agent(\n    model=basic_model,  # Default model\n    middleware=[dynamic_model_selection]\n)\n\nstate: MessagesState = {\"messages\": []}\nitems = ['汽车', '飞机', '摩托车', '自行车']\nfor idx, i in enumerate(items):\n    print(f\"\\n=== Round {idx+1} ===\")\n    state[\"messages\"] += [HumanMessage(content=f\"{i}有几个轮子，请简单回答\")]\n    result = agent.invoke(state)\n    state[\"messages\"] = result[\"messages\"]\n    print(f\"content: {result[\"messages\"][-1].content}\")\n\n","type":"content","url":"/middleware#id","position":3},{"hierarchy":{"lvl1":"中间件","lvl2":"二、消息截断"},"type":"lvl2","url":"/middleware#id-1","position":4},{"hierarchy":{"lvl1":"中间件","lvl2":"二、消息截断"},"content":"智能体系统的上下文长度是有限的，若超过限制，就要想办法压缩上下文。在所有方法中，最简单粗暴的方法就是截断。消息截断的功能可以用 @before_model 装饰器实现。\n\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import before_model\nfrom langgraph.runtime import Runtime\nfrom langchain_core.runnables import RunnableConfig\nfrom typing import Any\n\n在下面的例子中，由于我们始终保留第一条消息，因此智能体总是记得我叫 bob。\n\n@before_model\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    if len(messages) <= 3:\n        return None  # No changes needed\n\n    first_msg = messages[0]\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n    new_messages = [first_msg] + recent_messages\n\n    return {\n        \"messages\": [\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n            *new_messages\n        ]\n    }\n\nagent = create_agent(\n    basic_model,\n    middleware=[trim_messages],\n    checkpointer=InMemorySaver(),\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\ndef agent_invoke(agent):\n    agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n    agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n    agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n    final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n    \n    final_response[\"messages\"][-1].pretty_print()\n\nagent_invoke(agent)\n\n我们对中间件进行一些修改，仅保留最后两条对话记录，现在智能体不记得我是 bob 了。\n\n@before_model\ndef trim_without_first_message(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    return {\n        \"messages\": [\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n            *messages[-2:]\n        ]\n    }\n\nagent = create_agent(\n    basic_model,\n    middleware=[trim_without_first_message],\n    checkpointer=InMemorySaver(),\n)\n\nagent_invoke(agent)\n\n","type":"content","url":"/middleware#id-1","position":5},{"hierarchy":{"lvl1":"中间件","lvl2":"三、护栏：敏感词过滤"},"type":"lvl2","url":"/middleware#id-2","position":6},{"hierarchy":{"lvl1":"中间件","lvl2":"三、护栏：敏感词过滤"},"content":"智能体能在模型之外提供额外的安全性，我们一般把这种安全能力称为护栏（Guardrails）。在 LangGraph 中，护栏可以通过中间件实现。下面通过两个具体的例子，演示如何使用护栏提升智能体的安全性。\n\n我们先实现一个简单的安全策略：检测用户最新一条输入，若其中包含指定的敏感词，则智能体拒绝回答用户的问题。\n\nfrom typing import Any\n\nfrom langchain.agents.middleware import before_agent, AgentState, hook_config\nfrom langgraph.runtime import Runtime\n\nbanned_keywords = [\"hack\", \"exploit\", \"malware\"]\n\n@before_agent(can_jump_to=[\"end\"])\ndef content_filter(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n    # Get the first user message\n    if not state[\"messages\"]:\n        return None\n\n    last_message = state[\"messages\"][-1]\n    if last_message.type != \"human\":\n        return None\n\n    content = last_message.content.lower()\n\n    # Check for banned keywords\n    for keyword in banned_keywords:\n        if keyword in content:\n            # Block execution before any processing\n            return {\n                \"messages\": [{\n                    \"role\": \"assistant\",\n                    \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n                }],\n                \"jump_to\": \"end\"\n            }\n\n    return None\n\nagent = create_agent(\n    model=basic_model,\n    middleware=[content_filter],\n)\n\n# This request will be blocked before any processing\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n})\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n\n","type":"content","url":"/middleware#id-2","position":7},{"hierarchy":{"lvl1":"中间件","lvl2":"四、护栏：PII 检测"},"type":"lvl2","url":"/middleware#id-pii","position":8},{"hierarchy":{"lvl1":"中间件","lvl2":"四、护栏：PII 检测"},"content":"PII（Personally Identifiable Information）检测是一个常见的、用于过滤用户敏感信息的功能。它能检测用户的电子邮件、信用卡、IP 地址等敏感信息，以防止信息泄漏。\n\n在下面的例子中，我们使用 LLM 检测敏感信息，并使用两种策略进行处理。\n\nfrom textwrap import dedent\nfrom pydantic import BaseModel, Field\n\n# 可信任的模型，一般是本地模型，为了方便，这里依然使用qwen\ntrusted_model = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n)\n\n# 用于格式化智能体输出，若发现敏感信息返回Ture，没发现返回False\nclass PiiCheck(BaseModel):\n    \"\"\"Structured output indicating whether text contains PII.\"\"\"\n    is_pii: bool = Field(description=\"Whether the text contains PII\")\n\ndef message_with_pii(pii_middleware):\n    agent = create_agent(\n        model=basic_model,\n        middleware=[pii_middleware],\n    )\n\n    # This request will be blocked before any processing\n    result = agent.invoke({\n        \"messages\": [{\n            \"role\": \"user\",\n            \"content\": dedent(\n                \"\"\"\n                File \"/home/luochang/proj/agent.py\", line 53, in my_agent\n                    agent = create_react_agent(\n                            ^^^^^^^^^^^^^^^^^^^\n                File \"/home/luochang/miniconda3/lib/python3.12/site-packages/typing_extensions.py\", line 2950, in wrapper\n                    return arg(*args, **kwargs)\n                        ^^^^^^^^^^^^^^^^^^^^\n                File \"/home/luochang/miniconda3/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 566, in create_react_agent\n                    model = cast(BaseChatModel, model).bind_tools(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                AttributeError: 'RunnableLambda' object has no attribute 'bind_tools'\n    \n                ---\n    \n                为啥报错\n                \"\"\").strip()\n        }]\n    })\n\n    return result\n\n策略一：如遇敏感信息，拒绝回复。\n\n@before_agent(can_jump_to=[\"end\"])\ndef content_blocker(state: AgentState,  runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n    # Get the first user message\n    if not state[\"messages\"]:\n        return None\n\n    last_message = state[\"messages\"][-1]\n    if last_message.type != \"human\":\n        return None\n\n    content = last_message.content.lower()\n    prompt = (\n        \"你是一个隐私保护助手。请识别下面文本中涉及个人可识别信息（PII），\"\n        \"例如：姓名、身份证号、护照号、电话号码、邮箱、住址、银行卡号、社交账号、车牌等。\"\n        \"特别注意，若代码、文件路径中包含用户名，也应被视为敏感信息。\"\n        \"若包含敏感信息，请返回{\\\"is_pii\\\": True}，否则返回{\\\"is_pii\\\": False}。\"\n        \"请严格以 json 格式返回，并且只输出 json。文本如下：\\n\\n\" + content\n    )\n\n    pii_agent = trusted_model.with_structured_output(PiiCheck)\n    result = pii_agent.invoke(prompt)\n\n    if result.is_pii is True:\n        # Block execution before any processing\n        return {\n            \"messages\": [{\n                \"role\": \"assistant\",\n                \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n            }],\n            \"jump_to\": \"end\"\n        }\n    else:\n        print(\"No PII found\")\n\n    return None\n\nresult = message_with_pii(pii_middleware=content_blocker)\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n\n策略二：如遇敏感信息，使用 * 号屏蔽敏感信息。\n\n@before_agent(can_jump_to=[\"end\"])\ndef content_filter(state: AgentState,  runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n    # Get the first user message\n    if not state[\"messages\"]:\n        return None\n\n    last_message = state[\"messages\"][-1]\n    if last_message.type != \"human\":\n        return None\n\n    content = last_message.content.lower()\n    prompt = (\n        \"你是一个隐私保护助手。请识别下面文本中涉及个人可识别信息（PII），\"\n        \"例如：姓名、身份证号、护照号、电话号码、邮箱、住址、银行卡号、社交账号、车牌等。\"\n        \"特别注意，若代码、文件路径中包含用户名，也应被视为敏感信息。\"\n        \"若包含敏感信息，请返回{\\\"is_pii\\\": True}，否则返回{\\\"is_pii\\\": False}。\"\n        \"请严格以 json 格式返回，并且只输出 json。文本如下：\\n\\n\" + content\n    )\n\n    pii_agent = trusted_model.with_structured_output(PiiCheck)\n    result = pii_agent.invoke(prompt)\n\n    if result.is_pii is True:\n        mask_prompt = (\n            \"你是一个隐私保护助手。请将下面文本中的所有个人可识别信息（PII）用星号（*）替换。\"\n            \"仅替换敏感片段，其他文本保持不变。\"\n            \"只输出处理后的文本，不要任何解释或额外内容。文本如下：\\n\\n\" + last_message.content\n        )\n        masked_message = basic_model.invoke(mask_prompt)\n        return {\n            \"messages\": [{\n                \"role\": \"assistant\",\n                \"content\": masked_message.content\n            }]\n        }\n    else:\n        print(\"No PII found\")\n\n    return None\n\nresult = message_with_pii(pii_middleware=content_filter)\n\nfor message in result[\"messages\"]:\n    message.pretty_print()","type":"content","url":"/middleware#id-pii","position":9},{"hierarchy":{"lvl1":"人机交互"},"type":"lvl1","url":"/human-in-the-loop","position":0},{"hierarchy":{"lvl1":"人机交互"},"content":"人机交互（Human-in-the-loop, HITL）中间件用于在智能体执行过程中加入人工审批。\n\n当触发人工审批时，HITL 中间件会\n\n中断程序执行。此时，LangGraph 的状态保存在 \n\ncheckpointer 中，因此程序可以安全地暂停，并在人工审批之后恢复。\n\n本文只是演示 checkpoint 在人机交互中的作用，无所谓线程结束后记忆是否保持，因此使用最方便的 InMemorySaver。在生产环节中，推荐使用由数据库支持的检查点，比如：\n\nSqliteSaver\n\nPostgresSaver\n\nMongoDBSaver\n\nRedisSaver\n\nimport os\nimport uuid\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import Command\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 配置大模型服务\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n)\n\n# 工具函数\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\n@tool\ndef add_numbers(a: float, b: float) -> float:\n    \"\"\"Add two numbers and return the sum.\"\"\"\n    return a + b\n\n@tool\ndef calculate_bmi(weight_kg: float, height_m: float) -> float:\n    \"\"\"Calculate BMI given weight in kg and height in meters.\"\"\"\n    if height_m <= 0 or weight_kg <= 0:\n        raise ValueError(\"height_m and weight_kg must be greater than 0.\")\n    return weight_kg / (height_m ** 2)\n\n# 创建带工具调用的Agent\ntool_agent = create_agent(\n    model=llm,\n    tools=[get_weather, add_numbers, calculate_bmi],\n    middleware=[\n        HumanInTheLoopMiddleware( \n            interrupt_on={\n                # 无需触发人工审批\n                \"get_weather\": False,\n                # 需要审批，且允许approve,edit,reject三种审批类型\n                \"add_numbers\": True,\n                # 需要审批，允许approve,reject两种审批类型\n                \"calculate_bmi\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n            },\n            description_prefix=\"Tool execution pending approval\",\n        ),\n    ],\n    checkpointer=InMemorySaver(),\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# 运行Agent\nconfig = {'configurable': {'thread_id': str(uuid.uuid4())}}\nresult = tool_agent.invoke(\n    {\"messages\": [{\n        \"role\": \"user\",\n        \"content\": \"我身高180cm，体重180斤，我的BMI是多少\"\n        # \"content\": \"what is the weather in sf\"\n    }]},\n    config=config,\n)\n\n# result['messages'][-1].content\nresult.get('__interrupt__')\n\n# Resume with approval decision\nresult = tool_agent.invoke(\n    Command(\n        resume={\"decisions\": [{\"type\": \"approve\"}]}  # or \"edit\", \"reject\"\n    ), \n    config=config\n)\n\nresult['messages'][-1].content\n\n参考文档：\n\nlangchain​/human​-in​-the​-loop\n\nlangchain​/short​-term​-memory\n\nlangchain​/long​-term​-memory\n\nlanggraph​/persistence\n\nlanggraph​/use​-time​-travel\n\nlanggraph​/add​-memory","type":"content","url":"/human-in-the-loop","position":1},{"hierarchy":{"lvl1":"记忆"},"type":"lvl1","url":"/memory","position":0},{"hierarchy":{"lvl1":"记忆"},"content":"Memory 是一个可选组件。除非必要，你无需向智能体添加 memory 功能。\n\n常见的需要添加 memory 的场景包括：\n\n对话的上下文超过限制，需要记忆或压缩上下文\n\n触发人工干预（\n\ninterrupt）之后，需要存储智能体状态，并在人工干预后恢复\n\n需要跨对话提取用户偏好\n\n在 LangGraph 中，记忆功能被分为长、短两个模块。如果你想进一步了解，可以参阅它们的文档：\n\n短期记忆\n\n长期记忆\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 加载模型\nmodel = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n    temperature=0.7,\n)\n\n# 创建助手节点\ndef assistant(state: MessagesState):\n    return {'messages': [model.invoke(state['messages'])]}\n\n","type":"content","url":"/memory","position":1},{"hierarchy":{"lvl1":"记忆","lvl2":"一、短期记忆"},"type":"lvl2","url":"/memory#id","position":2},{"hierarchy":{"lvl1":"记忆","lvl2":"一、短期记忆"},"content":"短期记忆（工作记忆）一般用于临时存储，与当前对话内容强相关。与依赖上下文的记忆方式不同，短期记忆可以主动记住重要的内容，增加工程稳定性。","type":"content","url":"/memory#id","position":3},{"hierarchy":{"lvl1":"记忆","lvl3":"1）在 StateGraph 中使用短期记忆","lvl2":"一、短期记忆"},"type":"lvl3","url":"/memory#id-1-stategraph","position":4},{"hierarchy":{"lvl1":"记忆","lvl3":"1）在 StateGraph 中使用短期记忆","lvl2":"一、短期记忆"},"content":"为了方便演示，我们使用 InMemorySaver 存储短期记忆。这意味着短期记忆存储在内存中。如果退出当前程序，记忆将会消失。\n\n# 创建短期记忆\ncheckpointer = InMemorySaver()\n\n# 创建图\nbuilder = StateGraph(MessagesState)\n\n# 添加节点\nbuilder.add_node('assistant', assistant)\n\n# 添加边\nbuilder.add_edge(START, 'assistant')\nbuilder.add_edge('assistant', END)\n\ngraph = builder.compile(checkpointer=checkpointer)\n\n# 告诉智能体我叫 luochang\nresult = graph.invoke(\n    {'messages': ['hi! i am luochang']},\n    {\"configurable\": {\"thread_id\": \"1\"}},\n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n# 让智能体说出我的名字\nresult = graph.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is my name?\"}]},\n    {\"configurable\": {\"thread_id\": \"1\"}},  \n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n","type":"content","url":"/memory#id-1-stategraph","position":5},{"hierarchy":{"lvl1":"记忆","lvl3":"2）在 create_agent 中使用短期记忆","lvl2":"一、短期记忆"},"type":"lvl3","url":"/memory#id-2-create-agent","position":6},{"hierarchy":{"lvl1":"记忆","lvl3":"2）在 create_agent 中使用短期记忆","lvl2":"一、短期记忆"},"content":"\n\nfrom langchain.agents import create_agent\n\n# 创建短期记忆\ncheckpointer = InMemorySaver()\n\nagent = create_agent(\n    model=model,\n    checkpointer=checkpointer\n)\n\n# 告诉智能体我叫 luochang\nresult = agent.invoke(\n    {'messages': ['hi! i am luochang']},\n    {\"configurable\": {\"thread_id\": \"2\"}},\n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n# 让智能体说出我的名字\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is my name?\"}]},\n    {\"configurable\": {\"thread_id\": \"2\"}},  \n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n为了验证 InMemorySaver 是否真的有效果，可以将 checkpointer=checkpointer 注释后，再观察智能体能不能正确回复我的名字。\n\n","type":"content","url":"/memory#id-2-create-agent","position":7},{"hierarchy":{"lvl1":"记忆","lvl3":"3）使用外部数据库支持的短期记忆","lvl2":"一、短期记忆"},"type":"lvl3","url":"/memory#id-3","position":8},{"hierarchy":{"lvl1":"记忆","lvl3":"3）使用外部数据库支持的短期记忆","lvl2":"一、短期记忆"},"content":"如果使用 SQLite 保存当前工作状态，即使退出程序，依然能在下次进入时恢复上次退出时的状态，我们来测试这一点。\n\n在使用 SQLite 作为短期记忆的外部数据库之前，需要安装一个 Python 包以支持这项功能：pip install langgraph-checkpoint-sqlite\n\n# 删除SQLite数据库\nif os.path.exists(\"short-memory.db\"):\n    os.remove(\"short-memory.db\")\n\nimport os\nimport sqlite3\n\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langchain.agents import create_agent\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 加载模型\nmodel = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n    temperature=0.7,\n)\n\n# 创建sqlite支持的短期记忆\ncheckpointer = SqliteSaver(\n    sqlite3.connect(\"short-memory.db\", check_same_thread=False)\n)\n\n# 创建Agent\nagent = create_agent(\n    model=model,\n    checkpointer=checkpointer,\n)\n\n# 告诉智能体我叫 luochang\nresult = agent.invoke(\n    {'messages': ['hi! i am luochang']},\n    {\"configurable\": {\"thread_id\": \"3\"}},\n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n重启 Jupyter Notebook 后看智能体能否从 SQLite 中读取关于我名字的记忆。\n\n在 Kernel -> Restart Kernel... 中重启服务。然后运行以下代码。\n\nimport os\nimport sqlite3\n\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langchain.agents import create_agent\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 加载模型\nmodel = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n    temperature=0.7,\n)\n\n# 创建sqlite支持的短期记忆\ncheckpointer = SqliteSaver(\n    sqlite3.connect(\"short-memory.db\", check_same_thread=False)\n)\n\n# 创建Agent\nagent = create_agent(\n    model=model,\n    checkpointer=checkpointer,\n)\n\n# 让智能体回忆我的名字\nresult = agent.invoke(\n    {'messages': ['What is my name?']},\n    {\"configurable\": {\"thread_id\": \"3\"}},\n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n","type":"content","url":"/memory#id-3","position":9},{"hierarchy":{"lvl1":"记忆","lvl2":"二、长期记忆"},"type":"lvl2","url":"/memory#id-1","position":10},{"hierarchy":{"lvl1":"记忆","lvl2":"二、长期记忆"},"content":"\n\nimport os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.store.memory import InMemoryStore\nfrom dataclasses import dataclass\n\nEMBED_MODEL = \"text-embedding-v4\"\nEMBED_DIM = 1024\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 用于获取text embedding的接口\nclient = OpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n)\n\n# 加载模型\nmodel = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n    temperature=0.7,\n)\n\n# embedding生成函数\ndef embed(texts: list[str]) -> list[list[float]]:\n    response = client.embeddings.create(\n        model=EMBED_MODEL,\n        input=texts,\n        dimensions=EMBED_DIM,\n    )\n\n    return [item.embedding for item in response.data]\n\n# 测试能否正常生成text embedding\ntexts = [\n    \"LangGraph的中间件非常强大\",\n    \"LangGraph的MCP也很好用\",\n]\nvectors = embed(texts)\n\nlen(vectors), len(vectors[0])\n\n","type":"content","url":"/memory#id-1","position":11},{"hierarchy":{"lvl1":"记忆","lvl3":"1）直接读写长期记忆","lvl2":"二、长期记忆"},"type":"lvl3","url":"/memory#id-1-1","position":12},{"hierarchy":{"lvl1":"记忆","lvl3":"1）直接读写长期记忆","lvl2":"二、长期记忆"},"content":"\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nstore = InMemoryStore(index={\"embed\": embed, \"dims\": EMBED_DIM})\n\n# 添加两条用户数据\nnamespace = (\"users\", )\nkey = \"user_1\"\nstore.put(\n    namespace,\n    key,\n    {\n        \"rules\": [\n            \"User likes short, direct language\",\n            \"User only speaks English & python\",\n        ],\n        \"rule_id\": \"3\",\n    },\n)\n\nstore.put( \n    (\"users\",),  # Namespace to group related data together (users namespace for user data)\n    \"user_2\",  # Key within the namespace (user ID as key)\n    {\n        \"name\": \"John Smith\",\n        \"language\": \"English\",\n    }  # Data to store for the given user\n)\n\n# get the \"memory\" by ID\nitem = store.get(namespace, \"a-memory\") \n\n# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nitems = store.search( \n    namespace, filter={\"rule_id\": \"3\"}, query=\"language preferences\"\n)\n\nitems\n\n","type":"content","url":"/memory#id-1-1","position":13},{"hierarchy":{"lvl1":"记忆","lvl3":"2）使用工具读取长期记忆","lvl2":"二、长期记忆"},"type":"lvl3","url":"/memory#id-2","position":14},{"hierarchy":{"lvl1":"记忆","lvl3":"2）使用工具读取长期记忆","lvl2":"二、长期记忆"},"content":"\n\n@dataclass\nclass Context:\n    user_id: str\n\n@tool\ndef get_user_info(runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Look up user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store \n    user_id = runtime.context.user_id\n    # Retrieve data from store - returns StoreValue object with value and metadata\n    user_info = store.get((\"users\",), user_id) \n    return str(user_info.value) if user_info else \"Unknown user\"\n\nagent = create_agent(\n    model=model,\n    tools=[get_user_info],\n    # Pass store to agent - enables agent to access store when running tools\n    store=store, \n    context_schema=Context\n)\n\n# Run the agent\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n    context=Context(user_id=\"user_2\") \n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n","type":"content","url":"/memory#id-2","position":15},{"hierarchy":{"lvl1":"记忆","lvl3":"3）使用工具写入长期记忆","lvl2":"二、长期记忆"},"type":"lvl3","url":"/memory#id-3-1","position":16},{"hierarchy":{"lvl1":"记忆","lvl3":"3）使用工具写入长期记忆","lvl2":"二、长期记忆"},"content":"\n\nfrom typing_extensions import TypedDict\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nstore = InMemoryStore() \n\n@dataclass\nclass Context:\n    user_id: str\n\n# TypedDict defines the structure of user information for the LLM\nclass UserInfo(TypedDict):\n    name: str\n\n# Tool that allows agent to update user information (useful for chat applications)\n@tool\ndef save_user_info(user_info: UserInfo, runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Save user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store \n    user_id = runtime.context.user_id \n    # Store data in the store (namespace, key, data)\n    store.put((\"users\",), user_id, user_info) \n    return \"Successfully saved user info.\"\n\nagent = create_agent(\n    model=model,\n    tools=[save_user_info],\n    store=store,\n    context_schema=Context\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]},\n    # user_id passed in context to identify whose information is being updated\n    context=Context(user_id=\"user_123\") \n)\n\n# You can access the store directly to get the value\nstore.get((\"users\",), \"user_123\").value","type":"content","url":"/memory#id-3-1","position":17},{"hierarchy":{"lvl1":"上下文工程"},"type":"lvl1","url":"/context","position":0},{"hierarchy":{"lvl1":"上下文工程"},"content":"上下文工程（Context Engineering）对于 LLM 得出正确的结果至关重要。很多时候，模型回答不好并非因为模型能力不足，而是因为没有获得足以推断出正确结果的信息。LangGraph 可以通过上下文工程，增强上下文的管理能力。\n\n按发生作用的位置分，可将上下文分为：\n\n模型上下文（Model Context）\n\n工具上下文（Tool Context）\n\n生命周期上下文（Lift-cycle Context）\n\nLangGraph 提供了相当大的自由度，你可以使用 dataclasses、pydantic、TypedDict 中的任意一个创建 Context Schema.\n\n# !pip install ipynbname\n\nimport os\nimport uuid\nimport sqlite3\n\nfrom typing import Callable\nfrom dotenv import load_dotenv\nfrom dataclasses import dataclass\nfrom langchain_openai import ChatOpenAI\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, wrap_model_call, ModelRequest, ModelResponse, SummarizationMiddleware\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.store.sqlite import SqliteStore\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 加载模型\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n    temperature=0.7,\n)\n\n","type":"content","url":"/context","position":1},{"hierarchy":{"lvl1":"上下文工程","lvl2":"一、动态修改系统提示词"},"type":"lvl2","url":"/context#id","position":2},{"hierarchy":{"lvl1":"上下文工程","lvl2":"一、动态修改系统提示词"},"content":"上下文工程与前序章节的中间件（middleware）和记忆（memory）密不可分。上下文的具体实现依赖中间件，而上下文的存储则依赖记忆系统。具体来讲，LangGraph 预置了 @dynamic_prompt 中间件，用于动态修改系统提示词。\n\n既然是动态修改，肯定需要某个条件来触发修改。除了开发触发逻辑，我们还需要从智能体中获取触发逻辑所需的即时变量。这些变量通常存储在以下三个存储介质中：\n\n运行时（Runtime）- 所有节点共享一个 Runtime。同一时刻，所有节点取到的 Runtime 的值是相同的。一般用于存储时效性要求较高的信息。\n\n短期记忆（State）- 在节点之间按顺序传递，每个节点接收上一个节点处理后的 State。主要用于存储 Prompt 和 AI Message。\n\n长期记忆（Store）- 负责持久化存储，可以跨 Workflow / Agent 保存信息。可以用来存用户偏好、以前算过的统计值等。\n\n以下三个例子，分别演示如何使用来自 Runtime、State、Store 中的上下文，编写触发条件。","type":"content","url":"/context#id","position":3},{"hierarchy":{"lvl1":"上下文工程","lvl3":"1）使用 State 加载上下文","lvl2":"一、动态修改系统提示词"},"type":"lvl3","url":"/context#id-1-state","position":4},{"hierarchy":{"lvl1":"上下文工程","lvl3":"1）使用 State 加载上下文","lvl2":"一、动态修改系统提示词"},"content":"利用 State 中蕴含的信息操纵 system prompt.\n\n@dynamic_prompt\ndef state_aware_prompt(request: ModelRequest) -> str:\n    # request.messages is a shortcut for request.state[\"messages\"]\n    message_count = len(request.messages)\n\n    base = \"You are a helpful assistant.\"\n\n    if message_count > 6:\n        base += \"\\nThis is a long conversation - be extra concise.\"\n\n    # 临时打印base看效果\n    print(base)\n\n    return base\n\nagent = create_agent(\n    model=llm,\n    middleware=[state_aware_prompt]\n)\n\nresult = agent.invoke(\n    {\"messages\": [\n        {\"role\": \"user\", \"content\": \"广州今天的天气怎么样？\"},\n        {\"role\": \"assistant\", \"content\": \"广州天气很好\"},\n        {\"role\": \"user\", \"content\": \"吃点什么好呢\"},\n        {\"role\": \"assistant\", \"content\": \"要不要吃香茅鳗鱼煲\"},\n        {\"role\": \"user\", \"content\": \"香茅是什么\"},\n        {\"role\": \"assistant\", \"content\": \"香茅又名柠檬草，常见于泰式冬阴功汤、越南烤肉\"},\n        {\"role\": \"user\", \"content\": \"auv 那还等什么，咱吃去吧\"},\n    ]},\n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n把 message_count > 6 里的 6 改成 7，试试看会发生什么。\n\n","type":"content","url":"/context#id-1-state","position":5},{"hierarchy":{"lvl1":"上下文工程","lvl3":"2）使用 Store 加载上下文","lvl2":"一、动态修改系统提示词"},"type":"lvl3","url":"/context#id-2-store","position":6},{"hierarchy":{"lvl1":"上下文工程","lvl3":"2）使用 Store 加载上下文","lvl2":"一、动态修改系统提示词"},"content":"\n\n@dataclass\nclass Context:\n    user_id: str\n\n@dynamic_prompt\ndef store_aware_prompt(request: ModelRequest) -> str:\n    user_id = request.runtime.context.user_id\n\n    # Read from Store: get user preferences\n    store = request.runtime.store\n    user_prefs = store.get((\"preferences\",), user_id)\n\n    base = \"You are a helpful assistant.\"\n\n    if user_prefs:\n        style = user_prefs.value.get(\"communication_style\", \"balanced\")\n        base += f\"\\nUser prefers {style} responses.\"\n\n    return base\n\nstore = InMemoryStore()\n\nagent = create_agent(\n    model=llm,\n    middleware=[store_aware_prompt],\n    context_schema=Context,\n    store=store,\n)\n\n# 预置两条偏好信息\nstore.put((\"preferences\",), \"user_1\", {\"communication_style\": \"Chinese\"})\nstore.put((\"preferences\",), \"user_2\", {\"communication_style\": \"Korean\"})\n\n# 用户1喜欢中文回复\nresult = agent.invoke(\n    {\"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please be extra concise.\"},\n        {\"role\": \"user\", \"content\": 'What is a \"hold short line\"?'}\n    ]},\n    context=Context(user_id=\"user_1\"),\n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n# 用户2喜欢韩文回复\nresult = agent.invoke(\n    {\"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please be extra concise.\"},\n        {\"role\": \"user\", \"content\": 'What is a \"hold short line\"?'}\n    ]},\n    context=Context(user_id=\"user_2\"),\n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n","type":"content","url":"/context#id-2-store","position":7},{"hierarchy":{"lvl1":"上下文工程","lvl3":"3）使用 Runtime 加载上下文","lvl2":"一、动态修改系统提示词"},"type":"lvl3","url":"/context#id-3-runtime","position":8},{"hierarchy":{"lvl1":"上下文工程","lvl3":"3）使用 Runtime 加载上下文","lvl2":"一、动态修改系统提示词"},"content":"\n\n@dataclass\nclass Context:\n    user_role: str\n    deployment_env: str\n\n@dynamic_prompt\ndef context_aware_prompt(request: ModelRequest) -> str:\n    # Read from Runtime Context: user role and environment\n    user_role = request.runtime.context.user_role\n    env = request.runtime.context.deployment_env\n\n    base = \"You are a helpful assistant.\"\n\n    if user_role == \"admin\":\n        base += \"\\nYou can use the get_weather tool.\"\n    else:\n        base += \"\\nYou are prohibited from using the get_weather tool.\"\n\n    if env == \"production\":\n        base += \"\\nBe extra careful with any data modifications.\"\n\n    return base\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=llm,\n    tools=[get_weather],\n    middleware=[context_aware_prompt],\n    context_schema=Context,\n    checkpointer=InMemorySaver(),\n)\n\n# 利用 Runtime 中的两个变量，动态控制 System prompt\n# 将 user_role 设为 admin，允许使用天气查询工具\nconfig = {'configurable': {'thread_id': str(uuid.uuid4())}}\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"广州今天的天气怎么样？\"}]},\n    context=Context(user_role=\"admin\", deployment_env=\"production\"),\n    config=config,\n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n# 若将 user_role 改为 viewer，则无法使用天气查询工具\nconfig = {'configurable': {'thread_id': str(uuid.uuid4())}}\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"广州今天的天气怎么样？\"}]},\n    context=Context(user_role=\"viewer\", deployment_env=\"production\"),\n    config=config,\n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\nresult['messages']\n\n","type":"content","url":"/context#id-3-runtime","position":9},{"hierarchy":{"lvl1":"上下文工程","lvl2":"二、动态修改消息列表"},"type":"lvl2","url":"/context#id-1","position":10},{"hierarchy":{"lvl1":"上下文工程","lvl2":"二、动态修改消息列表"},"content":"LangGraph 预制了动态修改消息列表（Messages）的中间件 @wrap_model_call。上一节已经演示如何从 State、Store、Runtime 中获取上下文，本节将不再一一演示。在下面这个例子中，我们主要演示如何使用 Runtime 将本地文件的内容注入消息列表。\n\n@dataclass\nclass FileContext:\n    uploaded_files: list[dict]\n\n@wrap_model_call\ndef inject_file_context(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Inject context about files user has uploaded this session.\"\"\"\n    uploaded_files = request.runtime.context.uploaded_files\n\n    try:\n        base_dir = os.path.dirname(os.path.abspath(__file__))\n    except Exception as e:\n        import ipynbname\n        import os\n        notebook_path = ipynbname.path()\n        base_dir = os.path.dirname(notebook_path)\n\n    file_sections = []\n    for file in uploaded_files:\n        name, ftype = \"\", \"\"\n        path = file.get(\"path\")\n        if path:\n            base_filename = os.path.basename(path)\n            stem, ext = os.path.splitext(base_filename)\n            name = stem or base_filename\n            ftype = (ext.lstrip(\".\") if ext else None)\n\n            # 构建文件描述内容\n            content_list = [f\"名称: {name}\"]\n            if ftype:\n                content_list.append(f\"类型: {ftype}\")\n\n            # 解析相对路径为绝对路径\n            abs_path = path if os.path.isabs(path) else os.path.join(base_dir, path)\n\n            # 读取文件内容\n            content_block = \"\"\n            if abs_path and os.path.exists(abs_path):\n                try:\n                    with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n                        content_block = f.read()\n                except Exception as e:\n                    content_block = f\"[读取文件错误 '{abs_path}': {e}]\"\n            else:\n                content_block = \"[文件路径缺失或未找到]\"\n\n            section = (\n                f\"---\\n\"\n                f\"{chr(10).join(content_list)}\\n\\n\"\n                f\"{content_block}\\n\"\n                f\"---\"\n            )\n            file_sections.append(section)\n\n        file_context = (\n            \"已加载的会话文件：\\n\"\n            f\"{chr(10).join(file_sections)}\"\n            \"\\n回答问题时请参考这些文件。\"\n        )\n\n        # Inject file context before recent messages\n        messages = [  \n            *request.messages,\n            {\"role\": \"user\", \"content\": file_context},\n        ]\n        request = request.override(messages=messages)  \n\n    return handler(request)\n\nagent = create_agent(\n    model=llm,\n    middleware=[inject_file_context],\n    context_schema=FileContext,\n)\n\nresult = agent.invoke(\n    {\n        \"messages\": [{\n            \"role\": \"user\",\n            \"content\": \"关于上海地铁的无脸乘客，有什么需要注意的？\",\n        }],\n    },\n    context=FileContext(uploaded_files=[{\"path\": \"./docs/rule_horror.md\"}]),\n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n","type":"content","url":"/context#id-1","position":11},{"hierarchy":{"lvl1":"上下文工程","lvl2":"三、在工具中使用上下文"},"type":"lvl2","url":"/context#id-2","position":12},{"hierarchy":{"lvl1":"上下文工程","lvl2":"三、在工具中使用上下文"},"content":"下面，我们尝试在工具中使用存储在 SqliteStore 中的上下文信息。\n\n# 删除SQLite数据库\nif os.path.exists(\"user-info.db\"):\n    os.remove(\"user-info.db\")\n\n# 创建SQLite存储\nconn = sqlite3.connect(\"user-info.db\", check_same_thread=False, isolation_level=None)\nconn.execute(\"PRAGMA journal_mode=WAL;\")\nconn.execute(\"PRAGMA busy_timeout = 30000;\")\n\nstore = SqliteStore(conn)\n\n# 预置两条用户信息\nstore.put((\"user_info\",), \"柳如烟\", {\"description\": \"清冷才女，身怀绝技，为寻身世之谜踏入江湖。\", \"birthplace\": \"吴兴县\"})\nstore.put((\"user_info\",), \"苏慕白\", {\"description\": \"孤傲剑客，剑法超群，背负家族血仇，隐于市井追寻真相。\", \"birthplace\": \"杭县\"})\n\n","type":"content","url":"/context#id-2","position":13},{"hierarchy":{"lvl1":"上下文工程","lvl3":"1）基础用例","lvl2":"三、在工具中使用上下文"},"type":"lvl3","url":"/context#id-1-1","position":14},{"hierarchy":{"lvl1":"上下文工程","lvl3":"1）基础用例","lvl2":"三、在工具中使用上下文"},"content":"使用 ToolRuntime\n\n@tool\ndef fetch_user_data(\n    user_id: str,\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"\n    Fetch user information from the in-memory store.\n\n    :param user_id: The unique identifier of the user.\n    :param runtime: The tool runtime context injected by the framework.\n    :return: The user's description string if found; an empty string otherwise.\n    \"\"\"\n    store = runtime.store\n    user_info = store.get((\"user_info\",), user_id)\n\n    user_desc = \"\"\n    if user_info:\n        user_desc = user_info.value.get(\"description\", \"\")\n\n    return user_desc\n\nagent = create_agent(\n    model=llm,\n    tools=[fetch_user_data],\n    store=store,\n)\n\nresult = agent.invoke({\n    \"messages\": [{\n        \"role\": \"user\",\n        \"content\": \"五分钟之内，我要柳如烟的全部信息\"\n    }]\n})\n\nfor message in result['messages']:\n    message.pretty_print()\n\n","type":"content","url":"/context#id-1-1","position":15},{"hierarchy":{"lvl1":"上下文工程","lvl3":"2）复杂一点的例子","lvl2":"三、在工具中使用上下文"},"type":"lvl3","url":"/context#id-2-1","position":16},{"hierarchy":{"lvl1":"上下文工程","lvl3":"2）复杂一点的例子","lvl2":"三、在工具中使用上下文"},"content":"使用 ToolRuntime[Context]\n\n@dataclass\nclass Context:\n    key: str\n\n@tool\ndef fetch_user_data(\n    user_id: str,\n    runtime: ToolRuntime[Context]\n) -> str:\n    \"\"\"\n    Fetch user information from the in-memory store.\n\n    :param user_id: The unique identifier of the user.\n    :param runtime: The tool runtime context injected by the framework.\n    :return: The user's description string if found; an empty string otherwise.\n    \"\"\"\n    key = runtime.context.key\n\n    store = runtime.store\n    user_info = store.get((\"user_info\",), user_id)\n\n    user_desc = \"\"\n    if user_info:\n        user_desc = user_info.value.get(key, \"\")\n\n    return f\"{key}: {user_desc}\"\n\nagent = create_agent(\n    model=llm,\n    tools=[fetch_user_data],\n    store=store,\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"五分钟之内，我要柳如烟的全部信息\"}]},\n    context=Context(key=\"birthplace\"),\n)\n\nfor message in result['messages']:\n    message.pretty_print()\n\n","type":"content","url":"/context#id-2-1","position":17},{"hierarchy":{"lvl1":"上下文工程","lvl3":"四、压缩上下文","lvl2":"三、在工具中使用上下文"},"type":"lvl3","url":"/context#id-3","position":18},{"hierarchy":{"lvl1":"上下文工程","lvl3":"四、压缩上下文","lvl2":"三、在工具中使用上下文"},"content":"LangChain 提供了内置的中间件 SummarizationMiddleware 用于压缩上下文。该中间件维护的是典型的 生命周期上下文，与 模型上下文 和 工具上下文 的瞬态更新不同，生命周期上下文会持续更新：持续将旧消息替换为摘要。\n\n除非上下文超长，导致模型能力降低，否则不需要使用 SummarizationMiddleware。一般来说，触发摘要得值可以设得较大。比如：\n\nmax_tokens_before_summary: 3000\n\nmessages_to_keep: 20\n\n如果你想了解更多关于上下文腐坏（Context Rot）的信息，Chroma 团队在 2025 年 7 月 14 日发布的 \n\nContext Rot: How Increasing Input Tokens Impacts LLM Performance，系统性地揭示了长上下文导致模型性能退化的现象。\n\n# 创建短期记忆\ncheckpointer = InMemorySaver()\n\n# 创建带内置摘要中间件的Agent\n# 为了让配置能在我们的例子里生效，这里的触发值设得很小\nagent = create_agent(\n    model=llm,\n    middleware=[\n        SummarizationMiddleware(\n            model=llm,\n            max_tokens_before_summary=40,  # Trigger summarization at 40 tokens\n            messages_to_keep=1,  # Keep last 1 messages after summary\n        ),\n    ],\n)\n\nresult = agent.invoke(\n    {\"messages\": [\n        {\"role\": \"user\", \"content\": \"广州今天的天气怎么样？\"},\n        {\"role\": \"assistant\", \"content\": \"广州天气很好\"},\n        {\"role\": \"user\", \"content\": \"吃点什么好呢\"},\n        {\"role\": \"assistant\", \"content\": \"要不要吃香茅鳗鱼煲\"},\n        {\"role\": \"user\", \"content\": \"香茅是什么\"},\n        {\"role\": \"assistant\", \"content\": \"香茅又名柠檬草，常见于泰式冬阴功汤、越南烤肉\"},\n        {\"role\": \"user\", \"content\": \"auv 那还等什么，咱吃去吧\"},\n    ]},\n    checkpointer=checkpointer,\n)\n\nfor message in result['messages']:\n    message.pretty_print()","type":"content","url":"/context#id-3","position":19},{"hierarchy":{"lvl1":"MCP Server"},"type":"lvl1","url":"/mcp-server","position":0},{"hierarchy":{"lvl1":"MCP Server"},"content":"这一节，我们在 LangGraph 中接入 MCP Server。要接入 MCP Server，首先得先有 MCP Server。哦这可是我的老本行！我在纸牌魔术MCP（\n\ncard-magic-mcp）中已经总结出一套高效的写法了。\n\n相关代码在本仓库的 \n\nmcp_server 路径下。","type":"content","url":"/mcp-server","position":1},{"hierarchy":{"lvl1":"MCP Server","lvl2":"一、开发 MCP 服务"},"type":"lvl2","url":"/mcp-server#id-mcp","position":2},{"hierarchy":{"lvl1":"MCP Server","lvl2":"一、开发 MCP 服务"},"content":"","type":"content","url":"/mcp-server#id-mcp","position":3},{"hierarchy":{"lvl1":"MCP Server","lvl3":"1）天气 MCP","lvl2":"一、开发 MCP 服务"},"type":"lvl3","url":"/mcp-server#id-1-mcp","position":4},{"hierarchy":{"lvl1":"MCP Server","lvl3":"1）天气 MCP","lvl2":"一、开发 MCP 服务"},"content":"以 get_weather_mcp 为例，我们要把这个 MCP 写成一个 Python 包。当然仅供本地使用，如果你想传到 PyPI 上当然可以，但那就是另外的流程了，敬请参考我的博客 \n\n《PyPI 打包小记》。\n\n为了让它被识别为 Python 包，我们要在项目下，新建一个 __init__.py 文件。然后把主逻辑写在 server.py 中，接着在 __main__.py 中使用 from . import server 引入它。最后用 streamable-http 的方式部署它：def http():\n    \"\"\"streamable-http entry point for the package.\"\"\"\n    asyncio.run(server.mcp.run(transport=\"http\",\n                               host=host,\n                               port=port,\n                               path=\"/mcp\"))\n\n写到这里就齐活了。这里使用 __main__.py 是有小巧思的，这样我们可以将这个包作为模块直接在命令行使用。什么意思呢？就是我们用 python -m [包名] 就等于直接运行了 __main__.py 这个特殊文件。那由于我们先前在该特殊文件中启动了 http() 函数，这样就能快捷方便地把 MCP Server 启动起来了！对于我们的 get_weather_mcp，启动命令如下：python -m get_weather_mcp","type":"content","url":"/mcp-server#id-1-mcp","position":5},{"hierarchy":{"lvl1":"MCP Server","lvl3":"2）算数 MCP","lvl2":"一、开发 MCP 服务"},"type":"lvl3","url":"/mcp-server#id-2-mcp","position":6},{"hierarchy":{"lvl1":"MCP Server","lvl3":"2）算数 MCP","lvl2":"一、开发 MCP 服务"},"content":"这还需要赘述吗？开发流程照抄上面的步骤。\n\n真的是超级模版化。__init__.py 和 __main__.py 几乎完全相同。\n\n唯一需要改动的是 __main__.py。需要把端口 port 改成新号码，一般来说加 1 就行。这里我们把 8000 改成 8001，其他不变：# -*- coding: utf-8 -*-\nimport asyncio\nimport os\n\nfrom . import server\n\n\nhost = os.getenv('HOST', '127.0.0.1')\nport = int(os.getenv('PORT', 8001))\n\n\ndef stdio():\n    \"\"\"Stdio entry point for the package.\"\"\"\n    asyncio.run(server.mcp.run(transport=\"stdio\"))\n\n\ndef http():\n    \"\"\"streamable-http entry point for the package.\"\"\"\n    asyncio.run(server.mcp.run(transport=\"http\",\n                               host=host,\n                               port=port,\n                               path=\"/mcp\"))\n\n\nif __name__ == \"__main__\":\n    http()","type":"content","url":"/mcp-server#id-2-mcp","position":7},{"hierarchy":{"lvl1":"MCP Server","lvl3":"二、使用 supervisord 管理 MCP 服务","lvl2":"一、开发 MCP 服务"},"type":"lvl3","url":"/mcp-server#id-supervisord-mcp","position":8},{"hierarchy":{"lvl1":"MCP Server","lvl3":"二、使用 supervisord 管理 MCP 服务","lvl2":"一、开发 MCP 服务"},"content":"supervisord 是一个 进程管理工具。你告诉它有哪些 MCP 要跑，它会守护你的 MCP 宝宝。当 MCP 挂掉的时候，supervisord 能够自动拉起 MCP。这块内容在我的博客 \n\n《后台管理工具介绍》 中有做简略的介绍（但更多是关于 systemd 和 pm2 的）。\n\n首先，我们打开项目的 mcp_server 路径，在这里创建一个配置文件 mcp_supervisor.conf，来给 supervisord 使用。我的配置如下：[unix_http_server]\nfile=/tmp/supervisor.sock\n\n[supervisord]\nlogfile=/tmp/supervisord.log\nlogfile_maxbytes=50MB\nlogfile_backups=10\nloglevel=info\npidfile=/tmp/supervisord.pid\nnodaemon=false\nminfds=1024\nminprocs=200\n\n[rpcinterface:supervisor]\nsupervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface\n\n[supervisorctl]\nserverurl=unix:///tmp/supervisor.sock\n\n[program:math_mcp]\ncommand=python -m mcp_server.math_mcp\ndirectory=..\nautostart=true\nautorestart=true\nstartsecs=5\nstopwaitsecs=10\nstdout_logfile=/tmp/math_mcp.log\nstderr_logfile=/tmp/math_mcp_err.log\n\n[program:weather_mcp]\ncommand=python -m mcp_server.get_weather_mcp\ndirectory=..\nautostart=true\nautorestart=true\nstartsecs=5\nstopwaitsecs=10\nstdout_logfile=/tmp/weather_mcp.log\nstderr_logfile=/tmp/weather_mcp_err.log\n\n[group:mcp_servers]\nprograms=math_mcp,weather_mcp\n\n至此，math_mcp、weather_mcp 的配置就完成了。这种东西没必要自己写，我是让 \n\nTRAE 帮我写的。下面是关于常用命令的说明！","type":"content","url":"/mcp-server#id-supervisord-mcp","position":9},{"hierarchy":{"lvl1":"MCP Server","lvl3":"1）安装 supervisord","lvl2":"一、开发 MCP 服务"},"type":"lvl3","url":"/mcp-server#id-1-supervisord","position":10},{"hierarchy":{"lvl1":"MCP Server","lvl3":"1）安装 supervisord","lvl2":"一、开发 MCP 服务"},"content":"pip install supervisor","type":"content","url":"/mcp-server#id-1-supervisord","position":11},{"hierarchy":{"lvl1":"MCP Server","lvl3":"2）启动 supervisord","lvl2":"一、开发 MCP 服务"},"type":"lvl3","url":"/mcp-server#id-2-supervisord","position":12},{"hierarchy":{"lvl1":"MCP Server","lvl3":"2）启动 supervisord","lvl2":"一、开发 MCP 服务"},"content":"supervisord -c ./mcp_supervisor.conf","type":"content","url":"/mcp-server#id-2-supervisord","position":13},{"hierarchy":{"lvl1":"MCP Server","lvl3":"3）关闭 supervisord","lvl2":"一、开发 MCP 服务"},"type":"lvl3","url":"/mcp-server#id-3-supervisord","position":14},{"hierarchy":{"lvl1":"MCP Server","lvl3":"3）关闭 supervisord","lvl2":"一、开发 MCP 服务"},"content":"pkill -f supervisord","type":"content","url":"/mcp-server#id-3-supervisord","position":15},{"hierarchy":{"lvl1":"MCP Server","lvl3":"4）检查端口状态","lvl2":"一、开发 MCP 服务"},"type":"lvl3","url":"/mcp-server#id-4","position":16},{"hierarchy":{"lvl1":"MCP Server","lvl3":"4）检查端口状态","lvl2":"一、开发 MCP 服务"},"content":"lsof -i :8000\nlsof -i :8001\n\n","type":"content","url":"/mcp-server#id-4","position":17},{"hierarchy":{"lvl1":"MCP Server","lvl2":"三、在 LangGraph 中使用 MCP"},"type":"lvl2","url":"/mcp-server#id-langgraph-mcp","position":18},{"hierarchy":{"lvl1":"MCP Server","lvl2":"三、在 LangGraph 中使用 MCP"},"content":"在使用之前，需要安装配适该功能的 Python 包：pip install langchain-mcp-adapters\n\n我也是服了开发团队，依我看 LangChain、LangGraph 不如合成一个包。还要我们去功能在哪个包里，真费劲！而且各种功能也被拆得稀碎，看看我到目前为止都安装多少包了：langchain[openai]\nlangchain-mcp-adapters\nlanggraph\nlanggraph-cli[inmem]\nlanggraph-supervisor\nlanggraph-checkpoint-sqlite\n\n若非 LangGraph 1.0 更新了不少好功能，我是打心眼里看不上这个开源项目。衷心祝愿后起之秀 \n\nAgentScope 吸收 LangGraph 1.0 的长处并超越它。当然在此之前，我们得承认 LangGraph 的地位。它虽不完美，但依然是最强大的那个。","type":"content","url":"/mcp-server#id-langgraph-mcp","position":19},{"hierarchy":{"lvl1":"MCP Server","lvl3":"1）启动 MCP 服务","lvl2":"三、在 LangGraph 中使用 MCP"},"type":"lvl3","url":"/mcp-server#id-1-mcp-1","position":20},{"hierarchy":{"lvl1":"MCP Server","lvl3":"1）启动 MCP 服务","lvl2":"三、在 LangGraph 中使用 MCP"},"content":"我们只启动天气 MCP。算数 MCP 稍后我们将以 stdio 的方式调用，无需单独启动服务。\n\n启动 get_weather_mcp：python -m mcp_server.get_weather_mcp \n\n测试 MCP Server 是否成功启动：\n\n# !lsof -i :8000\n\n","type":"content","url":"/mcp-server#id-1-mcp-1","position":21},{"hierarchy":{"lvl1":"MCP Server","lvl3":"2）接入 MCP 服务","lvl2":"三、在 LangGraph 中使用 MCP"},"type":"lvl3","url":"/mcp-server#id-2-mcp-1","position":22},{"hierarchy":{"lvl1":"MCP Server","lvl3":"2）接入 MCP 服务","lvl2":"三、在 LangGraph 中使用 MCP"},"content":"使用 MultiServerMCPClient 接入 MCP Server.\n\nimport os\n\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain_mcp_adapters.client import MultiServerMCPClient  \nfrom langchain.agents import create_agent\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 加载模型\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n    temperature=0.7,\n)\n\nasync def mcp_agent():\n    # 我们用两种方式启动 MCP Server：stdio 和 streamable_http\n    client = MultiServerMCPClient(  \n        {\n            \"math\": {\n                \"command\": \"python\",\n                \"args\": [os.path.abspath(\"./mcp_server/math_mcp/server.py\")],\n                \"transport\": \"stdio\",\n            },\n            \"weather\": {\n                \"url\": \"http://localhost:8000/mcp\",\n                \"transport\": \"streamable_http\",\n            }\n        }\n    )\n    \n    tools = await client.get_tools()\n    agent = create_agent(\n        llm,\n        tools=tools,\n    )\n\n    return agent\n\nasync def use_mcp(messages):\n    agent = await mcp_agent()\n    response = await agent.ainvoke(messages)\n    return response\n\n在 Jupyter Notebook 中，使用 response = await use_mcp(messages) 命令调用函数。但是在 .py 文件中，这种调用方法会失败。\n\n# 调用天气 MCP\nmessages = {\"messages\": [{\"role\": \"user\", \"content\": \"福州天气怎么样？\"}]}\nresponse = await use_mcp(messages)\nresponse[\"messages\"][-1].content\n\n# 调用算数 MCP，由于是 stdio，启动会慢一点\nmessages = {\"messages\": [{\"role\": \"user\", \"content\": \"计算 (3 + 5) * 12\"}]}\nresponse = await use_mcp(messages)\nresponse[\"messages\"][-1].content\n\n在 .py 文件中，应该使用 asyncio，改动部分如下：import asyncio\n\nasync def main():\n    # 调用天气 MCP\n    messages = {\"messages\": [{\"role\": \"user\", \"content\": \"福州天气怎么样？\"}]}\n    response = await use_mcp(messages)\n    print(response[\"messages\"][-1].content)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n","type":"content","url":"/mcp-server#id-2-mcp-1","position":23},{"hierarchy":{"lvl1":"监督者模式"},"type":"lvl1","url":"/supervisor","position":0},{"hierarchy":{"lvl1":"监督者模式"},"content":"多智能体系统（MAS, Multi-Agent Systems）有很多种模式，比如 Supervisor、Handoff。\n\n目前最实用的依然是监督者模式（Supervisor）。它的工作流程是这样的，一个 Agent 充当监督者，它负责将任务拆解成具体步骤，交给 工具节点 或 工具 Agent 执行。再将执行结果返回给监督者，由监督者汇总这些结果，并返回给用户。执行过程如下：flowchart LR\n    A[用户] --> B[Supervisor]\n    B --> C[工具节点]\n    B --> D[工具Agent 1]\n    B --> E[工具Agent 2]\n    C --> B\n    D --> B\n    E --> B\n    B --> F[AI回复]\n\n在 LangGraph 中，可以用两种方式实现监督者模式：\n\n使用 LangChain 的 \n\ntool-calling 功能实现\n\n使用 \n\nlanggraph​-supervisor 包实现\n\n下面我将依次演示两种实现方法。\n\nimport os\n\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\nfrom langchain_core.tools import tool\nfrom langgraph_supervisor import create_supervisor\n\n# 加载模型配置\n_ = load_dotenv()\n\n# 加载模型\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n    model=\"qwen3-coder-plus\",\n    temperature=0.7,\n)\n\n@tool\ndef add(a: float, b: float) -> float:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n@tool\ndef multiply(a: float, b: float) -> float:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\n@tool\ndef divide(a: float, b: float) -> float:\n    \"\"\"Divide two numbers.\"\"\"\n    return a / b\n\n","type":"content","url":"/supervisor","position":1},{"hierarchy":{"lvl1":"监督者模式","lvl2":"一、使用 tool-calling 功能实现"},"type":"lvl2","url":"/supervisor#id-tool-calling","position":2},{"hierarchy":{"lvl1":"监督者模式","lvl2":"一、使用 tool-calling 功能实现"},"content":"我们首先使用 create_agent 创建 subagent1 和 subagent2，分别用于计算 两数相加 和 两数相乘。然后使用 @tool 装饰器包装两个 Agent，做成 call_subagent1 和 call_subagent2两个工具，它们仅传出 Agent 的最后一条信息。\n\n# 创建subagent1：用于计算两数相加\nsubagent1 = create_agent(\n    model=llm,\n    tools=[add],\n    name=\"subagent-1\",\n)\n\n@tool(\n    \"subagent-1\",\n    description=\"可以准确地计算两数相加\"\n)\ndef call_subagent1(query: str):\n    result = subagent1.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\n    })\n    return result[\"messages\"][-1].content\n\n# 创建subagent2：用于计算两数相乘\nsubagent2 = create_agent(\n    model=llm,\n    tools=[multiply],\n    name=\"subagent-2\",\n)\n\n@tool(\n    \"subagent-2\",\n    description=\"可以准确地计算两数相乘\"\n)\ndef call_subagent2(query: str):\n    result = subagent2.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\n    })\n    return result[\"messages\"][-1].content\n\n现在，我们可以将 call_subagent1、call_subagent2、divide 作为工具，直接传入 supervisor_agent.\n\n# 创建supervisor agent\nsupervisor_agent = create_agent(\n    model=llm,\n    tools=[call_subagent1, call_subagent2, divide],\n    name=\"supervisor-agent\",\n    system_prompt=\"提示：如遇两数相减仍可用两数相加工具实现，只需将一个数加上另一个数的负数\",\n)\n\n下面，我们来计算 38462 + 378 / 49 * 83723 - 123，看它能否正确地使用所以工具。\n\nresult = supervisor_agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"计算 38462 + 378 / 49 * 83723 - 123 的结果\"}]}\n)\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n\n# 验证结果是否正确\n38462 + 378 / 49 * 83723 - 123\n\n","type":"content","url":"/supervisor#id-tool-calling","position":3},{"hierarchy":{"lvl1":"监督者模式","lvl2":"二、使用 langgraph-supervisor 包实现"},"type":"lvl2","url":"/supervisor#id-langgraph-supervisor","position":4},{"hierarchy":{"lvl1":"监督者模式","lvl2":"二、使用 langgraph-supervisor 包实现"},"content":"要使用 \n\nlanggraph​-supervisor，首先需要安装 langgraph-supervisor 包：pip install -U langgraph-supervisor\n\nlanggraph-supervisor 提供了 create_supervisor 函数。该函数接受多个 Agent 作为入参，并通过 supervisor 调用它们。\n\nsubagent3 = create_agent(\n    model=llm,\n    tools=[divide],\n    name=\"subagent-3\",\n)\n\nsupervisor_graph = create_supervisor(\n    [subagent1, subagent2, subagent3],\n    model=llm,\n    prompt=\"提示：如遇两数相减仍可用两数相加工具实现，只需将一个数加上另一个数的负数\"\n)\n\nsupervisor_app = supervisor_graph.compile()\n\nresult = supervisor_app.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"计算 38462 + 378 / 49 * 83723 - 123 的结果\"}]}\n)\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n\n# 验证结果是否正确\n38462 + 378 / 49 * 83723 - 123\n\n别看现在它们都算对了，实际上这是我试了好几遍，并调整了 system prompt 的结果。你多次尝试应该也能 roll 出错误的结果。所以我对多智能体的效果仍有疑虑。我感觉现阶段多智能体是会降低准确性的，能不用就不用。什么时候应该使用多智能体呢？可以参考以下 LangGraph 给的 \n\n建议。\n\n多代理系统在以下情况下很有用：\n\n单个代理拥有太多工具，并且在使用哪个工具方面做出了错误的决定。\n\n上下文或内存变得太大，一个代理无法有效跟踪。\n\n任务需要专业化 （例如，规划师、研究人员、数学专家）。","type":"content","url":"/supervisor#id-langgraph-supervisor","position":5},{"hierarchy":{"lvl1":"并行"},"type":"lvl1","url":"/parallel","position":0},{"hierarchy":{"lvl1":"并行"},"content":"本节验证一件事：让我们看看使用 StateGraph 创建的可并行节点是否真的在并行，让我们打印时间看看。\n\nimport time\nfrom datetime import datetime\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.runnables import RunnableConfig\n\n# 创建虚拟并行节点1\ndef parallel_node1(state: MessagesState, config: RunnableConfig):\n    start_time = datetime.now()\n    print(f\"[parallel_node1] 进入函数时间: {start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")\n\n    # 使用 sleep 模拟占用时间\n    time.sleep(5)\n\n    end_time = datetime.now()\n    print(f\"[parallel_node1] 退出函数时间: {end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")\n\n    return {'messages': [HumanMessage(\n        content=f'虚拟节点1运行了{round((end_time - start_time).total_seconds(), 3)}秒'\n    )]}\n\n# 创建虚拟并行节点2\ndef parallel_node2(state: MessagesState, config: RunnableConfig):\n    start_time = datetime.now()\n    print(f\"[parallel_node2] 进入函数时间: {start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")\n\n    # 使用 sleep 模拟占用时间\n    time.sleep(10)\n\n    end_time = datetime.now()\n    print(f\"[parallel_node2] 退出函数时间: {end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")\n\n    return {'messages': [HumanMessage(\n        content=f'虚拟节点2运行了{round((end_time - start_time).total_seconds(), 3)}秒'\n    )]}\n\n# 创建图\nbuilder = StateGraph(MessagesState)\n\n# 添加节点\nbuilder.add_node('parallel_node1', parallel_node1)\nbuilder.add_node('parallel_node2', parallel_node2)\n\n# 添加边\nbuilder.add_edge(START, 'parallel_node1')\nbuilder.add_edge(START, 'parallel_node2')\nbuilder.add_edge('parallel_node1', END)\nbuilder.add_edge('parallel_node2', END)\n\n# 编译图\nmy_graph = builder.compile(name='my-graph')\nmy_graph\n\n# 调用图\nresponse = my_graph.invoke({'messages': [HumanMessage(content='执行 parallel_node1 和 parallel_node2')]})\n\nfor message in response['messages']:\n    message.pretty_print()","type":"content","url":"/parallel","position":1},{"hierarchy":{"lvl1":"LangGraph v1.0 完全指南"},"type":"lvl1","url":"/home","position":0},{"hierarchy":{"lvl1":"LangGraph v1.0 完全指南"},"content":"LangGraph 是由 \n\nLangChain 团队开发的 Agent 开源框架。","type":"content","url":"/home","position":1},{"hierarchy":{"lvl1":"LangGraph v1.0 完全指南","lvl2":"一、安装依赖"},"type":"lvl2","url":"/home#id","position":2},{"hierarchy":{"lvl1":"LangGraph v1.0 完全指南","lvl2":"一、安装依赖"},"content":"pip -r requirements.txt\n\n依赖包列表\n\n以下为 requirements.txt 中的依赖包清单：pydantic\npython-dotenv\nlangchain[openai]\nlangchain-mcp-adapters\nlanggraph\nlanggraph-cli[inmem]\nlanggraph-supervisor\nlanggraph-checkpoint-sqlite\nipynbname\nfastmcp\nsupervisord","type":"content","url":"/home#id","position":3},{"hierarchy":{"lvl1":"LangGraph v1.0 完全指南","lvl2":"二、章节目录"},"type":"lvl2","url":"/home#id-1","position":4},{"hierarchy":{"lvl1":"LangGraph v1.0 完全指南","lvl2":"二、章节目录"},"content":"主教程：\n\n序号\n\n章节\n\n1\n\n快速入门\n\n2\n\nStateGraph\n\n3\n\n中间件\n\n4\n\n人机交互\n\n5\n\n记忆\n\n6\n\n上下文工程\n\n7\n\nMCP Server\n\n8\n\n多智能体：Supervisor 模式\n\n9\n\n并行\n\n10\n\nDeep Agents\n\n11\n\n调试界面\n\n教程中未提及的一些关键代码实现：\n\n代码\n\n说明\n\ntest_rag.py\n\n使用 RAG 将本地文档片段注入智能体\n\ntest_langmem.py\n\n使用 LangMeM 管理智能体长期记忆\n\ntest_store.py\n\n使用 RedisStore 快速读写长期记忆\n\ntest_router.py\n\n实现一个简单的智能体路由","type":"content","url":"/home#id-1","position":5},{"hierarchy":{"lvl1":"LangGraph v1.0 完全指南","lvl2":"三、调试界面"},"type":"lvl2","url":"/home#id-2","position":6},{"hierarchy":{"lvl1":"LangGraph v1.0 完全指南","lvl2":"三、调试界面"},"content":"启动 LangGraph CLI 提供的本地开发界面：langgraph dev\n\n更多介绍请参阅 \n\n第11章","type":"content","url":"/home#id-2","position":7},{"hierarchy":{"lvl1":"LangGraph v1.0 完全指南","lvl2":"四、参考文档"},"type":"lvl2","url":"/home#id-3","position":8},{"hierarchy":{"lvl1":"LangGraph v1.0 完全指南","lvl2":"四、参考文档"},"content":"LangChain\n\nLangGraph\n\nDeep Agents\n\nLangMem\n\nlanggraph-101\n\n如果你觉得本项目对你有帮助，欢迎 Star GitHub 仓库：\n\nlanggraph-tutorial","type":"content","url":"/home#id-3","position":9}]}