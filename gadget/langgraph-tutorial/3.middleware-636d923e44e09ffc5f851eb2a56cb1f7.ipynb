{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da96a78-5bcd-4ea3-b5cc-addb660a1554",
   "metadata": {},
   "source": [
    "# 中间件\n",
    "\n",
    "## 一、预算控制\n",
    "\n",
    "利用运行时（runtime）和上下文（context），我们可以动态地更改模型配置。这种动态性能帮助我们更好地控制模型。\n",
    "\n",
    "一个实用的场景是「预算控制」。随着对话轮次增加，积累的历史对话越来越多，每次请求的费用也随之增加。为了控制预算，我们可以设定在对话超过某个轮次之后，切换到费率较低的模型。该功能可以通过中间件实现。\n",
    "\n",
    "事实上，中间件（middleware）能实现的功能有很多，可以将它视为 Agent 的万能控制接口。`LangChain v1.0` 通过引入中间件，极大增强了对 Agent 的掌控力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd14ba5-4b0c-4cfb-a976-040d0a365f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# 加载模型配置\n",
    "_ = load_dotenv()\n",
    "\n",
    "# 低费率模型\n",
    "basic_model = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n",
    "    model=\"qwen3-coder-plus\",\n",
    ")\n",
    "\n",
    "# 高费率模型\n",
    "advanced_model = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n",
    "    model=\"qwen3-max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4033cbf-b59f-4a6b-9fd1-f102b7fea252",
   "metadata": {},
   "source": [
    "具体来讲，使用 [@wrap_model_call](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) 装饰器可以创建控制模型的中间件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e758792-47d8-4ea4-857e-dddbbc71eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    if message_count > 5:\n",
    "        # Use an advanced model for longer conversations\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    request.model = model\n",
    "    print(f\"message_count: {message_count}\")\n",
    "    print(f\"model_name: {model.model_name}\")\n",
    "\n",
    "    return handler(request)\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model,  # Default model\n",
    "    middleware=[dynamic_model_selection]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b4ed25-2ef8-4a24-9760-89f1ed34bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Round 1 ===\n",
      "message_count: 1\n",
      "model_name: qwen3-coder-plus\n",
      "content: 汽车有4个轮子。\n",
      "\n",
      "=== Round 2 ===\n",
      "message_count: 3\n",
      "model_name: qwen3-coder-plus\n",
      "content: 飞机有3个轮子（起落架）。\n",
      "\n",
      "=== Round 3 ===\n",
      "message_count: 5\n",
      "model_name: qwen3-coder-plus\n",
      "content: 摩托车有2个轮子。\n",
      "\n",
      "=== Round 4 ===\n",
      "message_count: 7\n",
      "model_name: qwen3-max\n",
      "content: 自行车有2个轮子。\n"
     ]
    }
   ],
   "source": [
    "state: MessagesState = {\"messages\": []}\n",
    "items = ['汽车', '飞机', '摩托车', '自行车']\n",
    "for idx, i in enumerate(items):\n",
    "    print(f\"\\n=== Round {idx+1} ===\")\n",
    "    state[\"messages\"] += [HumanMessage(content=f\"{i}有几个轮子，请简单回答\")]\n",
    "    result = agent.invoke(state)\n",
    "    state[\"messages\"] = result[\"messages\"]\n",
    "    print(f\"content: {result[\"messages\"][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc58247-324d-40fe-83e5-b34f9ec564e6",
   "metadata": {},
   "source": [
    "## 二、消息截断\n",
    "\n",
    "智能体系统的上下文长度是有限的，若超过限制，就要想办法压缩上下文。在所有方法中，最简单粗暴的方法就是截断。消息截断的功能可以用 `@before_model` 装饰器实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b08619c1-6940-4043-be1b-3282be16c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.agents.middleware import before_model\n",
    "from langgraph.runtime import Runtime\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750eee1-a873-424c-878b-61ec5bf81f0e",
   "metadata": {},
   "source": [
    "在下面的例子中，由于我们始终保留第一条消息，因此智能体总是记得我叫 bob。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aefc09f4-d67f-4a7a-b930-171043e0a37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! You introduced yourself to me earlier.\n"
     ]
    }
   ],
   "source": [
    "@before_model\n",
    "def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if len(messages) <= 3:\n",
    "        return None  # No changes needed\n",
    "\n",
    "    first_msg = messages[0]\n",
    "    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n",
    "    new_messages = [first_msg] + recent_messages\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
    "            *new_messages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "agent = create_agent(\n",
    "    basic_model,\n",
    "    middleware=[trim_messages],\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "def agent_invoke(agent):\n",
    "    agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "    agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "    agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "    final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "    \n",
    "    final_response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "agent_invoke(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2afa04-18ad-487d-9cef-dfba0b5ec36d",
   "metadata": {},
   "source": [
    "我们对中间件进行一些修改，仅保留最后两条对话记录，现在智能体不记得我是 bob 了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2a8e62-b7c4-4275-8967-7c551a73c6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to your name or personal information. I don't know who you are beyond our current conversation. If you'd like to share your name, I'd be happy to use it, but I can't access that information on my own. Is there something specific I can help you with today?\n"
     ]
    }
   ],
   "source": [
    "@before_model\n",
    "def trim_without_first_message(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
    "            *messages[-2:]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "agent = create_agent(\n",
    "    basic_model,\n",
    "    middleware=[trim_without_first_message],\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "agent_invoke(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1220c4b-13f2-48e0-90d8-3fb2bc92e5bd",
   "metadata": {},
   "source": [
    "## 三、护栏：敏感词过滤\n",
    "\n",
    "智能体能在模型之外提供额外的安全性，我们一般把这种安全能力称为护栏（Guardrails）。在 LangGraph 中，护栏可以通过中间件实现。下面通过两个具体的例子，演示如何使用护栏提升智能体的安全性。\n",
    "\n",
    "我们先实现一个简单的安全策略：检测用户最新一条输入，若其中包含指定的敏感词，则智能体拒绝回答用户的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c5cc5b-7b4a-4658-8f86-feb8114d04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain.agents.middleware import before_agent, AgentState, hook_config\n",
    "from langgraph.runtime import Runtime\n",
    "\n",
    "banned_keywords = [\"hack\", \"exploit\", \"malware\"]\n",
    "\n",
    "@before_agent(can_jump_to=[\"end\"])\n",
    "def content_filter(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n",
    "    # Get the first user message\n",
    "    if not state[\"messages\"]:\n",
    "        return None\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.type != \"human\":\n",
    "        return None\n",
    "\n",
    "    content = last_message.content.lower()\n",
    "\n",
    "    # Check for banned keywords\n",
    "    for keyword in banned_keywords:\n",
    "        if keyword in content:\n",
    "            # Block execution before any processing\n",
    "            return {\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n",
    "                }],\n",
    "                \"jump_to\": \"end\"\n",
    "            }\n",
    "\n",
    "    return None\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model,\n",
    "    middleware=[content_filter],\n",
    ")\n",
    "\n",
    "# This request will be blocked before any processing\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55cad2e8-ab78-44d6-8682-1e0fe17469a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How do I hack into a database?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I cannot process requests containing inappropriate content. Please rephrase your request.\n"
     ]
    }
   ],
   "source": [
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761479b-d432-4e09-9341-4b5ad8363b48",
   "metadata": {},
   "source": [
    "## 四、护栏：PII 检测\n",
    "\n",
    "[PII](https://docs.langchain.com/oss/python/langchain/guardrails#pii-detection)（Personally Identifiable Information）检测是一个常见的、用于过滤用户敏感信息的功能。它能检测用户的电子邮件、信用卡、IP 地址等敏感信息，以防止信息泄漏。\n",
    "\n",
    "在下面的例子中，我们使用 LLM 检测敏感信息，并使用两种策略进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0697a36-a7f6-46dc-acf0-44eed2c56daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 可信任的模型，一般是本地模型，为了方便，这里依然使用qwen\n",
    "trusted_model = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n",
    "    model=\"qwen3-coder-plus\",\n",
    ")\n",
    "\n",
    "# 用于格式化智能体输出，若发现敏感信息返回Ture，没发现返回False\n",
    "class PiiCheck(BaseModel):\n",
    "    \"\"\"Structured output indicating whether text contains PII.\"\"\"\n",
    "    is_pii: bool = Field(description=\"Whether the text contains PII\")\n",
    "\n",
    "def message_with_pii(pii_middleware):\n",
    "    agent = create_agent(\n",
    "        model=basic_model,\n",
    "        middleware=[pii_middleware],\n",
    "    )\n",
    "\n",
    "    # This request will be blocked before any processing\n",
    "    result = agent.invoke({\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": dedent(\n",
    "                \"\"\"\n",
    "                File \"/home/luochang/proj/agent.py\", line 53, in my_agent\n",
    "                    agent = create_react_agent(\n",
    "                            ^^^^^^^^^^^^^^^^^^^\n",
    "                File \"/home/luochang/miniconda3/lib/python3.12/site-packages/typing_extensions.py\", line 2950, in wrapper\n",
    "                    return arg(*args, **kwargs)\n",
    "                        ^^^^^^^^^^^^^^^^^^^^\n",
    "                File \"/home/luochang/miniconda3/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 566, in create_react_agent\n",
    "                    model = cast(BaseChatModel, model).bind_tools(\n",
    "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "                AttributeError: 'RunnableLambda' object has no attribute 'bind_tools'\n",
    "    \n",
    "                ---\n",
    "    \n",
    "                为啥报错\n",
    "                \"\"\").strip()\n",
    "        }]\n",
    "    })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b344fe-5bb5-4349-9183-ed163b5fde4a",
   "metadata": {},
   "source": [
    "**策略一**：如遇敏感信息，拒绝回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6400b371-034f-4c71-8dd6-6c5f115a0b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@before_agent(can_jump_to=[\"end\"])\n",
    "def content_blocker(state: AgentState,  runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n",
    "    # Get the first user message\n",
    "    if not state[\"messages\"]:\n",
    "        return None\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.type != \"human\":\n",
    "        return None\n",
    "\n",
    "    content = last_message.content.lower()\n",
    "    prompt = (\n",
    "        \"你是一个隐私保护助手。请识别下面文本中涉及个人可识别信息（PII），\"\n",
    "        \"例如：姓名、身份证号、护照号、电话号码、邮箱、住址、银行卡号、社交账号、车牌等。\"\n",
    "        \"特别注意，若代码、文件路径中包含用户名，也应被视为敏感信息。\"\n",
    "        \"若包含敏感信息，请返回{\\\"is_pii\\\": True}，否则返回{\\\"is_pii\\\": False}。\"\n",
    "        \"请严格以 json 格式返回，并且只输出 json。文本如下：\\n\\n\" + content\n",
    "    )\n",
    "\n",
    "    pii_agent = trusted_model.with_structured_output(PiiCheck)\n",
    "    result = pii_agent.invoke(prompt)\n",
    "\n",
    "    if result.is_pii is True:\n",
    "        # Block execution before any processing\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n",
    "            }],\n",
    "            \"jump_to\": \"end\"\n",
    "        }\n",
    "    else:\n",
    "        print(\"No PII found\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ba27fa-cb71-4dd2-b051-7ef949d1cfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "File \"/home/luochang/proj/agent.py\", line 53, in my_agent\n",
      "    agent = create_react_agent(\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/luochang/miniconda3/lib/python3.12/site-packages/typing_extensions.py\", line 2950, in wrapper\n",
      "    return arg(*args, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/luochang/miniconda3/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 566, in create_react_agent\n",
      "    model = cast(BaseChatModel, model).bind_tools(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'RunnableLambda' object has no attribute 'bind_tools'\n",
      "\n",
      "---\n",
      "\n",
      "为啥报错\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I cannot process requests containing inappropriate content. Please rephrase your request.\n"
     ]
    }
   ],
   "source": [
    "result = message_with_pii(pii_middleware=content_blocker)\n",
    "\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9ad2b-18f9-41e1-995b-81572753f4d2",
   "metadata": {},
   "source": [
    "**策略二**：如遇敏感信息，使用 `*` 号屏蔽敏感信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a946d8-fde3-4348-82e4-3801481a332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@before_agent(can_jump_to=[\"end\"])\n",
    "def content_filter(state: AgentState,  runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n",
    "    # Get the first user message\n",
    "    if not state[\"messages\"]:\n",
    "        return None\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.type != \"human\":\n",
    "        return None\n",
    "\n",
    "    content = last_message.content.lower()\n",
    "    prompt = (\n",
    "        \"你是一个隐私保护助手。请识别下面文本中涉及个人可识别信息（PII），\"\n",
    "        \"例如：姓名、身份证号、护照号、电话号码、邮箱、住址、银行卡号、社交账号、车牌等。\"\n",
    "        \"特别注意，若代码、文件路径中包含用户名，也应被视为敏感信息。\"\n",
    "        \"若包含敏感信息，请返回{\\\"is_pii\\\": True}，否则返回{\\\"is_pii\\\": False}。\"\n",
    "        \"请严格以 json 格式返回，并且只输出 json。文本如下：\\n\\n\" + content\n",
    "    )\n",
    "\n",
    "    pii_agent = trusted_model.with_structured_output(PiiCheck)\n",
    "    result = pii_agent.invoke(prompt)\n",
    "\n",
    "    if result.is_pii is True:\n",
    "        mask_prompt = (\n",
    "            \"你是一个隐私保护助手。请将下面文本中的所有个人可识别信息（PII）用星号（*）替换。\"\n",
    "            \"仅替换敏感片段，其他文本保持不变。\"\n",
    "            \"只输出处理后的文本，不要任何解释或额外内容。文本如下：\\n\\n\" + last_message.content\n",
    "        )\n",
    "        masked_message = basic_model.invoke(mask_prompt)\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": masked_message.content\n",
    "            }]\n",
    "        }\n",
    "    else:\n",
    "        print(\"No PII found\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2480e24d-39b9-4329-a008-6b162850f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "File \"/home/luochang/proj/agent.py\", line 53, in my_agent\n",
      "    agent = create_react_agent(\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/luochang/miniconda3/lib/python3.12/site-packages/typing_extensions.py\", line 2950, in wrapper\n",
      "    return arg(*args, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/luochang/miniconda3/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 566, in create_react_agent\n",
      "    model = cast(BaseChatModel, model).bind_tools(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'RunnableLambda' object has no attribute 'bind_tools'\n",
      "\n",
      "---\n",
      "\n",
      "为啥报错\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "File \"/home/********/proj/agent.py\", line 53, in my_agent\n",
      "    agent = create_react_agent(\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/********/miniconda3/lib/python3.12/site-packages/typing_extensions.py\", line 2950, in wrapper\n",
      "    return arg(*args, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "File \"/home/********/miniconda3/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 566, in create_react_agent\n",
      "    model = cast(BaseChatModel, model).bind_tools(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'RunnableLambda' object has no attribute 'bind_tools'\n",
      "\n",
      "---\n",
      "\n",
      "为啥报错\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "这个错误的原因是：**你传给 `create_react_agent` 的 model 参数是一个 `RunnableLambda` 对象，而不是一个支持 `bind_tools` 方法的聊天模型**。\n",
      "\n",
      "## 问题分析\n",
      "\n",
      "`create_react_agent` 函数期望接收一个实现了 `bind_tools` 方法的聊天模型（如 `ChatOpenAI`、`ChatAnthropic` 等），但你传入的是一个 `RunnableLambda` 对象，这个对象没有 `bind_tools` 方法。\n",
      "\n",
      "## 解决方案\n",
      "\n",
      "### 方案1：直接使用聊天模型（推荐）\n",
      "\n",
      "```python\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langgraph.prebuilt import create_react_agent\n",
      "\n",
      "# 直接使用聊天模型\n",
      "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
      "\n",
      "agent = create_react_agent(\n",
      "    model=model,\n",
      "    tools=your_tools,\n",
      "    # 其他参数...\n",
      ")\n",
      "```\n",
      "\n",
      "### 方案2：如果你需要使用 RunnableLambda，需要包装一下\n",
      "\n",
      "```python\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_core.runnables import RunnableLambda\n",
      "\n",
      "# 先创建聊天模型\n",
      "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
      "\n",
      "# 如果你需要自定义处理逻辑\n",
      "def custom_process(input_data):\n",
      "    # 你的自定义逻辑\n",
      "    return input_data\n",
      "\n",
      "# 创建 RunnableLambda\n",
      "custom_runnable = RunnableLambda(custom_process)\n",
      "\n",
      "# 在 create_react_agent 中仍然使用原始的聊天模型\n",
      "agent = create_react_agent(\n",
      "    model=chat_model,  # 使用原始聊天模型，不是 RunnableLambda\n",
      "    tools=your_tools,\n",
      ")\n",
      "```\n",
      "\n",
      "### 方案3：检查你的 model 创建代码\n",
      "\n",
      "看起来你可能这样创建了 model：\n",
      "\n",
      "```python\n",
      "# 错误的方式\n",
      "model = RunnableLambda(some_function)\n",
      "agent = create_react_agent(model=model, tools=tools)  # 这会报错\n",
      "```\n",
      "\n",
      "应该改为：\n",
      "\n",
      "```python\n",
      "# 正确的方式\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "model = ChatOpenAI(model=\"gpt-3.5-turbo\")  # 或其他聊天模型\n",
      "agent = create_react_agent(model=model, tools=tools)\n",
      "```\n",
      "\n",
      "## 总结\n",
      "\n",
      "确保传给 `create_react_agent` 的 `model` 参数是一个标准的聊天模型对象，而不是 `RunnableLambda` 或其他不支持 `bind_tools` 方法的对象。\n"
     ]
    }
   ],
   "source": [
    "result = message_with_pii(pii_middleware=content_filter)\n",
    "\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d7771-f9b8-4408-9387-c67c51c1a155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3.13)",
   "language": "python",
   "name": "py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
