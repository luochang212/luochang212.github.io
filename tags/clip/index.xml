<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CLIP on Chang Luo</title>
    <link>https://luochang212.github.io/tags/clip/</link>
    <description>Recent content in CLIP on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 29 Dec 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/tags/clip/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>搭建一款简单的『以文搜图』应用</title>
      <link>https://luochang212.github.io/posts/clip_app/</link>
      <pubDate>Sun, 29 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/clip_app/</guid>
      <description>在手机相册搜索框中输入“笔记本电脑”时，即使照片本身并未包含“笔记本电脑”这几个字，相关图片依然能够被精准地检索出来。这说明现代相册 APP 不仅使用图片 OCR 文本来召回搜索结果。它综合使用了多种技术，包括多模态技术。&#xA;本文使用多模态模型 CLIP 搭建一个简单的『以文搜图』应用，实现与相册 APP 类似的搜索效果。&#xA;GitHub 项目地址：clip-app&#xA;本文涉及的内容包括：&#xA;用 FastAPI 搭建图文向量生成服务 用 FastAPI 搭建 Faiss 向量搜索服务 集成以上两个服务，实现『以文搜图』应用 使用 @torch.inference_mode() 装饰器，优化推理性能 ✨ 如果你对多模态模型的效果有更高的要求，可以使用 BLIP-2。&#xA;一、加载 CLIP 模型 加载 CLIP 模型，生成图文 Embedding。&#xA;获取文本 Embedding 获取图像 Embedding 多条文本对一张图片 一条文本对多张图片 多条文本对多张图片 查看笔记 二、Faiss 向量搜索 用 FastAPI + Faiss 写一个向量检索服务 faiss/server.py，该服务提供两个接口：&#xA;路由名 描述 add 用于向索引中添加新的 Embedding search 用于从索引中取回给定 Embedding 的最近邻 Embedding 查看笔记 三、CLIP 向量生成 用 FastAPI 写一个简单的 CLIP Embedding 生成服务 embedding/server.</description>
    </item>
    <item>
      <title>Triton 部署 CLIP 图文 Embedding 推理服务</title>
      <link>https://luochang212.github.io/posts/clip_triton_server/</link>
      <pubDate>Sat, 30 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/clip_triton_server/</guid>
      <description>本文介绍如何用 Triton 在多 GPU 环境下部署高性能 CLIP 推理服务。&#xA;GitHub 项目地址：clip-server&#xA;CLIP 是一个多模态模型。它能将图像和文本映射到同一个向量空间中，由此可以产生诸多应用。比如，通过计算图片与文本的相似性，可以用近似最近邻 (ANN) 从相册中检索与给定 query 语义相近的图片。此外，CLIP 的 Vision Encoder 可以作为特征提取器使用，用于生成的图像 Embedding。如果在 Vision Encoder 后加一个 fc 层，并且冻住骨干网络仅对 fc 层做训练，通常可以得到一个效果不错的图像分类器。&#xA;本文涉及的内容包括：&#xA;用 transformers 库运行 openai/clip-vit-base-patch32 的简单示例 在 titanic 数据集上训练一个 MLP 并导出成 ONNX 格式 介绍如何安装预装了 Triton 的 Nvidia 官方 Docker 镜像 &amp;amp; 启动容器 介绍如何将 MLP 的 ONNX 模型配置到 Triton 模型仓库中 写了一个简单的 客户端 用于获取 Triton 的推理结果 介绍 Triton 的 Python Backend，其通常用于模型预处理和后处理 用 Model Ensemble 组装 Python Backend 和 ONNX 组成完整的推理服务 ✨ 注意：运行以下代码依赖 utils.</description>
    </item>
  </channel>
</rss>
