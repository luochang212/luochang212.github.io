<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLaMA Factory on Chang Luo</title>
    <link>https://luochang212.github.io/tags/llama-factory/</link>
    <description>Recent content in LLaMA Factory on Chang Luo</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 26 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/tags/llama-factory/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>三种方法实现监督微调：LLaMA Factory, trl 和 unsloth</title>
      <link>https://luochang212.github.io/posts/sft_note/</link>
      <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/sft_note/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;尝试了三种流行的微调框架，其中最推荐的是 unsloth，因为快！另外两种框架 LLaMA Factory 和 trl 是在夜里跑的，显卡风扇响了一宿。第二天看日志，它们都跑了三个多小时才跑完。但是同样的任务，unsloth 只需要五分钟，快得有点离谱。当然，这么比不是完全公平的，因为它们的量化方法、LoRA 参数是不同的。但是 unsloth 快这一点依然是无可质疑的。如果在 GPU 服务器上认真微调，那么用 LLaMA Factory 没毛病；但如果只是在笔记本上随便玩玩，unsloth 的优势就太大了。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/sft-note&#34; target=&#34;_blank&#34;&gt;sft-note&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;⭐ 本文的内容包括：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;大模型微调的三种范式：无监督微调、监督微调、强化学习微调&lt;/li&gt;&#xA;&lt;li&gt;介绍用于监督微调的数据格式，以及如何加载数据集&lt;/li&gt;&#xA;&lt;li&gt;如何下载 Qwen 模型，代码见 &lt;a href=&#34;https://github.com/luochang212/sft-note/blob/main/model/download_qwen.py&#34; target=&#34;_blank&#34;&gt;download_qwen.py&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;使用三种框架微调大模型：LLaMA Factory, trl, unsloth&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;一引言&#34;&gt;一、引言&lt;/h2&gt;&#xA;&lt;p&gt;大语言模型有很强的通用能力，但在特定领域，它的表现不如领域小模型。为了让大模型适应特定任务，我们对大模型进行微调，使大模型在保持通用性的同时，兼具领域模型的专业知识、对话风格和输出格式等特质。&lt;/p&gt;&#xA;&lt;p&gt;微调大模型有三种范式：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;无监督微调&lt;/strong&gt;：在海量数据上进行二次预训练&#xA;&lt;ul&gt;&#xA;&lt;li&gt;PT 增量预训练&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;监督微调 (SFT)&lt;/strong&gt;：构造领域数据集，增强模型的指令遵循能力，并注入领域知识&#xA;&lt;ul&gt;&#xA;&lt;li&gt;指令微调&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;强化学习微调&lt;/strong&gt;：通过 reward 引导模型优化&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;RLHF&lt;/a&gt; 基于人类反馈的强化学习&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.18290&#34;&gt;DPO&lt;/a&gt; 直接偏好优化方法&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07691&#34;&gt;ORPO&lt;/a&gt; 比值比偏好优化&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.03300&#34;&gt;GRPO&lt;/a&gt; 群体相对策略优化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;本文聚焦 &lt;strong&gt;监督微调 (Supervised Fine-Tuning)&lt;/strong&gt;。监督微调是一种简单但有效的微调方式，能够快速融合业务数据、适应业务场景，因此它的性价比极高！&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-sft-的简单介绍&#34;&gt;1. SFT 的简单介绍&lt;/h3&gt;&#xA;&lt;p&gt;监督微调的优化目标是 &lt;strong&gt;最小化模型生成回答与目标回答之间的差异&lt;/strong&gt;，通常使用交叉熵损失。为避免破坏预训练阶段获得的知识，SFT 阶段通常使用 &lt;strong&gt;较低的学习率&lt;/strong&gt;，并且只更新部分参数层，其他参数保持不变。与预训练阶段所需的海量数据相比，SFT 只需 &lt;strong&gt;较小的数据量&lt;/strong&gt;（数千到数十万样本），即可完成微调。&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-sft-的使用场景&#34;&gt;2. SFT 的使用场景&lt;/h3&gt;&#xA;&lt;p&gt;为了让大家感受一下 SFT 能做什么，下面列举一些使用场景：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
