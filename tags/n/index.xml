<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>N on Chang Luo</title>
    <link>http://localhost:1313/tags/n/</link>
    <description>Recent content in N on Chang Luo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Jun 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/n/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>如何生成中文词向量</title>
      <link>http://localhost:1313/posts/chinese_embedding/</link>
      <pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/chinese_embedding/</guid>
      <description>怎么训练词向量&#xA;预训练模型&#xA;自己训练&#xA;方法有哪些，都需要哪些输入，怎么训练&#xA;word2vec&#xA;two tower DNN https://medium.com/better-ml/embedding-learning-for-retrieval-29af1c9a1e65&#xA;bert&#xA;transformer&#xA;roformer https://huggingface.co/docs/transformers/model_doc/roformer&#xA;langchain(2)—基于开源embedding模型的中文向量效果测试 - 袋鼠猪的文章 - 知乎 https://zhuanlan.zhihu.com/p/635670918&#xA;embedding方法分类为：word2vec（基于seq2seq的神经网络结构）、Glove（词共现矩阵）、Item2Vec（推荐中的双塔模型）、FastText（浅层神经网络）、ELMo（独立训练双向，stacked Bi-LSTM架构）、GPT（从左到右的单向Transformer）、BERT（双向transformer的encoder，attention联合上下文双向训练）&#xA;附录：和 Q 老师的对话记录 Q: nn.Embedding 是一种预训练的embedding模型吗&#xA;Qwen-2.5: nn.Embedding本身并不是一个预训练模型，而是深度学习框架（如PyTorch）中提供的一个层（Layer），用于将离散的类别（如词汇ID）转换为连续的向量空间中的向量，即词嵌入（word embeddings）。这个层的权重是可学习的，意味着在模型训练过程中，这些嵌入向量会根据任务目标进行优化。&#xA;当你创建一个nn.Embedding层时，你可以指定词汇表的大小（vocab_size）和嵌入向量的维度（embedding_dim）。初始时，这些嵌入向量通常是随机初始化的。然而，你可以选择用预训练好的词向量（如Word2Vec、GloVe）来初始化这个层的权重，从而利用预训练模型学到的语义信息作为模型训练的起点，这可以加速收敛过程并可能提高最终模型的表现。&#xA;总结来说，nn.Embedding是一个工具，它允许你在神经网络中使用并且学习词嵌入，但它本身不包含预训练的过程。预训练的词向量可以被用来初始化nn.Embedding层，但它们是分开的概念。&#xA;Q: nn.Embedding 中每个词的对应的向量在训练过程中会发生变化吗&#xA;Qwen-2.5: 是的，在使用nn.Embedding的训练过程中，每个词对应的向量会发生变化。这是因为在PyTorch中，nn.Embedding层的权重是模型参数的一部分，它们会随着反向传播和优化算法（如梯度下降）的进行而更新。这意味着在训练迭代中，根据网络的损失函数以及优化目标，词嵌入向量会被调整以更好地捕获输入数据中的模式和语义关系。&#xA;简而言之，nn.Embedding层中的词嵌入向量不是静态不变的，而是可以根据训练数据动态学习和优化的，这一特性使得它们能够为特定任务提供更合适的词表示。</description>
    </item>
  </channel>
</rss>
