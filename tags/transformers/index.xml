<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformers on Chang Luo</title>
    <link>https://luochang212.github.io/tags/transformers/</link>
    <description>Recent content in Transformers on Chang Luo</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 21 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/tags/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>本地部署大模型：Ollama 和 vLLM</title>
      <link>https://luochang212.github.io/posts/llm_deploy/</link>
      <pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/llm_deploy/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;2023 年本地部署大模型的报价近千万，2024 年初便骤降至百万，如今是 2025 年，只需要一行 vLLM 命令就可以部署大模型，人工成本几近于零。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;GitHub 项目地址：&lt;a href=&#34;https://github.com/luochang212/llm-deploy&#34; target=&#34;_blank&#34;&gt;llm-deploy&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;本文内容包括：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;3 种方式部署 DeepSeek R1：Ollama, vLLM 和 Transformers&lt;/li&gt;&#xA;&lt;li&gt;使用 vLLM 部署 Qwen2.5 模型&lt;/li&gt;&#xA;&lt;li&gt;安装 Open WebUI 作为本地模型的前端聊天框&lt;/li&gt;&#xA;&lt;li&gt;通过 &lt;code&gt;vllm serve&lt;/code&gt; 实现一行代码启动 vLLM 推理服务&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;✨ 快速部署说明在 &lt;a href=&#34;https://github.com/luochang212/llm-deploy/tree/main/deploy&#34; target=&#34;_blank&#34;&gt;/deploy&lt;/a&gt;，vLLM 服务启动脚本在 &lt;a href=&#34;https://github.com/luochang212/llm-deploy/tree/main/server&#34; target=&#34;_blank&#34;&gt;/server&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;一本地部署-deepseek-r1&#34;&gt;一、本地部署 DeepSeek R1&lt;/h2&gt;&#xA;&lt;p&gt;大模型本地部署依赖推理引擎，目前比较流行的推理引擎有：&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;推理引擎&lt;/th&gt;&#xA;          &lt;th&gt;场景&lt;/th&gt;&#xA;          &lt;th&gt;介绍&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;适合个人开发者和轻量级应用&lt;/td&gt;&#xA;          &lt;td&gt;基于 &lt;a href=&#34;https://github.com/ggml-org/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; 开发，支持 CPU 推理，安装简单，开箱即用，适合快速原型开发和测试&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;适合高并发生产环境&lt;/td&gt;&#xA;          &lt;td&gt;支持多 GPU 并行、批处理、PagedAttention，吞吐量高，延迟低，适合大规模服务部署&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;适合模型研究和实验&lt;/td&gt;&#xA;          &lt;td&gt;提供完整的模型训练和推理接口，支持模型微调、量化、加速，适合研究人员和需要深度定制的场景&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;SGLang&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;适合需要复杂推理流程的场景&lt;/td&gt;&#xA;          &lt;td&gt;支持结构化输出、并行推理、流式输出，特别适合需要多轮对话和复杂推理的应用&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://github.com/InternLM/lmdeploy&#34;&gt;LMDeploy&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;适合企业级部署和边缘计算&lt;/td&gt;&#xA;          &lt;td&gt;由上海人工智能实验室开发，提供完整的模型量化、加速和部署工具链，支持多种硬件平台，特别适合资源受限场景&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;下面介绍如何部署 Ollama, vLLM, Transformers 这三款推理引擎，简要部署步骤见本项目的 &lt;a href=&#34;https://github.com/luochang212/llm-deploy/tree/main/deploy&#34;&gt;deploy&lt;/a&gt; 目录。&lt;/p&gt;&#xA;&lt;p&gt;目录：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
