<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dropout on Chang Luo</title>
    <link>https://luochang212.github.io/tags/dropout/</link>
    <description>Recent content in Dropout on Chang Luo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/tags/dropout/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>像搭积木一样搭神经网络</title>
      <link>https://luochang212.github.io/posts/dl_tricks/</link>
      <pubDate>Sun, 07 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/dl_tricks/</guid>
      <description>搭积木的时候，我们将不同类型的积木搭在一起。而对每一种类型的积木，又有多种变体可供选择。比如门框，既可以是文艺复兴风格，也可以是中式庭园风格。搭神经网络就和搭积木很像。神经网络中也充满了各种类型的积木，以及同类型下积木的变体。本文的目标是带大家认识神经网络中的“积木”。&#xA;GitHub 项目地址：dl-tricks&#xA;一、概述 1.1 从 MLP 说起 多层感知机 (MLP) 是最简单的深度神经网络。麻雀虽小，五脏俱全。其实只要想一遍数据是如何在 MLP 中流动的，就能把深度学习中的各种组件想个七七八八了。&#xA;下图是一个 4 层的多层感知机，左边是特征侧，右边是标签侧。训练开始时，样本数据先从左到右，做 正向传播。数据流转到标签侧后，用 损失函数 计算梯度，再拿 优化器 沿负梯度方向逐层反向更新参数。这样下一 批量 数据 (batch) 重新从特征侧开始做正向传播时，正好能用上前一轮更新的参数。&#xA;为了让例子更具体，我们假设有 2560 个样本，把它们分成 10 个批量，这样每批量就有 256 个样本。训练过程中，如果把 10 个批量数据全部训练一遍，就叫完成一个训练轮次 (epoch). 通常，神经网络需要训练多个轮次才会收敛。&#xA;下图展示样本、批量和轮次的关系：&#xA;1.2 神经元里发生了什么 让我们来看看每个神经元里发生了什么。&#xA;假定我们的样本有 10 个特征 (features)，考虑各个层的维数：&#xA;第一层的维数必须与样本特征数相同 最后一层的维数必须与预测类别数相同 中间隐藏层的维数比较自由，可以灵活地调整 假定一到四层的维数如下：&#xA;[Layer 0] 第一层维数为 10 [Layer 1] 第二层维数为 12 [Layer 2] 第三层维数为 12 [Layer 3] 第四层维数为 10 现在，我们来考虑一个样本做正向传播的情况：&#xA;第一层：&#xA;直接把样本的 10 个特征，分别填入 10 个神经元里就好了。</description>
    </item>
  </channel>
</rss>
