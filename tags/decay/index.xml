<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Decay on Chang Luo</title>
    <link>https://luochang212.github.io/tags/decay/</link>
    <description>Recent content in Decay on Chang Luo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/tags/decay/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>像搭积木一样搭神经网络</title>
      <link>https://luochang212.github.io/posts/dl_tricks/</link>
      <pubDate>Sun, 07 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/dl_tricks/</guid>
      <description>搭积木时，要用到多种类型的积木，而对同一类型的积木，又有多种可用变体。比如，我们既可以用中式田园风格的门框，也可以用文艺复兴时期的门框。神经网络中也充满了各种类型的积木，以及同一类型下的不同变体。本文的目标是带大家看看神经网络中的各种“积木”。&#xA;GitHub 项目地址：dl-tricks/note.ipynb&#xA;一、概述 1.1 从 MLP 说起 多层感知机 (MLP) 是非常简单的深度神经网络。麻雀虽小，五脏俱全。只要想想数据是如何在 MLP 中流动的，就能把深度学习中的各种组件想个七七八八。&#xA;下图是一个 4 层感知机，左边是特征，右边是标签。训练开始时，样本数据先从左到右做 正向传播。待数据流到右侧，用 损失函数 计算损失。此时损失是一个标量，而最后一层的节点权重 $W$ 是一个矩阵，标量对矩阵的偏导是矩阵。优化器 会用大小合适的梯度矩阵，沿负梯度方向逐层反向更新权重 $W$。这样下一 批量 (batch) 数据进入网络时，正好能用上一轮更新后的参数做正向传播。&#xA;1.2 DataLoader 样本是有限的。为了榨干样本的最后一丝性能，同一个样本往往要反复使用多次。这就要 DataLoader 上场了。DataLoader 把数据编排成一个个批量。并构建了一个迭代器，每次调用它都能从第一个批量开始遍历。&#xA;我们来写一个实现以上功能的野生 DataLoader：&#xA;import math import torch class DataLoader: def __init__(self, data: list, batch_size: int): self.i = 0 self.batch_size = batch_size self.batch_num = math.floor(len(data) / batch_size) self._data = self.gen_batch(data) def gen_batch(self, data): lst = [] s = self.batch_size for i in range(self.</description>
    </item>
  </channel>
</rss>
