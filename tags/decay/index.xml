<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Decay on Chang Luo</title>
    <link>https://luochang212.github.io/tags/decay/</link>
    <description>Recent content in Decay on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/tags/decay/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>像搭积木一样搭神经网络</title>
      <link>https://luochang212.github.io/posts/dl_tricks/</link>
      <pubDate>Sun, 07 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/dl_tricks/</guid>
      <description>搭积木的时候，需要将不同类型的积木搭在一起：门框、窗子、走廊、屋顶。对每一种类型的积木，又有多种变体可供选择。比如，屋顶可以用文艺复兴风格，也可以用中式庭园风格。神经网络也是，学神经网络，本质上就是认识各种各样“积木”的过程。&#xA;GitHub 项目地址：dl-tricks/note.ipynb&#xA;一、必要组件 1.1 从 MLP 说起 我们从最简单的深度神经网络 多层感知机 (MLP) 开始说起。麻雀虽小，五脏俱全。了解数据如何在 MLP 中流动，就能大致勾勒一个神经网络的 必要组件。&#xA;下图是一个 4 层感知机，左边是特征，右边是标签。训练开始时，样本数据先从左到右做 正向传播。待数据流到右侧，用 损失函数 计算损失。此时损失是一个标量，而最后一层的节点权重 $W$ 是一个矩阵，标量对矩阵的偏导是矩阵。优化器 会用大小合适的梯度矩阵，沿负梯度方向逐层反向更新权重 $W$。这样下一 批量 (batch) 数据进入网络时，正好能用上一轮更新后的参数做正向传播。&#xA;1.2 DataLoader 样本是有限的，为了让模型获得最强性能，必须榨干每个样本的价值。&#xA;因此在训练中，一个样本往往要复用多次。DataLoader 就在做这样一件事。它把数据编排成一个个批量，并构建一个迭代器。每次调用它，会返回一个从第一个批量开始遍历的迭代器。这个特性使得复用样本变得更加方便。&#xA;原生的 PyTorch DataLoader 很复杂，让我们来实现一个野生 DataLoader：&#xA;import math import torch class DataLoader: def __init__(self, data: list, batch_size: int): self.i = 0 self.batch_size = batch_size self.batch_num = math.ceil(len(data) / batch_size) self._data = self.gen_batch(data) def gen_batch(self, data): lst = [] s = self.</description>
    </item>
  </channel>
</rss>
