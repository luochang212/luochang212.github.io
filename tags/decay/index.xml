<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Decay on Chang Luo</title>
    <link>https://luochang212.github.io/tags/decay/</link>
    <description>Recent content in Decay on Chang Luo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/tags/decay/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>像搭积木一样搭神经网络</title>
      <link>https://luochang212.github.io/posts/dl_tricks/</link>
      <pubDate>Sun, 07 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/dl_tricks/</guid>
      <description>搭积木的时候，我们将不同类型的积木搭在一起。而对每一种类型的积木，又有多种变体可供选择。比如门框，既可以是文艺复兴风格，也可以是中式庭园风格。神经网络也一样，充满了各种各样的“积木”，以及“积木”的变体。学习神经网络，其实就是认“积木”的过程。&#xA;GitHub 项目地址：dl-tricks/note.ipynb&#xA;一、概述 1.1 从 MLP 说起 多层感知机 (MLP) 是非常简单的深度神经网络。麻雀虽小，五脏俱全。只要想想数据是如何在 MLP 中流动的，就能把深度学习中的各种组件想个七七八八。注意到，我们这里给神经网络中的“积木”一个比较正式的名称 &amp;mdash;&amp;mdash; 组件。&#xA;下图是一个 4 层感知机，左边是特征，右边是标签。训练开始时，样本数据先从左到右做 正向传播。待数据流到右侧，用 损失函数 计算损失。此时损失是一个标量，而最后一层的节点权重 $W$ 是一个矩阵，标量对矩阵的偏导是矩阵。优化器 会用大小合适的梯度矩阵，沿负梯度方向逐层反向更新权重 $W$。这样下一 批量 (batch) 数据进入网络时，正好能用上一轮更新后的参数做正向传播。&#xA;1.2 DataLoader 样本是有限的。为了榨干样本的最后一丝价值，一个样本往往要反复利用多次。这就要 DataLoader 上场了。DataLoader 把数据编排成一个个批量，并构建一个迭代器，使得每次调用它，都能从第一个批量开始遍历。&#xA;让我们来实现一个野生 DataLoader：&#xA;import math import torch class DataLoader: def __init__(self, data: list, batch_size: int): self.i = 0 self.batch_size = batch_size self.batch_num = math.floor(len(data) / batch_size) self._data = self.gen_batch(data) def gen_batch(self, data): lst = [] s = self.</description>
    </item>
  </channel>
</rss>
