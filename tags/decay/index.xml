<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Decay on Chang Luo</title>
    <link>http://localhost:1313/tags/decay/</link>
    <description>Recent content in Decay on Chang Luo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 30 Jun 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/decay/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深度学习优化技术</title>
      <link>http://localhost:1313/posts/dl_tricks/</link>
      <pubDate>Sun, 30 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/dl_tricks/</guid>
      <description>今天来复习深度学习中的组件。其实只要试着想一遍数据是如何在 MLP 中流动的，就能把深度学习的关键组件想得七七八八了。&#xA;GitHub 项目地址：dl-tricks&#xA;下图中，我们约定，左边是特征侧，右边是标签侧。&#xA;训练开始时，样本数据先从左到右，从特征侧到标签侧，做正向传播。当传播到标签侧时，我们用 损失函数 计算梯度。再拿梯度，用 优化器 反向逐层更新参数，做反向传播。这样下一个批量 (batch) 做计算就可以用新参数做正向传播了。以上就是一轮迭代的过程，怎么样，深度学习很简单吧 (`ヮ´ )&#xA;以上，我们已经接触到两个组件了，分别是：&#xA;损失函数 优化器 实际上，正向传播过程中，还有一些小细节。在每个节点上，都有一个参数 W。W 的维度取决于上一层网络神经元的数量。&#xA;我们以第 2 列，从上往下数第 3 个节点为例。它的上一层网络有 m=10 个神经元，因此 W 是一个 10 维向量。当然，节点自身还有当前批量样本，计算到当前节点位置的 x 值。&#xA;$$ ReLU (W x + b) $$&#xA;正向传播时，先计算 W 和 x 的内积，然后加上一个偏置 b，再计算以上结果过 激活函数 的值，最后用这个值更新 x。我们注意到，这里出现了激活函数。激活函数也是深度学习中的重要组件。&#xA;回顾完 MLP 的整体框架，接下来，我们来复习各个组件的实现。&#xA;一、激活函数 1.1 Sigmoid 1.2 ReLU 1.3 Tanh 1.4 Softmax 二、优化器 2.1 SGD 2.2 Adam 三、损失函数 3.1 均方误差 均方误差（Mean Squared Error, MSE）</description>
    </item>
  </channel>
</rss>
