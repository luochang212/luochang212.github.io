<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>jupyter notebook on Chang Luo</title>
    <link>http://luochang212.github.io/tags/jupyter-notebook/</link>
    <description>Recent content in jupyter notebook on Chang Luo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Apr 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://luochang212.github.io/tags/jupyter-notebook/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>我的微博数据可视化</title>
      <link>http://luochang212.github.io/posts/my_weibo/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>http://luochang212.github.io/posts/my_weibo/</guid>
      <description>GitHub项目链接：weibo-analysis
这次的数据用的是本科期间就已经爬好，丢在硬盘里的数据。但因为raw data里夹杂着大量和博文无关的字符，当时没有足够的编程技巧处理数据，这一丢就是两年。现在，本python初丁趁着还有机会摸鱼，赶紧把数据翻出来，让它们发光发热。
文本获取 因为新浪微博严防死打，现如今微博的数据越来越不好爬。Github上的微博爬虫生存周期通常都很短，使爬取数据的成本大大增加。但是万变不离其宗，最笨的方式常常是最有效的。这里我用的是微博@失眠狸 同学的方法，用鼠标精灵写了个插件，控制快捷键和页面拖动，把内容从浏览器上粘贴到sublime里。
生成csv文件 有了原始数据，接下来我们要把数据归一化，做成方便处理的数据。一个常用的方法就是将数据整理成csv文件。
第一步，先将原始文件按字段进行分割。根据原数据，我划分了五个字段: id, date, time, device, content, 它们分别记录本条微博的文件位置、发布日期、发布时间、发送设备和文本内容。
设计完数据结构之后，我们先用split函数对原数据进行粗略划分，再用find函数进一步定位到精确位置。接着提取各字段内容，再依次存入csv。整理数据的工作就完成啦。
可视化微博数据 有了csv文件，可视化数据就很方便了。此时我从PyCharm平台换到jupyter notebook工作，因为相较于PyCharm, ipynb可以制作的各式各样的可视化图表和窗口小工具(widget), 更适合于数据科学工作。至于工具包，这里我选择了数据分析工具pandas和可视化工具seaborn.
好了，废话少说，让我们进入分析流程。
首先是需求分析，我的目标如下:
 绘制日期分布热力图，观察今年使用微博频率的趋势
 绘制设备使用直方图，看看平时最常用什么平台发博
 绘制时间分布直方图，观察一天之中各时段的发博频率
 调用窗口工具，拖动查看各个时间段都发了什么内容
  以上代码详见github repository, 就不赘述了。
分析结果如下:
热力图总的来说颜色逐年加深，说明我正在逐渐成为一个微博控。而且注意到往年年初我是不怎么玩微博的，但随着年纪渐长，1-3月份我玩微博的频率越来越高，这意味着过年可能越来越无聊，没有年味，从而加长了我混迹微博的时间。
是你吗？华为的舔狗~
晚上2点不睡的坏小孩，早上10点起的偷懒者。（此处是一个卑微的笑容）</description>
    </item>
    
    <item>
      <title>用Jupyter notebook规划旅行路线</title>
      <link>http://luochang212.github.io/posts/tsp_route/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>http://luochang212.github.io/posts/tsp_route/</guid>
      <description>GitHub项目链接：tsp-route
对于在欧洲的小伙伴们，复活节假在这一周就正式开始啦。大家都是怎么计划旅行的呢？
我的习惯是在出发前的最后一个晚上，花上半小时，在网上搜索当地感兴趣的景点 (POI), 然后在Google Maps上为它们点上小星星，以免和它们插肩而过。
每次搜完资料，我的谷歌地图上都攒满了密密麻麻的小星星。此时此刻，望着这些密集的星星，本人的偷懒本能发动了。我不禁发出灵魂之问：如何才能走最少的路，就周游所有景点呢?
找遍了谷歌和百度，都没发现我要路径规划功能。最接近需求还是谷歌的&amp;rdquo;Add destination&amp;rdquo;功能。然而这个功能只是把你点选的地点依次连接在一起罢了。可是次序对我们来说并不重要，只要总路径最短就好了。
没有现成货怎么办，自己写一个呗。
适用模型：TSP 模型 首先把问题抽象化，概括要解决的问题。我们的问题是：求从一个地方出发，遍历所有城市，回到起点的最短路径
这其实对应了一个非常经典的问题，那就是旅行商问题，又名TSP问题。旅行商问题的描述是这样的：一个商人在各个城市之间旅行，要求遍历所有城市并返回到出发点，要如何规划路线，才能使总路径最短。(这里就不过多解释了，更多信息见维基百科)
解决思路  用googlemaps包获取纬度和经度信息 用OR-Tools包求解TSP问题 最后用gmaps可视化结果  敲下来以后不禁感慨，Google Maps的API真的太好用了，到头来根本不需要自己敲多少代码orz
食用指南 项目地址 &amp;ndash;&amp;gt; 传送门
在体验之前，你需要以下配置：
 你需要一个Jupyter notebook, 推荐是直接安Anaconda3. 你还需要安装这些包：googleplaces, googlemaps, gmaps, ortools. 你需要一个Google Maps API key，获取链接: https://developers.google.com/maps/documentation/distance-matrix/start#get-a-key  完成配置等于成功的一半。在Jupyter notebook打开TSPSolver.ipynb，将第一个代码块的所有变量，改成自己的。比如自己的目的地和自己的API密码……最后从头到尾运行所有代码块，你就可以得到自己的旅行地图辣~
配置代码如下。
# input the places of interest (POI) places = &#39;YHA London Central Hostel&#39;, &#39;Coca-Cola London Eye&#39;, &#39;St. Paul\&#39;s Cathedral&#39;, &#39;Leadenhall Market&#39;, &#39;The National Gallery&#39; \ &#39;Big Ben&#39;, &#39;Buckingham Palace&#39;, &#39;Waterloo Station&#39; # the region Location=&#39;London&#39; # choose a mode Mode = &amp;quot;walking&amp;quot; # &amp;quot;driving&amp;quot;, &amp;quot;walking&amp;quot;, &amp;quot;bicycling&amp;quot;, &amp;quot;transit&amp;quot; # get Google API key from following website: # https://developers.</description>
    </item>
    
  </channel>
</rss>