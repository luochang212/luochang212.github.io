<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jupyter-Notebook on Chang Luo</title>
    <link>http://localhost:1313/tags/jupyter-notebook/</link>
    <description>Recent content in Jupyter-Notebook on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Apr 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/jupyter-notebook/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>微博数据可视化</title>
      <link>http://localhost:1313/posts/my_weibo/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/my_weibo/</guid>
      <description>GitHub 项目地址：weibo-analysis&#xA;这次的数据用的是本科期间就已经爬好，但因为当时没有足够的处理技巧，这些数据在硬盘里一丢就是两年。如今，本Python初丁趁着还有机会摸鱼，赶紧把数据翻出来，让它们发光发热。&#xA;文本获取 因为新浪微博的严防死打，现如今微博的数据越来越不好爬。GitHub上的微博爬虫生存周期通常都很短，使爬取数据的成本大大增加。这里我用的是微博@失眠狸 同学的方法，用鼠标精灵写了个插件，控制快捷键和页面拖动，把内容从浏览器上粘贴到sublime里。&#xA;生成csv文件 有了原始数据，接下来我们要把数据归一化，做成方便处理的数据。一个常用的方法就是将数据整理成csv文件。&#xA;Step 1. 分析需要保存的字段以及数据的维度，从而设计出数据的存储结构。根据原数据，我划分了五个字段: id, date, time, device, content, 它们分别记录一条微博的文件位置、发布日期、发布时间、发送设备和文本内容。&#xA;Step 2. 分割raw data. 先用split函数进行粗略分割，再用find函数精确分割。接着将分割好的内容提取到各字段，并依次存入csv。&#xA;经过上述两步，数据的整理工作就做完啦。&#xA;可视化微博数据 有了csv文件，做数据可视化是分分钟的事。此时我把工作平台从PyCharm搬到了Jupyter Notebook。这是因为Jupyter Notebook可以制作的各式各样的可视化图表和窗口小工具(widget), 比PyCharm更适合数据处理。至于工具包，这里我选的是pandas和seaborn.&#xA;首先是需求分析，我的目标如下:&#xA;绘制日期分布热力图，观察今年使用微博频率的趋势&#xA;绘制设备使用直方图，看看平时最常用什么平台发博&#xA;绘制时间分布直方图，观察一天之中各时段的发博频率&#xA;使用窗口滑块小部件，拖动查看各个时间段都发了什么内容&#xA;这里不描述具体过程，详见GitHub Repository.&#xA;分析结果如下:&#xA;热力图总体来说颜色逐年加深，说明我正在逐渐成为一个微博控。而且注意到往年年初我是不怎么玩微博的，但随着年纪渐长，1-3月份我玩微博的频率越来越高，这意味着过年可能越来越无聊，没有年味，从而加长了我混迹微博的时间。&#xA;是你吗？华为的舔狗~&#xA;晚上2点不睡的坏小孩，早上10点起的偷懒者。（此处是一个卑微的笑容）&#xA;附录：部分代码 下面这段代码分割了文本。&#xA;def classification(self, txt_array, file_index): id = np.array([]) date = np.array([]) time = np.array([]) device = np.array([]) content = np.array([]) count = 0 for ite in range(1, np.size(txt_array), 1): if txt_array[ite].</description>
    </item>
    <item>
      <title>【项目】TSP旅行路线规划</title>
      <link>http://localhost:1313/posts/tsp_route/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/tsp_route/</guid>
      <description>GitHub 项目地址：tsp-route&#xA;对于在欧洲的小伙伴们，复活节假在这一周就正式开始啦。大家都是怎么计划旅行的呢？&#xA;我的习惯是在出发前最后一晚，花上半小时，搜索目的地景点 (Point of interest, POI), 然后在Google Maps上为它们点上小星星，以免和它们擦肩而过。我的地图经过一番操作，就成了下面这副模样。&#xA;此时此刻，望着这些密集的星星，我不禁自问：如何才能走最少的路，遍历所有景点呢?&#xA;搜索了谷歌和百度，都没找到我要路径规划功能。最接近需求的是谷歌地图的&amp;quot;Add destination&amp;quot;功能。然而这个功能只是依次连接你点选的地点。并不能由一组地点，确定连接它们的一条全局最短路径。&#xA;没有现成应用怎么办，我打算自己动手写一个。&#xA;下图是Google Add destination功能。&#xA;适用模型：TSP 模型 用一句话概括需求就是：我们需要一条从某地方出发，遍历所有地点，最终回到起点的最短路径。&#xA;这个需求其实就是运筹学的一个经典问题，旅行商问题(TSP)。旅行商问题的确切描述是这样的：一个商人在各个城市之间旅行，要求遍历所有城市并返回到出发点，要如何规划路线，才能使总路径最短。（打开维基百科了解更多)&#xA;解决思路 用googlemaps包获取纬度和经度信息 用OR-Tools包求解TSP问题 最后用gmaps可视化结果 在敲代码的过程中，最难的地方莫过于看文档查API, 搞清楚输入输出和调用结构。不过敲完这一顿之后我还是不禁感慨，GoogleI太为开发者着想了。一旦学会调用API，实现一个简单应用的代码量还是很小的 orz&#xA;食用指南 项目地址 &amp;ndash;&amp;gt; 传送门&#xA;在运行代码之前，你需要以下配置：&#xA;一个Jupyter Notebook. 你需要安装这些包：googleplaces, googlemaps, gmaps, ortools. 你需要一个Google Maps API key, 从这里获取API. 完成配置等于成功的一半。在Jupyter notebook打开TSPSolver.ipynb，将第二个代码块的所有变量，改成自己的，比如自己的目的地自己的区域和自己的API密码……最后从头到尾运行所有代码块，你就可以得到自己的定制路线辣~&#xA;配置代码如下。&#xA;# input the places of interest (POI) places = &amp;#39;YHA London Central Hostel&amp;#39;, &amp;#39;Coca-Cola London Eye&amp;#39;, &amp;#39;St. Paul\&amp;#39;s Cathedral&amp;#39;, &amp;#39;Leadenhall Market&amp;#39;, &amp;#39;The National Gallery&amp;#39; \ &amp;#39;Big Ben&amp;#39;, &amp;#39;Buckingham Palace&amp;#39;, &amp;#39;Waterloo Station&amp;#39; # the region Location=&amp;#39;London&amp;#39; # choose a mode Mode = &amp;#34;walking&amp;#34; # &amp;#34;driving&amp;#34;, &amp;#34;walking&amp;#34;, &amp;#34;bicycling&amp;#34;, &amp;#34;transit&amp;#34; # get Google API key from following website: # https://developers.</description>
    </item>
  </channel>
</rss>
