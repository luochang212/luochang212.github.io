<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Trl on Chang Luo</title>
    <link>https://luochang212.github.io/tags/trl/</link>
    <description>Recent content in Trl on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 26 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/tags/trl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>三种方法实现监督微调 (SFT)：LLaMA Factory, trl 和 unsloth</title>
      <link>https://luochang212.github.io/posts/sft_note/</link>
      <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/sft_note/</guid>
      <description>我尝试了目前流行的三种微调框架，最推荐 unsloth，因为它快！LLaMA Factory 和 trl 是在夜里跑的，显卡风扇响了一宿。第二天看日志时，发现它们跑了三个小时才跑完。但是 unsloth 五分钟就跑完了，快得都有些夸张了。当然，这么比较不是完全公平的，因为它们的量化方法、LoRA 参数都是不同的。但是 unsloth 快这一点依然是非常显著的。如果你想在 GPU 服务器上认真微调，那么用 LLaMA Factory 没毛病。但如果只是在笔记本上随便玩玩，unsloth 的优势就太明显了！&#xA;GitHub 项目地址：sft-note&#xA;一、引言 大语言模型有很强的通用能力，但在特定领域，它的表现不如领域小模型。为了让大模型适应特定任务，我们对大模型进行微调，使大模型在保持通用性的同时，兼具领域模型的专业知识、对话风格和输出格式等特质。&#xA;微调大模型有三种范式：&#xA;无监督微调：在海量数据上进行二次预训练 PT 增量预训练 监督微调 (SFT)：构造领域数据集，增强模型的指令遵循能力，并注入领域知识 指令微调 强化学习微调：通过 reward 引导模型优化 RLHF 基于人类反馈的强化学习 DPO 直接偏好优化方法 ORPO 比值比偏好优化 GRPO 群体相对策略优化 本文聚焦 监督微调 (Supervised Fine-Tuning)。监督微调是一种简单但有效的微调方式，能够快速融合业务数据、适应业务场景，因此它的性价比极高！&#xA;1. SFT 的简单介绍 监督微调的优化目标是 最小化模型生成回答与目标回答之间的差异，通常使用交叉熵损失。为避免破坏预训练阶段获得的知识，SFT 阶段通常使用 较低的学习率，并且只更新部分参数层，其他参数保持不变。与预训练阶段所需的海量数据相比，SFT 只需 较小的数据量（数千到数十万样本），即可完成微调。&#xA;2. SFT 的使用场景 为了让大家感受一下 SFT 能做什么，下面列举一些使用场景：&#xA;任务 场景举例 类型 文案生成 输入标签：红色#女士#卫衣，输出文案：女士专属红色卫衣，解锁秋冬时尚密码 任务对齐 情感分类 输入用户评论：蓝牙连接不稳定，输出情感标签：负面 任务对齐 合同审核 输入合同文本，输出潜在法律风险，并引述法条和案例 知识迁移 3.</description>
    </item>
  </channel>
</rss>
