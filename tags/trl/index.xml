<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Trl on Chang Luo</title>
    <link>https://luochang212.github.io/tags/trl/</link>
    <description>Recent content in Trl on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 26 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/tags/trl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>三种方法实现监督微调 (SFT)：LLaMA Factory, trl 和 unsloth</title>
      <link>https://luochang212.github.io/posts/sft_note/</link>
      <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/sft_note/</guid>
      <description>试了三种流行的微调框架，其中最推荐的是 unsloth，因为快！另外两种框架 LLaMA Factory 和 trl 是在夜里跑的，显卡风扇响了一宿。第二天看日志，它们都跑了三个多小时才跑完。但是同样的任务，unsloth 只需要五分钟，快得有些离谱。当然，这么比不是完全公平的，因为它们的量化方法、LoRA 参数是不同的。但是 unsloth 快这一点依然是无可质疑的。如果在 GPU 服务器上认真微调，那么用 LLaMA Factory 没毛病；但如果只是在笔记本上随便玩玩，unsloth 的优势就太大了！&#xA;GitHub 项目地址：sft-note&#xA;⭐ 本文的内容包括：&#xA;大模型微调的三种范式：无监督微调、监督微调、强化学习微调 介绍用于监督微调的数据格式，以及如何加载数据集 如何下载 Qwen 模型，代码见 download_qwen.py 使用三种框架微调大模型：LLaMA Factory, trl, unsloth 一、引言 大语言模型有很强的通用能力，但在特定领域，它的表现不如领域小模型。为了让大模型适应特定任务，我们对大模型进行微调，使大模型在保持通用性的同时，兼具领域模型的专业知识、对话风格和输出格式等特质。&#xA;微调大模型有三种范式：&#xA;无监督微调：在海量数据上进行二次预训练 PT 增量预训练 监督微调 (SFT)：构造领域数据集，增强模型的指令遵循能力，并注入领域知识 指令微调 强化学习微调：通过 reward 引导模型优化 RLHF 基于人类反馈的强化学习 DPO 直接偏好优化方法 ORPO 比值比偏好优化 GRPO 群体相对策略优化 本文聚焦 监督微调 (Supervised Fine-Tuning)。监督微调是一种简单但有效的微调方式，能够快速融合业务数据、适应业务场景，因此它的性价比极高！&#xA;1. SFT 的简单介绍 监督微调的优化目标是 最小化模型生成回答与目标回答之间的差异，通常使用交叉熵损失。为避免破坏预训练阶段获得的知识，SFT 阶段通常使用 较低的学习率，并且只更新部分参数层，其他参数保持不变。与预训练阶段所需的海量数据相比，SFT 只需 较小的数据量（数千到数十万样本），即可完成微调。&#xA;2. SFT 的使用场景 为了让大家感受一下 SFT 能做什么，下面列举一些使用场景：</description>
    </item>
  </channel>
</rss>
