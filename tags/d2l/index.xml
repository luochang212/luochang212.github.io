<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>D2L on Chang Luo</title>
    <link>https://luochang212.github.io/tags/d2l/</link>
    <description>Recent content in D2L on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Feb 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/tags/d2l/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>手写深度学习</title>
      <link>https://luochang212.github.io/posts/d2l_from_scratch/</link>
      <pubDate>Fri, 09 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/d2l_from_scratch/</guid>
      <description>与其说深度学习是一门技术，不如说深度学习是一种语言&#xA;GitHub 项目地址：AI-Project/scratch&#xA;一、自动微分 1. 简单的例子 1.1 张量 x 的梯度&#xA;张量 $x$ 的梯度可以存储在 $x$ 上。&#xA;要点：&#xA;x.grad: 取 $x$ 的梯度 x.requires_grad_(True): 允许 tenser $x$ 存储自己的梯度 x.grad.zero_(): 将 $x$ 的梯度置零 import torch # 初始化张量 x (tenser x) x = torch.arange(4.0) x.requires_grad_(True) # 允许 tensor x 存储梯度 x.grad == None # 梯度默认为 None &amp;gt; True&#xA;初始化带梯度的张量，下面是两个例子：&#xA;torch.tensor([1., 2., 3.], requires_grad=True) &amp;gt; tensor([1., 2., 3.], requires_grad=True)&#xA;torch.randn((2, 5), requires_grad=True) &amp;gt; tensor([[ 0.4075, 1.</description>
    </item>
  </channel>
</rss>
