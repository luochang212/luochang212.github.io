<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>VLLM on Chang Luo</title>
    <link>https://luochang212.github.io/tags/vllm/</link>
    <description>Recent content in VLLM on Chang Luo</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://luochang212.github.io/tags/vllm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>本地部署大模型：Ollama 和 vLLM</title>
      <link>https://luochang212.github.io/posts/llm_deploy/</link>
      <pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://luochang212.github.io/posts/llm_deploy/</guid>
      <description>2023 年本地部署大模型的报价近千万，2024 年初便骤降至百万，如今是 2025 年，只需要一行 vLLM 命令就可以部署大模型，人工成本几近于零。&#xA;GitHub 项目地址：llm-deploy&#xA;本文内容包括：&#xA;3 种方式部署 DeepSeek R1：Ollama, vLLM 和 Transformers 使用 vLLM 部署 Qwen2.5 模型 安装 Open WebUI 作为本地模型的前端聊天框 通过 vllm serve 实现一行代码启动 vLLM 推理服务 ✨ 快速部署说明在 /deploy，vLLM 服务启动脚本在 /server.&#xA;一、本地部署 DeepSeek R1 大模型本地部署依赖推理引擎，目前比较流行的推理引擎有：&#xA;推理引擎 场景 介绍 Ollama 适合个人开发者和轻量级应用 基于 llama.cpp 开发，支持 CPU 推理，安装简单，开箱即用，适合快速原型开发和测试 vLLM 适合高并发生产环境 支持多 GPU 并行、批处理、PagedAttention，吞吐量高，延迟低，适合大规模服务部署 Transformers 适合模型研究和实验 提供完整的模型训练和推理接口，支持模型微调、量化、加速，适合研究人员和需要深度定制的场景 SGLang 适合需要复杂推理流程的场景 支持结构化输出、并行推理、流式输出，特别适合需要多轮对话和复杂推理的应用 LMDeploy 适合企业级部署和边缘计算 由上海人工智能实验室开发，提供完整的模型量化、加速和部署工具链，支持多种硬件平台，特别适合资源受限场景 下面介绍如何部署 Ollama, vLLM, Transformers 这三款推理引擎，简要部署步骤见本项目的 deploy 目录。</description>
    </item>
  </channel>
</rss>
