<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>weibo on Chang Luo</title>
    <link>http://luochang212.github.io/tags/weibo/</link>
    <description>Recent content in weibo on Chang Luo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Apr 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://luochang212.github.io/tags/weibo/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>我的微博数据可视化</title>
      <link>http://luochang212.github.io/posts/my_weibo/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>http://luochang212.github.io/posts/my_weibo/</guid>
      <description>GitHub项目链接：weibo-analysis
这次的数据用的是本科期间就已经爬好，丢在硬盘里的数据。但因为原始数据里夹杂着大量和博文无关的字符，当时没有足够的编程技巧处理数据，这一丢就是两年。本Python初丁趁着现在还有机会摸鱼，赶紧把数据翻出来，让它们发光发热。
文本获取 因为新浪微博严防死打，现如今微博的数据越来越不好爬。Github上的微博爬虫生存周期通常都很短，使爬取数据的成本大大增加。但是万变不离其宗，最笨的方式常常是最有效的。这里我用的是微博@失眠狸 同学的方法，用鼠标精灵写了个插件，控制快捷键和页面拖动，把内容从浏览器上粘贴到sublime里。
生成csv文件 有了原始数据，接下来我们要把数据归一化，做成方便处理的数据。一个常用的方法就是将数据整理成csv文件。
第一步，先将原始文件按字段进行分割。根据原数据，我划分了五个字段: id, date, time, device, content, 它们分别记录本条微博的文件位置、发布日期、发布时间、发送设备和文本内容。
设计完数据结构之后，我们先用split函数对原数据进行粗略划分，再用find函数进一步定位到精确位置。接着提取各字段内容，再依次存入csv。整理数据的工作就完成啦。
可视化微博数据 有了csv文件，可视化数据就很方便了。此时我从PyCharm平台换到jupyter notebook工作，因为相较于PyCharm, ipynb可以制作的各式各样的可视化图表和窗口小工具(widget), 更适合于数据科学工作。至于工具包，这里我选择了数据分析工具pandas和可视化工具seaborn.
好了，废话少说，让我们进入分析流程。
首先是需求分析，我的目标如下:
 绘制日期分布热力图，观察今年使用微博频率的趋势
 绘制设备使用直方图，看看平时最常用什么平台发博
 绘制时间分布直方图，观察一天之中各时段的发博频率
 调用窗口工具，拖动查看各个时间段都发了什么内容
  过程见github repository, 就不赘述了。
分析结果如下:
热力图总的来说颜色逐年加深，说明我正在逐渐成为一个微博控。而且注意到往年年初我是不怎么玩微博的，但随着年纪渐长，1-3月份我玩微博的频率越来越高，这意味着过年可能越来越无聊，没有年味，从而加长了我混迹微博的时间。
是你吗？华为的舔狗~
晚上2点不睡的坏小孩，早上10点起的偷懒者。（此处是一个卑微的笑容）</description>
    </item>
    
  </channel>
</rss>